{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "grid_search.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN7wg5lqS31CYFtBALKBzz0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JuanJoseMV/neuraltextgen/blob/main/grid_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JsanwnfEwEn"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmc88etKbnkI"
      },
      "source": [
        "%%capture\n",
        "!git clone --recursive https://github.com/JuanJoseMV/neuraltextgen.git\n",
        "!pip install -r /content/neuraltextgen/texygen/requirements.txt\n",
        "!pip install simpletransformers"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7kWk-0gcHUA"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "os.chdir(\"/content/neuraltextgen/\")\n",
        "from NeuralTextGenerator import BertTextGenerator\n",
        "\n",
        "APEX_AVAILABLE = False\n",
        "NUM_TEST = 10 # number of runs for each set of parameters"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNHcFwngF1f7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHrueM9KFpWb"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "## Texygen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOHUsZVOcMM8",
        "outputId": "e663a815-4d58-4449-9936-d91a199b6913"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "os.chdir(\"/content/neuraltextgen/texygen\")\n",
        "from utils.metrics.Bleu import Bleu\n",
        "from utils.metrics.SelfBleu import SelfBleu\n",
        "os.chdir(\"/content\")\n",
        "\n",
        "n_grams = 4\n",
        "WIKI103_PATH = '/content/neuraltextgen/data/wiki103.5k.txt'\n",
        "TBC_PATH = '/content/neuraltextgen/data/tbc.5k.txt'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtN0fCyOlx2T"
      },
      "source": [
        "def evaluate_texygen(file_path, n_grams):\n",
        "  bleu_score_tbc = Bleu(file_path, TBC_PATH, gram = n_grams).get_bleu()\n",
        "  bleu_score_wiki_en = Bleu(file_path, WIKI103_PATH, gram = n_grams).get_bleu()\n",
        "  self_bleu_score = SelfBleu(file_path, gram=n_grams).get_bleu()\n",
        "\n",
        "\n",
        "  return (bleu_score_tbc, bleu_score_wiki_en, self_bleu_score)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X74hBurrFvyK"
      },
      "source": [
        "## Original scoring functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BEBtXQFwM7M"
      },
      "source": [
        "from nltk.translate import bleu_score as bleu\n",
        "\n",
        "def prepare_data(data_file, replacements={}, uncased=True):\n",
        "    data = [d.strip().split() for d in open(data_file, 'r').readlines()]\n",
        "    if uncased:\n",
        "        data = [[t.lower() for t in sent] for sent in data]\n",
        "        \n",
        "    for k, v in replacements.items():\n",
        "        data = [[t if t != k else v for t in sent] for sent in data]\n",
        " \n",
        "    return data\n",
        "\n",
        "def prepare_wiki(data_file, uncased=True):\n",
        "    replacements = {\"@@unknown@@\": \"[UNK]\"}\n",
        "    return prepare_data(data_file, replacements=replacements, uncased=uncased)\n",
        "\n",
        "def prepare_tbc(data_file):        \n",
        "    replacements = {\"``\": \"\\\"\", \"\\'\\'\": \"\\\"\"}\n",
        "    return prepare_data(data_file, replacements=replacements)\n",
        "\n",
        "def corpus_bleu(generated, references):\n",
        "    \"\"\" Compute similarity between two corpora as measured by\n",
        "    comparing each sentence of `generated` against all sentences in `references` \n",
        "    \n",
        "    args:\n",
        "        - generated (List[List[str]]): list of sentences (split into tokens)\n",
        "        - references (List[List[str]]): list of sentences (split into tokens)\n",
        "        \n",
        "    returns:\n",
        "        - bleu (float)\n",
        "    \"\"\"    \n",
        "    return bleu.corpus_bleu([references for _ in range(len(generated))], generated)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DWAfUelwOjy"
      },
      "source": [
        "wiki_data = prepare_wiki(WIKI103_PATH)\n",
        "tbc_data = prepare_tbc(TBC_PATH)\n",
        "\n",
        "def evaluate_original(bert_sents):\n",
        "  return (corpus_bleu(bert_sents, tbc_data)), corpus_bleu(bert_sents, wiki_data)))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1wnwdiEgxUs"
      },
      "source": [
        "# Log results\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofXE83ehc21H"
      },
      "source": [
        "# LOG_FILE_PATH = '/content/drive/MyDrive/neuraltextgen/results.log'\n",
        "LOG_FILE_PATH = 'file.log'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtcKMM_eelJ6"
      },
      "source": [
        "import logging\n",
        "logging.basicConfig(level=logging.DEBUG, \n",
        "            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "            datefmt='%m/%d/%Y %H:%M:%S')\n",
        "logger = logging.getLogger(__name__)  # generally use __name__\n",
        "logger.propagate = False\n",
        "\n",
        "# setup\n",
        "file_h = logging.FileHandler('file.log')\n",
        "file_h.setLevel(logging.INFO)\n",
        "\n",
        "# formatter\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "            datefmt='%m/%d/%Y %H:%M:%S')\n",
        "file_h.setFormatter(formatter)\n",
        "\n",
        "logger.addHandler(file_h)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rdY2GkVGGlN"
      },
      "source": [
        "#Grid search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fShxooLjeX7D",
        "outputId": "3fb61e58-3738-49b3-85cb-b93d432f8d17"
      },
      "source": [
        "model = BertTextGenerator(\"bert-base-uncased\", use_apex = APEX_AVAILABLE)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "06/08/2021 17:31:45 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443\n",
            "06/08/2021 17:31:45 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/config.json HTTP/1.1\" 200 0\n",
            "06/08/2021 17:31:45 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443\n",
            "06/08/2021 17:31:45 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/pytorch_model.bin HTTP/1.1\" 302 0\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "06/08/2021 17:31:50 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443\n",
            "06/08/2021 17:31:50 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/config.json HTTP/1.1\" 200 0\n",
            "06/08/2021 17:31:50 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443\n",
            "06/08/2021 17:31:51 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/vocab.txt HTTP/1.1\" 200 0\n",
            "06/08/2021 17:31:51 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443\n",
            "06/08/2021 17:31:51 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/tokenizer.json HTTP/1.1\" 200 0\n",
            "06/08/2021 17:31:51 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443\n",
            "06/08/2021 17:31:51 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/added_tokens.json HTTP/1.1\" 404 0\n",
            "06/08/2021 17:31:51 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443\n",
            "06/08/2021 17:31:52 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/special_tokens_map.json HTTP/1.1\" 404 0\n",
            "06/08/2021 17:31:52 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443\n",
            "06/08/2021 17:31:52 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AoJJ37cGMYi"
      },
      "source": [
        "We define two parameters dictionaries\n",
        "\n",
        "\n",
        "*   fixed_parameters: for the parameters that should not be tested in the grid search\n",
        "*   parameters_to_test: as a dict with keys the parameters  and values a list of values to test\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jx23OWK-dRjk"
      },
      "source": [
        "fixed_parameters = {'n_sentences': 10, 'batch_size': 10,'max_iter': 100,'seed_text': \"\", 'sample': True}\n",
        "\n",
        "parameters_to_test = {'temperature': [0.001, 0.5, 1],\n",
        "                      'top_k': [None, 10, 50, 100],\n",
        "                      'generation_method': ['parallel', 'sequential', 'attention'] \n",
        "                      }"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuIg4ndAjJCb",
        "outputId": "793721fc-fc90-46a4-b525-c1e3d1fcf518"
      },
      "source": [
        "from itertools import product\n",
        "\n",
        "for p in product(*parameters_to_test.values()):\n",
        "  parameters = {**fixed_parameters, **dict(zip(parameters_to_test.keys(), p))} \n",
        "  print(parameters)\n",
        "\n",
        "  parameters_str = \",\".join([f\"{k}={v}\" for k, v in parameters.items()])\n",
        "  \n",
        "  #change as you prefer\n",
        "  file_path = parameters_str+\".txt\"\n",
        "  \n",
        "  for _ in range(NUM_TEST):\n",
        "    model.generate(save_to_path = file_path, **parameters)\n",
        "    texygen_bleu_tbc, texygen_bleu_wiki, texygen_self_bleu = evaluate(file_path, n_grams=4)\n",
        "    bleu_tbc, bleu_wiki, self_bleu = \n",
        "    \n",
        "    logger.info(parameters_str + f\",{bleu_tbc},{bleu_wiki},{self_bleu}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'n_sentences': 10, 'batch_size': 10, 'max_iter': 100, 'seed_text': '', 'sample': True, 'temperature': 0.001, 'top_k': None, 'generation_method': 'parallel'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmt8j08_jwYW"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    }
  ]
}