\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{biblatex}
\graphicspath{ {images/} }

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{4321}
\begin{document}

%%%%%%%%% TITLE
\title{\LaTeX\ Author Guidelines for CVPR Proceedings}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
   The ABSTRACT is to be in fully-justified italicized text, at the top
   of the left-hand column, below the author and affiliation
   information. Use the word ``Abstract'' as the title, in 12-point
   Times, boldface type, centered relative to the column, initially
   capitalized. The abstract is to be in 10-point, single-spaced type.
   Leave two blank lines after the Abstract, then begin the main text.
   Look at previous CVPR abstracts to get a feel for style and length.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

Please follow the steps outlined below when submitting your manuscript to
the IEEE Computer Society Press.  This style guide now has several
important modifications (for example, you are no longer warned against the
use of sticky tape to attach your artwork to the paper), so all authors
should read this new version.

%-------------------------------------------------------------------------
\section{Current methods}

\subsection{RNN}

\subsection{Transformers}


%------------------------------------------------------------------------
\section{BertTextGenerator}
BERT is a language representation model designed to achieve state-of-art results
in several natural language processing tasks like question answering and language inference.
BERT is trained on masked language modeling objective, in which it predicts a word given its
left and right context, given this behavior it is possible to deduct that BERT is not the most
indicated model for text generation tasks, however have demonstrated why
it is possible to use BERT as a traditional language model by means of showing that BERT is a
combination of a 'Markov random field language model (MRF-LM), with pseudo Log-Likelihood training'. \cite{Wang2019BERTHA}

This conclusion allows using BERT as a generative model of sentences to either score a sentence or
sample a sentence.
To score a sentence with BERT is possible to compute the unnormalized log-probabilities of a set of
sentences and then based of this it is possible to sort the set of sentences according to their probabilities.
To sample a sentence with BERT it is necessary to start with a random initial state which is initialized to be
an all-mask sequence i.e., ([MASK], â€¦,[MASK]), then at each iteration i, a position is selected in a random uniformly
way and mask out to be replaced by a new token according with the probability of the token of being present in current
sequence of tokens.

They have made several experiments to demonstrate the potential of BERT as a text generation algorithm,
with the obtained results was possible to conclude that even if the BERT generations are of worst quality
compared with models like GPT, the text generated by BERT is more diverse and this is a meaningful result.


%------------------------------------------------------------------------
\subsection{Attention method}
The attention mechanism is composed by a feed forward neural network that is trained to
identify the more relevant components in the context in order to predict a new token.

For example take the sentence "the animal didn't cross the street because it was too tired",
as human is easy to know that in the sentence the pronoun "it" refers to the subject "the animal"
and the verb "was" is the correct conjugation of the verb "To be" for the third person, but for an
algorithm is not easy to know these relationships and that is the task of the attention mechanism.

The FIGURE # shows how the attention mechanism create a relationship between the token "it" and several components
of the sentence, for example the first attention head is totally focus on the verb "was", the second and third
attention heads are focus on the special tokens "[CLS]" and "[SEP]" that represent the beginning and the end of
the sentence respectively, finally the fourth attention head is focus on the word "animal". In this way BERT
can understand which parts of the context are more relevant for each one of the tokens and when a token
is masked to be replaced on which parts of the context must focus to make the prediction of the new token.

\includegraphics{attentionPlot.PNG}

\subsubsection{Using the attention mechanism to improve the sampling method.}
When a token is predicted, it is possible to know which tokens of the context were more relevant to
retrieve the prediction based on the attention weights.
This information is a very meaningful insight to improve
the way in which the sampling method is made.

To make the sampling in the iteration t we design the following procedure:

\begin{itemize}
\item Step 1: Retrieve the attention masks of the iterations t-1, then get the average value of the attention masks for each position inside each one of the encoders.
\item Step 2: Get the average value of the attention mask for each position over all the encoders.
\item Step 3: For each position p, there is a counter c that register how many times the position p has been selected to be replaced.
\item Step 4: the average value of the attention masks is divided by the counter with the aim of avoid that the sampling method get stuck sampling always the same positions that are more related with the given context.
\item Step 5: normalize the result of the step 4 to get a probabilistic interpretation.
\item Step 6: pass the normalized result to the sampling method to sample the new token to be replaced.
\end{itemize}

%------------------------------------------------------------------------
\subsection{Finetuning}
BERT is a powerful model pre-trained on a broad corpus composed by the whole
Wikipedia and Brown corpora.
Typical fine-tuning techniques depends on the final task that the user wants BERT to perform.
For example, a classical application of the model is to predict the sentiment of a sentence.
In this case the focus of the fine-tuning would be on the [CLS] token,
a special token specifically used for classification tasks.

In this case however the task is different and unusual for BERT.
Since we need to use BERT to generate text it is important that during the
fine-tuning the model understands the structure of the text.

We have implemented a fine-tuning method that gave completely freedom to the user to decide the language,
the structure and even the sentiment of the text to generate.
The fine-tuning is performed considering only one task of the
two original used to pre-train BERT; that is the mask-prediction.
15\% of the tokens of each sentence are replaced with a masked token and
BERT have to predict the original tokens.
The loss is computed as the cross-entropy between the logits for the
masked tokens outputted by BERT and the original tokens.
This method allows the final user to start from a pretrained model or, if
it is not available for the language chose by the user, from a cross-language model
and fine-tuning on a specific text corpus.

The fine-tuning allows also to exploit the enormous potentialities of BERT tokenizer.
The default vocabulary of the tokenizer contains 1000 unused tokens,
whose weights are randomly initialized.
Typically, these tokens are replaced with domain specific words,
so that during the fine-tuning the model will be able to learn them.
We have extended this idea in order to comprehend also tokens that defines
the structure of the text like '\textbackslash n' '\textbackslash t'.
Note that replacing in the vocabulary for example the token '[unused1]'
with '\textbackslash n' would have no effect since the tokenizer would
remove these tokens from the text even before the tokenization,
during the normalization step.
To solve this problem we have implemented a Formatter class that helps
the user maintaining a map between some user specified tokens that needs
to be preserved and some unused tokens chose to replace them.
The user-specified tokens are replaced with the unused tokens before
the tokenization and the fine-tuning and are replaced back only after the
text generation.
Using this method we were able to make a model learn the tercet structure
of Dante's Divine Comedy.

\subsubsection{Sentiment generation}
BERT tokenizer gave also the possibility to define some special tokens.
We have took advantage of this to define a new method to generate text
with a specific sentiment.
Considering a set of sentences each one with a possible sentiment labels
in the set ['pos', 'neg'], we define $n$ (3 by default) new special tokens
for each possible sentiment.
The special tokens are of the type [pos-i] $i=1,ldots,n$.
Before the fine-tuning the special tokens corresponding to the sentence
sentiment label are appended at the beginning of the sentence.
In this manner the model, during the fine-tuning is able to build a
relations among the special tokens of a sentiment and the specific words
related to that sentiment.
At inference time, to generate some text with a specific sentiment,
we simply pass the special tokens of that sentiment as seed\_text to the
generate method.
This method, even in its simplicity, showed its efficacy in generating positive
and negative italian tweets about football

%------------------------------------------------------------------------
\section{Experiments and evaluation}

%-------------------------------------------------------------------------
\section{Conclusions}

\section*{References}


\end{document}
