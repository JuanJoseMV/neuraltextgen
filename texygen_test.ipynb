{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "texygen_test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JuanJoseMV/neuraltextgen/blob/main/texygen_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-gcgNG2Hqy-"
      },
      "source": [
        "%%capture\n",
        "!git clone --recursive https://github.com/JuanJoseMV/neuraltextgen.git\n",
        "!pip install -r /content/neuraltextgen/texygen/requirements.txt\n",
        "!pip install simpletransformers\n",
        "! pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BG38Z9ooecZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c9e0876-1720-4d68-e5bb-0c2a73188bca"
      },
      "source": [
        "%%writefile setup.sh\n",
        "export CUDA_HOME=/usr/local/cuda-10.1\n",
        "! git clone https://github.com/NVIDIA/apex\n",
        "! pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing setup.sh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CK3X4eNTohN5"
      },
      "source": [
        "%%capture\n",
        "! sh setup.sh"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3RdQ0qXHtHS",
        "outputId": "c9d684ee-af3a-49b0-d7ca-d82623a3200e"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import sys, os\n",
        "\n",
        "os.chdir(\"/content/neuraltextgen/texygen\")\n",
        "from utils.metrics.Bleu import Bleu\n",
        "from utils.metrics.SelfBleu import SelfBleu\n",
        "\n",
        "os.chdir(\"/content/neuraltextgen/\")\n",
        "from NeuralTextGenerator import BertTextGenerator\n",
        "\n",
        "os.chdir(\"/content\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8vEfVzAIFK0"
      },
      "source": [
        "bert_model = BertTextGenerator('bert-base-uncased')\n",
        "\n",
        "parameters = {'n_sentences': 100,  # 1000\n",
        "              'batch_size': 50,  # 50\n",
        "              'max_len': 40,\n",
        "              'top_k': 100,\n",
        "              'temperature': 1,\n",
        "              'burnin': 250,\n",
        "              'sample': True,\n",
        "              'max_iter': 500,\n",
        "              'seed_text': \"\"\n",
        "              }\n",
        "\n",
        "# \"key1=val1_key2=val2_...txt\"\n",
        "file_path = \"_\".join([f\"{k}={v}\" for k, v in parameters.items()])+\".txt\"\n",
        "\n",
        "bert_sents = bert_model.generate(save_to_path=file_path, **parameters)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8J_yZEmpsDAX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ad07485-f54b-472c-98f6-4a4cf35fb903"
      },
      "source": [
        "print(\"\\nEnglish text generated: \")\n",
        "for sent in bert_sents:\n",
        "  print(f\"\\t{sent}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "English text generated: \n",
            "\toh, but there was something wonderful, wonderful about ian mackenzie and his family, and the wonderful people they all were.\n",
            "\tit's not like i will be here all night, or all day. i will just be burrowed inside.\n",
            "\t\" and so am i. \" \" is that so, little one? \" the queen asked, fully awake now.\n",
            "\t''i must go, and i will not go back there without you.'' my voice comes out hoarse.\n",
            "\tmr. thomas as the chief medical officer of the st mary's hospital. mr. john thomas as medical officer.\n",
            "\tpeter ( peter ) davies, alan bateman, alan davies, alan davies. writer and director : david dench.\n",
            "\t' heven, leave the girls alone. leave the bride and groom alone.'i looked up into his eyes.\n",
            "\the is an executive and management consultant working in the international divisions of ici group, among others, and companies worldwide.\n",
            "\tthe japanese version also features the cover art of the japanese version of the album, along with the japanese version released internationally.\n",
            "\tit was as loud as he could hear in the night, the same as day and night. he scanned the area.\n",
            "\t\" did i what? \" no. \" i never slept with keely. \" shit, was that even possible?\n",
            "\t( david davies, for example, a history of the russian orthodox church, 8 may 2006 ). davies, david.\n",
            "\t( see below ) 1864 - 1865. letters from new york to jack morris :.. letters to jack morris :.\n",
            "\t\" the seed of seed... seed of seed... the seed... the seed of seed... \" the boy nodded slowly.\n",
            "\t\" sometimes you have to find someone who has a whole lot of emotional baggage. \" \" yeah, \" i said.\n",
            "\t2006 rossi, anni ( ed ( ed. ) ) : 2006 - 2007 rossi, anni ( ed. )\n",
            "\the looked so serious, like he was thinking about being with me again. \" let me go, \" he said.\n",
            "\tand the following entities : israel port authority : the port authority's legal successor is the israel economic and monetary authority.\n",
            "\tsir william smith, mp, who was mp for harrow and battersea, died aged 66 years old ( unmarried ).\n",
            "\twhy?'' ( because i live here! )'' you're crazy!'' i love you!\n",
            "\twritten by david bowie. dedicated to david bowie. \" cry for me love, love, love cry for me \".\n",
            "\t\" a horrible, horrible, horrible boy. he was just a little boy. \" and a little boy, too.\n",
            "\tbut i also knew the real world i was in. i had walked into it a year ago and turned it around.\n",
            "\tand i do. well, at first, we all agreed that it was an easy decision. now, i do.\n",
            "\t\" your family is thick as thieves. \" \" no husband, no children. \" the door slammed again and again.\n",
            "\talthough the men had worked hard, they were not very successful. it is also said that the women were very successful.\n",
            "\tbut i also thought about how much my hooking up with my older brother ( and who he was ) had helped.\n",
            "\t\" the symbol can also be attached to something, just like a't.'\" my heart skipped a beat.\n",
            "\the made a sharp turn and entered the cave, lying flat on his back. kendra and the others followed behind him.\n",
            "\tprinciples and applications of mathematics, princeton university press, princeton, 1987, p. 1.......\n",
            "\t\" my lady, hear the voice of richard! the voice of richard! you must hear what he has said! \"\n",
            "\tcolonel john mcdonald ( 1971 - 1999 ) ( retired ). air marshal john j. mcdonald ( retired in 1999 ).\n",
            "\t\" and what do i do if my father wants a piece of me? \" saetan's long fingers steepled.\n",
            "\t\" i'm not sure what that word would mean.'come on,'\" i said, standing up.\n",
            "\tp. 9. jones, nigel ( 2012 - 13 ) - - ; jones, nigel ; - - - - ;\n",
            "\t\" you remember when i said i was watching those old movies and loved them so much? \" i forced a smile.\n",
            "\tthe character first became a playable playable character in the japanese version of revelations ii : revelations and has been appearing ever since.\n",
            "\t\" your presence in the room was designed to protect it. \" \" it was designed, \" daemon sadi said.\n",
            "\tthen the pain goes away and i drift off. i forget where i am. i forget what he did to me.\n",
            "\tjames wilson was born in in the baltimore area of maryland, to lieutenant colonel james milburn wilson and mary ann wilson.\n",
            "\tthe people. the people the ex - cops were talking about, i thought, looking at the people who knew me.\n",
            "\t( 2000 ). our lives together. heart - to - heart. our lives heart to heart. ( 2000 ).\n",
            "\tit is a double double - cd set, with one disc each containing songs by john lennon and clark terry - lewis.\n",
            "\ti now understood why so many different people had lived here since the late 1800s, but they were all gone by now.\n",
            "\talex and i are more than friends. alex and i have always gotten along. alex and i have gotten back together.\n",
            "\toh god, oh my god. if it did happen, then what? \" no, \" i said, stunned.\n",
            "\t\" well, if i said'no government, there would be no government,'\" vanion said bluntly.\n",
            "\thonourable william henry alexander hamilton, member of parliament. the honourable william alexander hamilton, of ballymun. alexander hamilton.\n",
            "\toh, and then he was doing it, laughing and talking again, like he was actually talking to his best friend.\n",
            "\talthough the single version was released in two versions, \" someone like you \" and \" teenage dream \" are almost identical.\n",
            "\thon william thomson lieut. hon james thomson william thomson hon james thomson hon william thomson lieut.\n",
            "\t' why, exactly, are you talking to me?'' my whole life is different. '\n",
            "\the was as far gone as his father and brother. swanny had been raised by swanny.\n",
            "\tthen one, two, three, then three, three, and always one, and always one.\n",
            "\tshe and he are married and he will live with her. she and kabir have two sons.\n",
            "\t27 - 30 ( 2001 ). piano sonata : sonata ( sonata for piano ) in a minor.\n",
            "\teven the dauphin, of the royal house of france, was more a tory than a jacobin.\n",
            "\taccording to her, he was a high - level agent and they often worked together as a team.\n",
            "\tj. bott. ( 1911 ) history of johnson & johnson life insurance co. bott.\n",
            "\tpreparations for the hungarian revolution of 1848. budapest : a. g. varga. english translation.\n",
            "\tand just what the hell was she doing here? far too dangerous to be in a public place.\n",
            "\ti took my wet clothes off, leaving all my wet clothes behind in a very small metal box.\n",
            "\t{ 1 }. two other issues are under consideration, both of which are considered by the committee.\n",
            "\t1, issue # 1, no. 1, 1986. smith, simms ( 1987 ).\n",
            "\tbesson had a young adult son, jean - baptiste, and a daughter marie - josephe.\n",
            "\tcaptain james alexander mcleod. warrant officer james alexander mcleod mc. warrant officer james a. mcleod mc.\n",
            "\ti was shocked, not shocked, not shocked, not in any way. i was shocked too.\n",
            "\tbut more importantly, he had more than a few kids and had a wife who lived with him.\n",
            "\t\" not for me, \" said the wolf. \" no, \" said the crossbreed.\n",
            "\ti asked in disbelief. \" you know her name? \" \" her real name is ember.\n",
            "\tthe \" a \" model and the \" b \" are currently ( ) only available for retail sale.\n",
            "\tten minutes later he was gone. vinny : well, he's a good looking guy.\n",
            "\teven when we are apart, he is always so gentle, and his gentleness always lingers.\n",
            "\t....................\n",
            "\t\" like the blue - eyed girl i was dancing with, \" i said, and he laughed.\n",
            "\tpp. 780 - 788. london : penguin books, 1980. li li and wang li.\n",
            "\the taught courses in philosophy, history, business administration, etc., very popular among his students.\n",
            "\tthey had one daughter, elizabeth ( now a widow ), before she remarried to lord dartmoor.\n",
            "\tnone of the men showed the slightest sign of any kind of evil intent, aside from briec.\n",
            "\tcharacters named \" bob \" \" bob \", \" bob, \" and \" bob \" were introduced.\n",
            "\tshe married john smith, private secretary to john stuart mill and had two sons, but no daughters.\n",
            "\tsci., sci., sci., cit., cit., syst.\n",
            "\t\" so this is it, \" i always tell her, and it always makes me antsy.\n",
            "\tthere, she meets her brother paul ( peter cook ) who lives in st kilda with selina.\n",
            "\ta man who could change the world. but shahara knew what to do with him right now.\n",
            "\tthey had three children, and a very small number of grandchildren, who are buried in the cemetery.\n",
            "\twhen he did, finally, finally calm down, he was back back to his normal normal self.\n",
            "\t\" for the last time, \" he said, still staring out the window at the setting sun.\n",
            "\tafter the album is completed, phil collins also joins the band, replacing keyboard player simon cowell.\n",
            "\ti did not really know him, but sometimes i wondered what he had to do with my life.\n",
            "\tindeed, he knows without a doubt the power he [ the occultist ] has over us all.\n",
            "\t' talk about'write '. talk about'' read'' and'' write '.\n",
            "\tin summer it is dried, and then sun - dried again, and then mixed with fresh water.\n",
            "\tsir sir john mottram, 1st baronet, 2nd baronet, c. b., bt.\n",
            "\ton the most recent occasion he had stopped talking. \" i... \" he sounded almost regretful.\n",
            "\tex - husband, john, and his wife elizabeth, and children, john, john and elizabeth ;\n",
            "\tmember of the conservative party of norway. before the election the deputy party leader was per nilsen.\n",
            "\tit would also be agreed that the scottish government would build a line from mersey to glasgow central.\n",
            "\tif glenda hopewell is in charge of the investigation, she has a point, after all.\n",
            "\tchristian st. germain is a swiss footballer. he has played internationally for the switzerland national football team.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAyLelGANeRN"
      },
      "source": [
        "#Evaluation - Original"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwHPw0SvG7UW"
      },
      "source": [
        "! git clone https://github.com/nyu-dl/bert-gen\n",
        "wiki103_file = 'bert-gen/data/wiki103.5k.txt'\n",
        "tbc_file = 'bert-gen/data/tbc.5k.txt'\n",
        "\n",
        "wiki_data = prepare_wiki(wiki103_file)\n",
        "tbc_data = prepare_tbc(tbc_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WReMKKtxNd3Y"
      },
      "source": [
        "from nltk.translate import bleu_score as bleu\n",
        "\n",
        "def prepare_data(data_file, replacements={}, uncased=True):\n",
        "    data = [d.strip().split() for d in open(data_file, 'r').readlines()]\n",
        "    if uncased:\n",
        "        data = [[t.lower() for t in sent] for sent in data]\n",
        "        \n",
        "    for k, v in replacements.items():\n",
        "        data = [[t if t != k else v for t in sent] for sent in data]\n",
        " \n",
        "    return data\n",
        "\n",
        "def prepare_wiki(data_file, uncased=True):\n",
        "    replacements = {\"@@unknown@@\": \"[UNK]\"}\n",
        "    return prepare_data(data_file, replacements=replacements, uncased=uncased)\n",
        "\n",
        "def prepare_tbc(data_file):        \n",
        "    replacements = {\"``\": \"\\\"\", \"\\'\\'\": \"\\\"\"}\n",
        "    return prepare_data(data_file, replacements=replacements)\n",
        "\n",
        "def corpus_bleu(generated, references):\n",
        "    \"\"\" Compute similarity between two corpora as measured by\n",
        "    comparing each sentence of `generated` against all sentences in `references` \n",
        "    \n",
        "    args:\n",
        "        - generated (List[List[str]]): list of sentences (split into tokens)\n",
        "        - references (List[List[str]]): list of sentences (split into tokens)\n",
        "        \n",
        "    returns:\n",
        "        - bleu (float)\n",
        "    \"\"\"    \n",
        "    return bleu.corpus_bleu([references for _ in range(len(generated))], generated)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4eu_1FlRUbT"
      },
      "source": [
        "Try to evaluate using original functions and no cleaning of wiki-data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Apeq6y0zN6Z4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "053bce5b-a7d0-4dd0-82bd-a11445182338"
      },
      "source": [
        "print(\"BERT-TBC BLEU: %.2f\" % (100 * corpus_bleu(bert_sents, tbc_data)))\n",
        "print(\"BERT-Wiki103 BLEU: %.2f\" % (100 * corpus_bleu(bert_sents, wiki_data)))\n",
        "print(\"BERT-{TBC + Wiki103} BLEU: %.2f\" % (100 * corpus_bleu(bert_sents, tbc_data[:2500] + wiki_data[:2500])))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "BERT-TBC BLEU: 19.98\n",
            "BERT-Wiki103 BLEU: 27.60\n",
            "BERT-{TBC + Wiki103} BLEU: 27.31\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBtiIe1lRbCt"
      },
      "source": [
        "Try to evaluate after cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZCYFkW9RfKe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8539ea1c-3e8e-4f83-f11a-65872de2c55c"
      },
      "source": [
        "def cleaner(data):\n",
        "  len_mask = []\n",
        "  \n",
        "  for i in range(len(data)):\n",
        "    if len(data[i]) < 4:\n",
        "      len_mask.append(False)\n",
        "    else:\n",
        "      len_mask.append(True)\n",
        "\n",
        "  data = [b for a, b in zip(len_mask, data) if a]\n",
        "  return data\n",
        "\n",
        "wiki_data = cleaner(wiki_data)\n",
        "tbc_data = cleaner(tbc_data)\n",
        "\n",
        "print(\"BERT-TBC BLEU: %.2f\" % (100 * corpus_bleu(bert_sents, tbc_data)))\n",
        "print(\"BERT-Wiki103 BLEU: %.2f\" % (100 * corpus_bleu(bert_sents, wiki_data)))\n",
        "print(\"BERT-{TBC + Wiki103} BLEU: %.2f\" % (100 * corpus_bleu(bert_sents, tbc_data[:2500] + wiki_data[:2500])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "BERT-TBC BLEU: 2.09\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "BERT-Wiki103 BLEU: 27.48\n",
            "BERT-{TBC + Wiki103} BLEU: 26.46\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6GquBooOFDS"
      },
      "source": [
        "## Evaluation - Texygen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQmE68W1LkXt",
        "outputId": "daebddd2-e75a-4258-8bbf-cd73458c3118"
      },
      "source": [
        "bleu_score_tbc = Bleu(file_path, tbc_file)\n",
        "bleu_score_wiki = Bleu(file_path, wiki103_file)\n",
        "\n",
        "print(\"(Texygen) BERT-TBC BLEU: %.2f\" % (100 * bleu_score_tbc.get_bleu()))\n",
        "print(\"(Texygen) BERT-Wiki103 BLEU: %.2f\" % (100 * bleu_score_wiki.get_bleu()))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(Texygen) BERT-TBC BLEU: 30.90\n",
            "(Texygen) BERT-Wiki103 BLEU: 21.87\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLM6yrJXxqvf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9f8d9bf-c241-44fc-dc5d-a56bc53fef2e"
      },
      "source": [
        "self_bleu_score = SelfBleu(file_path)\n",
        "\n",
        "print(\"(Texygen) BERT- SelfBLEU: %.2f\" % (100 * self_bleu_score.get_bleu())) ## Oddly behaving\n",
        "print(\"(Texygen) BERT- SelfBLEU: %.2f\" % (100 * self_bleu_score.get_bleu_parallel())) ## Expected results"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(Texygen) BERT- SelfBLEU: 100.00\n",
            "(Texygen) BERT- SelfBLEU: 11.20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0v5S0ELkOOWF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbdbfacd-eee3-4f8e-fc50-161663c5dc7e"
      },
      "source": [
        "## (Texygen) methods testing\n",
        "\n",
        "import numpy as np\n",
        "def get_reference(test_data):\n",
        "    if True:\n",
        "        reference = list()\n",
        "        with open(test_data) as real_data:\n",
        "            for text in real_data:\n",
        "                text = nltk.word_tokenize(text)\n",
        "                reference.append(text)\n",
        "        # self.reference = reference\n",
        "        return reference\n",
        "\n",
        "def get_bleu(test_data):\n",
        "    ngram = 4\n",
        "    bleu = list()\n",
        "    reference = get_reference(test_data)\n",
        "    weight = tuple((1. / ngram for _ in range(ngram)))\n",
        "\n",
        "    with open(test_data) as test_data:\n",
        "        for the_hypothesis in test_data:\n",
        "            the_hypothesis = nltk.word_tokenize(the_hypothesis)\n",
        "            ## Fix: Exclude hypothesis\n",
        "            index = reference.index(the_hypothesis)\n",
        "            cleaned_reference = reference[:index] + reference[index + 1:]\n",
        "            ##\n",
        "            score = nltk.translate.bleu_score.sentence_bleu(cleaned_reference, the_hypothesis, weight) # Missing SmoothingFunction\n",
        "            bleu.append(score)\n",
        "    return sum(bleu) / len(bleu)\n",
        "\n",
        "get_bleu(file_path)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5311765501135502"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlzAZtw3sOPN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e4d56d6-a399-42df-912f-9f2f811d2c57"
      },
      "source": [
        "## Paper self-bleu scoring\n",
        "bleu.corpus_bleu([[s for (j, s) in enumerate(bert_sents) if j != i] for i in range(len(bert_sents))], bert_sents)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7922691299230896"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}