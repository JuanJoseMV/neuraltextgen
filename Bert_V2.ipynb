{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Bert_V2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JuanJoseMV/neuraltextgen/blob/main/Bert_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E75WTl6tC8Qf",
        "outputId": "22bc6fc2-e81c-4635-e9dc-56e20b54d3cc"
      },
      "source": [
        "!pip3 install pytorch_pretrained_bert"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch_pretrained_bert in /usr/local/lib/python3.7/dist-packages (0.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.19.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.17.84)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.7.4.3)\n",
            "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (0.4.2)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.21.0,>=1.20.84 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (1.20.84)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.84->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.84->boto3->pytorch_pretrained_bert) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvQImI89CEkV",
        "outputId": "1921a783-e1e5-48f4-d259-7e02722a0473"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "zzL_g2mgCyZH"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "QKwu1bNRCyZt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5705963f-7c43-4972-df0c-6e8db5a2b22c"
      },
      "source": [
        "# Load pre-trained model (weights)\n",
        "model_version = 'bert-base-uncased'\n",
        "model = BertForMaskedLM.from_pretrained(model_version)\n",
        "model.eval()\n",
        "cuda = torch.cuda.is_available()\n",
        "if cuda:\n",
        "    model = model.cuda(0)\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=model_version.endswith(\"uncased\"))\n",
        "\n",
        "def tokenize_batch(batch):\n",
        "    return [tokenizer.convert_tokens_to_ids(sent) for sent in batch]\n",
        "\n",
        "def untokenize_batch(batch):\n",
        "    return [tokenizer.convert_ids_to_tokens(sent) for sent in batch]\n",
        "\n",
        "def detokenize(sent):\n",
        "    \"\"\" Roughly detokenizes (mainly undoes wordpiece) \"\"\"\n",
        "    new_sent = []\n",
        "    for i, tok in enumerate(sent):\n",
        "        if tok.startswith(\"##\"):\n",
        "            new_sent[len(new_sent) - 1] = new_sent[len(new_sent) - 1] + tok[2:]\n",
        "        else:\n",
        "            new_sent.append(tok)\n",
        "    return new_sent\n",
        "\n",
        "CLS = '[CLS]'\n",
        "SEP = '[SEP]'\n",
        "MASK = '[MASK]'\n",
        "mask_id = tokenizer.convert_tokens_to_ids([MASK])[0]\n",
        "sep_id = tokenizer.convert_tokens_to_ids([SEP])[0]\n",
        "cls_id = tokenizer.convert_tokens_to_ids([CLS])[0]"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 407873900/407873900 [00:11<00:00, 34978241.28B/s]\n",
            "100%|██████████| 231508/231508 [00:00<00:00, 1146403.15B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmkQRGxKCyZz"
      },
      "source": [
        "# Generations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OF8xriowCyZ1"
      },
      "source": [
        "def generate_step(out, gen_idx, temperature=None, top_k=0, sample=False, return_list=True):\n",
        "    \"\"\" Generate a word from from out[gen_idx]\n",
        "    \n",
        "    args:\n",
        "        - out (torch.Tensor): tensor of logits of size batch_size x seq_len x vocab_size\n",
        "        - gen_idx (int): location for which to generate for\n",
        "        - top_k (int): if >0, only sample from the top k most probable words\n",
        "        - sample (Bool): if True, sample from full distribution. Overridden by top_k \n",
        "    \"\"\"\n",
        "    logits = out[:, gen_idx]\n",
        "    if temperature is not None:\n",
        "        logits = logits / temperature\n",
        "    if top_k > 0:\n",
        "        kth_vals, kth_idx = logits.topk(top_k, dim=-1)\n",
        "        dist = torch.distributions.categorical.Categorical(logits=kth_vals)\n",
        "        idx = kth_idx.gather(dim=1, index=dist.sample().unsqueeze(-1)).squeeze(-1)\n",
        "    elif sample:\n",
        "        dist = torch.distributions.categorical.Categorical(logits=logits)\n",
        "        idx = dist.sample().squeeze(-1)\n",
        "    else:\n",
        "        idx = torch.argmax(logits, dim=-1)\n",
        "    return idx.tolist() if return_list else idx"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGuAwi8nCyZ4"
      },
      "source": [
        "# Generation modes as functions\n",
        "import math\n",
        "import time\n",
        "\n",
        "def get_init_text(seed_text, max_len, batch_size = 1, rand_init=False):\n",
        "    \"\"\" Get initial sentence by padding seed_text with either masks or random words to max_len \"\"\"\n",
        "    batch = [seed_text + [MASK] * max_len + [SEP] for _ in range(batch_size)]\n",
        "    #if rand_init:\n",
        "    #    for ii in range(max_len):\n",
        "    #        init_idx[seed_len+ii] = np.random.randint(0, len(tokenizer.vocab))\n",
        "    \n",
        "    return tokenize_batch(batch)\n",
        "\n",
        "def parallel_sequential_generation(seed_text, max_len=15, top_k=0, temperature=None, max_iter=300, burnin=200,\n",
        "                                   cuda=False, print_every=10, verbose=True):\n",
        "    \"\"\" Generate for one random position at a timestep\n",
        "    \n",
        "    args:\n",
        "        - burnin: during burn-in period, sample from full distribution; afterwards take argmax\n",
        "    \"\"\"\n",
        "    seed_len = len(seed_text)\n",
        "    batch = get_init_text(seed_text, max_len, batch_size)\n",
        "    \n",
        "    for ii in range(max_iter):\n",
        "        kk = np.random.randint(0, max_len)\n",
        "        for jj in range(batch_size):\n",
        "            batch[jj][seed_len+kk] = mask_id\n",
        "        inp = torch.tensor(batch).cuda() if cuda else torch.tensor(batch)\n",
        "        out = model(inp)\n",
        "        topk = top_k if (ii >= burnin) else 0\n",
        "        idxs = generate_step(out, gen_idx=seed_len+kk, top_k=topk, temperature=temperature, sample=(ii < burnin))\n",
        "        for jj in range(batch_size):\n",
        "            batch[jj][seed_len+kk] = idxs[jj]\n",
        "            \n",
        "        if verbose and np.mod(ii+1, print_every) == 0:\n",
        "            for_print = tokenizer.convert_ids_to_tokens(batch[0])\n",
        "            for_print = for_print[:seed_len+kk+1] + ['(*)'] + for_print[seed_len+kk+1:]\n",
        "            print(\"iter\", ii+1, \" \".join(for_print))\n",
        "            \n",
        "    return untokenize_batch(batch)\n",
        "\n",
        "def generate(n_samples, seed_text=\"[CLS]\", batch_size=10, max_len=25, \n",
        "             sample=True, top_k=100, temperature=1.0, burnin=200, max_iter=500,\n",
        "             cuda=False, print_every=1):\n",
        "    # main generation function to call\n",
        "    sentences = []\n",
        "    n_batches = math.ceil(n_samples / batch_size)\n",
        "    start_time = time.time()\n",
        "    for batch_n in range(n_batches):\n",
        "\n",
        "        batch = parallel_sequential_generation(seed_text, max_len=max_len, top_k=top_k,\n",
        "                                               temperature=temperature, burnin=burnin, max_iter=max_iter, \n",
        "                                               cuda=cuda, verbose=False)\n",
        "    \n",
        "\n",
        "        if (batch_n + 1) % print_every == 0:\n",
        "            print(\"Finished batch %d in %.3fs\" % (batch_n + 1, time.time() - start_time))\n",
        "            start_time = time.time()\n",
        "        \n",
        "        sentences += batch\n",
        "    return sentences"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBhSXoTsCyaB"
      },
      "source": [
        "# Utility functions\n",
        "\n",
        "def printer(sent, should_detokenize=True):\n",
        "    if should_detokenize:\n",
        "        sent = detokenize(sent)[1:-1]\n",
        "    print(\" \".join(sent))\n",
        "    \n",
        "def read_sents(in_file, should_detokenize=False):\n",
        "    sents = [sent.strip().split() for sent in open(in_file).readlines()]\n",
        "    if should_detokenize:\n",
        "        sents = [detokenize(sent) for sent in sents]\n",
        "    return sents\n",
        "\n",
        "def write_sents(out_file, sents, should_detokenize=False):\n",
        "    with open(out_file, \"w\") as out_fh:\n",
        "        for sent in sents:\n",
        "            sent = detokenize(sent[1:-1]) if should_detokenize else sent\n",
        "            ###print(\"%s\\n\" % \" \".join(sent))\n",
        "            out_fh.write(\"%s\\n\" % \" \".join(sent))"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thNZJi07CyaD",
        "outputId": "4c4194cc-df42-4d8d-b66c-60b82ff92891"
      },
      "source": [
        "n_samples = 100 #1000\n",
        "batch_size = 5 #50\n",
        "max_len = 40\n",
        "top_k = 100\n",
        "temperature = 0.7\n",
        "\n",
        "leed_out_len = 5 # max_len\n",
        "burnin = 250\n",
        "sample = True\n",
        "max_iter = 500\n",
        "\n",
        "# Choose the prefix context\n",
        "seed_text = \"[CLS]\".split()\n",
        "\n",
        "for temp in [1.0]:\n",
        "    bert_sents = generate(n_samples, seed_text=seed_text, batch_size=batch_size, max_len=max_len,\n",
        "                          sample=sample, top_k=top_k, temperature=temp, burnin=burnin, max_iter=max_iter,\n",
        "                          cuda=True)\n",
        "    out_file = \"/content/%s-len%d-burnin%d-topk%d-temp%.3f.txt\" % (model_version, max_len, burnin, top_k, temp)\n",
        "    write_sents(out_file, bert_sents, should_detokenize=True)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished batch 1 in 9.629s\n",
            "Finished batch 2 in 9.392s\n",
            "Finished batch 3 in 9.487s\n",
            "Finished batch 4 in 9.558s\n",
            "Finished batch 5 in 9.661s\n",
            "Finished batch 6 in 9.736s\n",
            "Finished batch 7 in 9.856s\n",
            "Finished batch 8 in 9.943s\n",
            "Finished batch 9 in 10.003s\n",
            "Finished batch 10 in 10.098s\n",
            "Finished batch 11 in 10.105s\n",
            "Finished batch 12 in 10.105s\n",
            "Finished batch 13 in 10.110s\n",
            "Finished batch 14 in 10.094s\n",
            "Finished batch 15 in 10.109s\n",
            "Finished batch 16 in 10.144s\n",
            "Finished batch 17 in 10.177s\n",
            "Finished batch 18 in 10.211s\n",
            "Finished batch 19 in 10.263s\n",
            "Finished batch 20 in 10.304s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddeb0px1CyaH"
      },
      "source": [
        "in_file = \"/content/bert-base-uncased-len40-burnin250-topk100-temp1.000.txt\"\n",
        "bert_sents = read_sents(in_file, should_detokenize=False)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "oKqNgtBhCyaI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd3fed7e-1988-4c83-8b32-6ee1fc7f8618"
      },
      "source": [
        "for i in range(25):\n",
        "    printer(bert_sents[i], should_detokenize=True)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "later , he appeared on live albums . they were live from cleveland , where he \" went wild with feeling \" before the performance , and dead kennedys , where \" taxi star \" performed briefly\n",
            "... the man from earlier , remembering his conversation with doctor marlin , the so called doc who always plugged into my brain , would stop me from thinking about the doctor who worked in space\n",
            "the years he had heard that several members of the council had shown up at evernight academy before and they were pushing them back , too , and it took as much time as anyone possibly could\n",
            "is however very keen to go to dimarzio to retrieve a piece of the dark archive , which is the result of explosion . meanwhile , simon , emily and carol are currently on the move\n",
            "trees had expired . the shadows who lay dead on the hill had grown so black that there was no joy , no suffering or grief or fear , no recognition of the shadows who were long gone\n",
            "change in restaurants , eateries , and urban cuisines . westphal syss . sage . ( january , 1971 ) . \" urban cuisine . \" the \" art of cooking \" chapter\n",
            "gannon was its manager . here he originated with connie ray , wife of scott ray who now assisted parents with unborn children - amber ray , jesse ray , olivia ray and then weezer\n",
            "images were of a pivotal event in the movie . a tele - mental crisis . a delusional haze . this had to be far from over . but how could he tell her this\n",
            "( scriptwriter : tim quaye , cinematographer : jason kennedy , executive producer : trudeau ) the artistically - directed opening theme , as well as other videos about neo - nazis , no\n",
            ". 1982 - october . 1982 [ first day of attendance ] - autumn and winter . from 1982 - 1992 . list of exhibitions : ? ? ? 1982 - 1992 - ' joe on guitar '\n",
            "was part of yasmine lee . ~ ~ ~ ~ \" dismayed , and dismayed enough to believe in his god , and the healing energy inside of him , \" remarked christian quietly\n",
            "fernando , the 2nd - act ; a 1627 biography of catherine de ' medici . see also barros , francisco ( 1827 ) zacarias , don pedro , the 3rd in 4 acts\n",
            "even , by now , a holy man , is he right - no wrong ? julie white plays beava , with anna lee as anna , mike clark as muck , bree lawson as anna\n",
            ", danny , danny ( including his biggest hits ) ; \" baby caprice \" ; / \" what i owe you \" [ danny glover / dave ramsey / bobby brown / mr . show ]\n",
            ": trevor phillips manager : trevor phillips the sr - p - specification was the american a - xs - p90c motor , originally badge engineered and intended exclusively for the road - bicycles market\n",
            "founded fashion records ( a label promoting fashion ) . in a 2009 interview with forbes , she was interested in music and fashion and became friends after listening to sex , modeling , and starring vincent price\n",
            "correct answer typically follows five days of one ' s request . skeeter smith consistently notes that three answers must work within a certain checklist : 1st . good answer2 . bad answer3\n",
            "do crave killing , pain . i was a monster ! before him ... before the monsters . monsters . they destroyed this boy and his siblings and turned him into a monster - for that reason\n",
            "( 2016 ) onwards 2009 : dr . mclane jazz festival along with concerts by the \" ' ton ' tono \" band , \" akon \" , delta band and international band of malawi\n",
            "large stone fountain that was discovered at sheraton is called the sramijen , or [ sandin ] , it depicts [ muhammad ] ' s secret entrance into the pool of love\n",
            "had happened once meant everything to her , but just as she was reminded of her life with those two men trying to carry her through town and turning corners and making friends , everything was screwed up\n",
            "is seen picking up on the world ' s highest - lying pyramid ' ( difficulties are implied in the first novel ) , and talks to the white / catholic race as if the war were endless\n",
            "was no floral wallpaper on the south wall , no dishes on hangers , no sheet music sitting next to pots , or a broken - down barbie dress that was draped around the living room\n",
            "van buren , a native of fuqua , received money and arms from president benjamin harrison , who appointed him to the ' south end of the area , which now extends back to main street .\n",
            "contributions to the opc include including working with the community of conestoga ( mount hood ) ; appointed to euc design council selection committee ; became chairman of the canadian institute of architects board\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZZFmJnXCyaK"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rD1jETBzCyaL"
      },
      "source": [
        "from nltk.translate import bleu_score as bleu"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ir1f0hB5CyaM"
      },
      "source": [
        "## Quality Measures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKgArUXbCyaN"
      },
      "source": [
        "How similar are the generated sentences to the original training data (Toronto Book Corpus and Wikipedia dumps). We follow Yu et al., (2017) and compute the BLEU between the generations and the test sets of both corpora by treating the test set as the references for each generation. The tests sets are large; we subsample 5000 examples from each."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e62AX81CyaO"
      },
      "source": [
        "def prepare_data(data_file, replacements={}, uncased=True):\n",
        "    data = [d.strip().split() for d in open(data_file, 'r').readlines()]\n",
        "    if uncased:\n",
        "        data = [[t.lower() for t in sent] for sent in data]\n",
        "        \n",
        "    for k, v in replacements.items():\n",
        "        data = [[t if t != k else v for t in sent] for sent in data]\n",
        " \n",
        "    return data\n",
        "\n",
        "def prepare_wiki(data_file, uncased=True):\n",
        "    replacements = {\"@@unknown@@\": \"[UNK]\"}\n",
        "    return prepare_data(data_file, replacements=replacements, uncased=uncased)\n",
        "\n",
        "def prepare_tbc(data_file):        \n",
        "    replacements = {\"``\": \"\\\"\", \"\\'\\'\": \"\\\"\"}\n",
        "    return prepare_data(data_file, replacements=replacements)\n",
        "\n",
        "def corpus_bleu(generated, references):\n",
        "    \"\"\" Compute similarity between two corpora as measured by\n",
        "    comparing each sentence of `generated` against all sentences in `references` \n",
        "    \n",
        "    args:\n",
        "        - generated (List[List[str]]): list of sentences (split into tokens)\n",
        "        - references (List[List[str]]): list of sentences (split into tokens)\n",
        "        \n",
        "    returns:\n",
        "        - bleu (float)\n",
        "    \"\"\"    \n",
        "    return bleu.corpus_bleu([references for _ in range(len(generated))], generated)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkeUhZ1ECyaQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a59b3845-9efd-428c-ddfd-5f30155bc633"
      },
      "source": [
        "!git clone https://github.com/nyu-dl/bert-gen\n",
        "wiki103_file = 'bert-gen/data/wiki103.5k.txt'\n",
        "tbc_file = 'bert-gen/data/tbc.5k.txt'\n",
        "\n",
        "wiki_data = prepare_wiki(wiki103_file)\n",
        "tbc_data = prepare_tbc(tbc_file)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'bert-gen' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUr1orj_IYGM"
      },
      "source": [
        "def cleaner(data):\n",
        "  len_mask = []\n",
        "  for i in range(len(data)):\n",
        "    if len(data[i]) <4:\n",
        "      len_mask.append(False)\n",
        "    else:\n",
        "      len_mask.append(True)\n",
        "\n",
        "  data = [b for a, b in zip(len_mask, data) if a]\n",
        "  return data\n",
        "\n",
        "wiki_data = cleaner(wiki_data)\n",
        "tbc_data = cleaner(tbc_data)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpbwNz36CyaR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab074c88-f5a5-4f78-d6c6-0f4c2b715cd7"
      },
      "source": [
        "print(\"BERT-TBC BLEU: %.2f\" % (100 * corpus_bleu(bert_sents, tbc_data)))\n",
        "print(\"BERT-Wiki103 BLEU: %.2f\" % (100 * corpus_bleu(bert_sents, wiki_data)))\n",
        "print(\"BERT-{TBC + Wiki103} BLEU: %.2f\" % (100 * corpus_bleu(bert_sents, tbc_data[:2500] + wiki_data[:2500])))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT-TBC BLEU: 7.48\n",
            "BERT-Wiki103 BLEU: 8.83\n",
            "BERT-{TBC + Wiki103} BLEU: 9.37\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTGUEYjJCyah"
      },
      "source": [
        "## Diversity Measures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSnouDNrCyai"
      },
      "source": [
        "Self-BLEU: treat each sentence as a hypothesis and treat rest of corpus as reference. Lower is better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf1BPxK9Cyaj"
      },
      "source": [
        "from collections import Counter\n",
        "from nltk.util import ngrams\n",
        "\n",
        "def self_bleu(sents):\n",
        "    return bleu.corpus_bleu([[s for (j, s) in enumerate(sents) if j != i] for i in range(len(sents))], sents)\n",
        "\n",
        "def get_ngram_counts(sents, max_n=4):\n",
        "    size2count = {}\n",
        "    for i in range(1, max_n + 1):\n",
        "        size2count[i] = Counter([n for sent in sents for n in ngrams(sent, i)])\n",
        "    return size2count\n",
        "\n",
        "def ref_unique_ngrams(preds, refs, max_n=4):\n",
        "    # get # of *distinct* pred ngrams that don't appear in ref\n",
        "    pct_unique = {}\n",
        "    pred_ngrams = get_ngram_counts(preds, max_n)\n",
        "    ref_ngrams = get_ngram_counts(refs, max_n)\n",
        "    for i in range(1, max_n + 1):\n",
        "      pred_ngram_counts = set(pred_ngrams[i].keys())\n",
        "      total = sum(pred_ngrams[i].values())\n",
        "      ref_ngram_counts = set(ref_ngrams[i].keys())\n",
        "      pct_unique[i] = len(pred_ngram_counts.difference(ref_ngram_counts)) / total\n",
        "    return pct_unique\n",
        "        \n",
        "def self_unique_ngrams(preds, max_n=4):\n",
        "    # get # of pred ngrams with count 1\n",
        "    pct_unique = {}\n",
        "    pred_ngrams = get_ngram_counts(preds, max_n)\n",
        "    for i in range(1, max_n + 1):\n",
        "        n_unique = len([k for k, v in pred_ngrams[i].items() if v == 1])\n",
        "        total = sum(pred_ngrams[i].values())\n",
        "        pct_unique[i] = n_unique / total\n",
        "    return pct_unique"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "jvz6yF3ECyak",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0867f910-bdc6-4ab6-e76d-958766029d78"
      },
      "source": [
        "print(\"BERT self-BLEU: %.2f\" % (100 * self_bleu(bert_sents)))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT self-BLEU: 15.15\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWsNJWiYCyal",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b011b26-cc67-4620-8522-d081fcbe8558"
      },
      "source": [
        "max_n = 4\n",
        "\n",
        "pct_uniques = ref_unique_ngrams(bert_sents, wiki_data, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"BERT unique %d-grams relative to Wiki: %.2f\" % (i, 100 * pct_uniques[i]))\n",
        "pct_uniques = ref_unique_ngrams(bert_sents, tbc_data, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"BERT unique %d-grams relative to TBC: %.2f\" % (i, 100 * pct_uniques[i]))\n",
        "pct_uniques = self_unique_ngrams(bert_sents, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"BERT unique %d-grams relative to self: %.2f\" % (i, 100 * pct_uniques[i]))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT unique 1-grams relative to Wiki: 10.68\n",
            "BERT unique 2-grams relative to Wiki: 64.27\n",
            "BERT unique 3-grams relative to Wiki: 93.85\n",
            "BERT unique 4-grams relative to Wiki: 99.19\n",
            "BERT unique 1-grams relative to TBC: 16.87\n",
            "BERT unique 2-grams relative to TBC: 68.44\n",
            "BERT unique 3-grams relative to TBC: 94.42\n",
            "BERT unique 4-grams relative to TBC: 99.30\n",
            "BERT unique 1-grams relative to self: 29.62\n",
            "BERT unique 2-grams relative to self: 81.44\n",
            "BERT unique 3-grams relative to self: 97.43\n",
            "BERT unique 4-grams relative to self: 99.39\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0dCigCAPtE6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}