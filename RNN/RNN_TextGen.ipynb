{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN-TextGen.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JuanJoseMV/neuraltextgen/blob/main/RNN/RNN_TextGen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMSi0xt3lmij"
      },
      "source": [
        "# Instalations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZaPQHqdK3Bn"
      },
      "source": [
        "###### Apex"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVwHa6LMLmQD"
      },
      "source": [
        "%%capture\n",
        "!pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-1rILR6K4Sz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed82b983-ad44-4447-adc2-d1ea046b7cc1"
      },
      "source": [
        "%%writefile setup.sh\n",
        "export CUDA_HOME=/usr/local/cuda-10.1\n",
        "git clone https://github.com/NVIDIA/apex\n",
        "pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex\n",
        "# Writing setup.sh"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing setup.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvO6kiebLDW8"
      },
      "source": [
        "%%capture\n",
        "!sh setup.sh"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGpH7mioX-8Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c17002a3-07fe-47ba-b2e1-3c5768e3f8c9"
      },
      "source": [
        "! git clone --recursive https://github.com/JuanJoseMV/neuraltextgen.git"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'neuraltextgen'...\n",
            "remote: Enumerating objects: 944, done.\u001b[K\n",
            "remote: Counting objects: 100% (458/458), done.\u001b[K\n",
            "remote: Compressing objects: 100% (381/381), done.\u001b[K\n",
            "remote: Total 944 (delta 211), reused 171 (delta 74), pack-reused 486\u001b[K\n",
            "Receiving objects: 100% (944/944), 13.85 MiB | 18.16 MiB/s, done.\n",
            "Resolving deltas: 100% (375/375), done.\n",
            "Submodule 'texygen' (https://github.com/geek-ai/Texygen.git) registered for path 'texygen'\n",
            "Cloning into '/content/neuraltextgen/texygen'...\n",
            "remote: Enumerating objects: 888, done.        \n",
            "remote: Total 888 (delta 0), reused 0 (delta 0), pack-reused 888        \n",
            "Receiving objects: 100% (888/888), 21.85 MiB | 17.20 MiB/s, done.\n",
            "Resolving deltas: 100% (537/537), done.\n",
            "Submodule path 'texygen': checked out '3104e22ac75f3cc2070da2bf5e2da6d2bef149ad'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2REsainlXgA"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_rvBhZVlfhS"
      },
      "source": [
        "# import gensim.models.wrappers.fasttext\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "# from transformers import AutoModelForMaskedLM, AutoTokenizer, AutoModel, BertConfig, AutoConfig\n",
        "from collections import Counter"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXcpSeRYSkn2"
      },
      "source": [
        "# Cleaning the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hr7ElL5NVnt-"
      },
      "source": [
        "## When using Wiki.tokens (not wiki.5k)\n",
        "\n",
        "with open('/content/wiki.train.tokens') as f:\n",
        "  content = f.readlines()\n",
        "\n",
        "clean = []\n",
        "for c in content:\n",
        "  clean.append(c.replace('\\n', '[EOS]'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEnnOUVo2PYY"
      },
      "source": [
        "with open('/content/neuraltextgen/data/tbc.5k.txt') as f:\n",
        "  content = f.readlines()\n",
        "\n",
        "clean_first = []\n",
        "for c in content:\n",
        "  clean_first.append(c.replace(\"``\", \"\\\"\"))\n",
        "\n",
        "clean = []\n",
        "for c in clean_first:\n",
        "  clean.append(c.replace(\"\\'\\'\", \"\\\"\"))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcJ6YE0k4At8"
      },
      "source": [
        "! touch '/content/neuraltextgen/data/cleaned_tbc.5k.txt'"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiTsB50S20jt"
      },
      "source": [
        "with open('/content/neuraltextgen/data/cleaned_tbc.5k.txt', 'w') as f:\n",
        "    for item in clean:\n",
        "        f.write(\"%s\\n\" % item)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PluZGXYHmFL9"
      },
      "source": [
        "# Downloading pre-trained wordembeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-7av7vCmRnc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7581b34c-85eb-4e20-896c-6e5370036df0"
      },
      "source": [
        "# It takes some minutes, avoid if won't use\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format('/content/wiki-news-300d-1M.vec')\n",
        "word_vectors = model.wv\n",
        "\n",
        "weights = torch.FloatTensor(word_vectors.vectors)\n",
        "embedding = nn.Embedding.from_pretrained(weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOWABFfxUM1f"
      },
      "source": [
        "# Train the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5HR0v0dml6V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24ce9d23-1d3f-4132-ec43-897c76d57733"
      },
      "source": [
        "os.chdir('/content/neuraltextgen/RNN/')\n",
        "from RNNGenerator import RNNGenerator\n",
        "\n",
        "params = {\n",
        "    \"seq_size\": 512, \n",
        "    \"batch_size\": 32, \n",
        "    \"embedding_size\": 300, \n",
        "    \"lstm_size\": 128,\n",
        "    \"lstm_num_layers\": 3, \n",
        "    \"lstm_bidirectional\": True, \n",
        "    \"lstm_dropout\": 0.5, \n",
        "    \"gradients_norm\": 5,\n",
        "    \"predict_top_k\": 10, \n",
        "    \"training_epocs\": 300, \n",
        "    \"lr\": 0.01, \n",
        "    \"weights\": None\n",
        "}\n",
        "\n",
        "train_file = '../data/cleaned_tbc.5k.txt'\n",
        "generator = RNNGenerator(**params)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "trained_net = generator.train(device, train_file)\n",
        "\n",
        "# list of sentences\n",
        "sentences = generator.predict(device, trained_net, n_sentences=100)\n",
        "sentences"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size 8678\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Epoch: 24/300 Iteration: 100 Loss: 2.8042151927948\n",
            "Epoch: 49/300 Iteration: 200 Loss: 0.7751246690750122\n",
            "Epoch: 74/300 Iteration: 300 Loss: 0.19095322489738464\n",
            "Epoch: 99/300 Iteration: 400 Loss: 0.07505667209625244\n",
            "Epoch: 124/300 Iteration: 500 Loss: 0.039560843259096146\n",
            "Epoch: 149/300 Iteration: 600 Loss: 0.025733910501003265\n",
            "Epoch: 174/300 Iteration: 700 Loss: 0.017543671652674675\n",
            "Epoch: 199/300 Iteration: 800 Loss: 0.014400954358279705\n",
            "Epoch: 224/300 Iteration: 900 Loss: 0.011351017281413078\n",
            "Epoch: 249/300 Iteration: 1000 Loss: 0.009401139803230762\n",
            "Epoch: 274/300 Iteration: 1100 Loss: 0.007687651552259922\n",
            "Epoch: 299/300 Iteration: 1200 Loss: 0.006783031392842531\n",
            "75\n",
            "43\n",
            "27\n",
            "92\n",
            "36\n",
            "51\n",
            "82\n",
            "71\n",
            "68\n",
            "55\n",
            "13\n",
            "21\n",
            "69\n",
            "24\n",
            "56\n",
            "78\n",
            "31\n",
            "49\n",
            "78\n",
            "12\n",
            "35\n",
            "18\n",
            "17\n",
            "75\n",
            "0\n",
            "52\n",
            "24\n",
            "59\n",
            "20\n",
            "24\n",
            "31\n",
            "78\n",
            "-2\n",
            "84\n",
            "3\n",
            "49\n",
            "12\n",
            "9\n",
            "15\n",
            "14\n",
            "45\n",
            "80\n",
            "52\n",
            "58\n",
            "54\n",
            "58\n",
            "96\n",
            "62\n",
            "77\n",
            "50\n",
            "17\n",
            "63\n",
            "58\n",
            "18\n",
            "30\n",
            "0\n",
            "93\n",
            "49\n",
            "51\n",
            "39\n",
            "52\n",
            "23\n",
            "16\n",
            "91\n",
            "19\n",
            "10\n",
            "64\n",
            "2\n",
            "85\n",
            "26\n",
            "69\n",
            "1\n",
            "2\n",
            "94\n",
            "88\n",
            "67\n",
            "12\n",
            "92\n",
            "8\n",
            "44\n",
            "26\n",
            "81\n",
            "48\n",
            "25\n",
            "54\n",
            "30\n",
            "35\n",
            "75\n",
            "7\n",
            "33\n",
            "12\n",
            "5\n",
            "94\n",
            "36\n",
            "96\n",
            "30\n",
            "87\n",
            "28\n",
            "-1\n",
            "60\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['... mid-air to well take say never try at mine against save up tightly on before question right question after since be something all then watch can kill dare probably beside saw doing all yet and far were afternoon and lucas in ophella at bet against become from bet until pleasure dare st. youre bundled youre sigh youre distraught so distraught me distraught about distraught can yuell just afford so earth just landing youve hot how office ,',\n",
              " 'macbugall smiling said when said if rhys , wait for okay her number that dress is dress her place her under in as line as in about was something . me . him , you something do something ? something \" nothing he anything was',\n",
              " 'camera witnessed human tree figure without meeting except hours somehow coffee somehow hundred flash bowl taking older older school hours older tread hours older older women second human strands',\n",
              " \"playboy wiggleigh explained said ; said although well if humans because humans something crazy , violet , 'scaped as ash about shitkickers about peculiar something extinct anything exist not melanthe me melanthe about dina me us from around or dress dare around here away up off up off from asked about something else , nothing we something he like i anything i ... do again do know i mary he nick i guess ? fate . thats my might his choice his choice her inside me happened nothing beside help beside indeed beside anybody anybody\",\n",
              " \"locals lethal 'i very it uncle had made look seem hands open room made came straight night trouble ill dress back away went place back onto against onto turned after turned onto up at around straight look blow hand\",\n",
              " 'bandini throbbed } muttered wasnt muttered then listen then takin so laughing me deck here mooning me wait nothing ye about beifar for earlier to slowly her die is die and number and trouble all than all once right suit its than all held right pushed make shop right pushed from out over held',\n",
              " 'divide her might her might its needed holding might make to so me as nothing then say even help live nothing shes help jack help hardly anything remembering say drawing knew escaped here magnified now magnified now escaped onto lil ill jimmy into itself ill ethan kept people always feet giving pay walked riveted get cooking ill emptied never squarely or legally never dad by vampires kept woods seen bill kept old using an using . been he part , hell for breakfast is',\n",
              " 'eggs been drinking fell ronan center orange orange flash high human twenty-four except hours flash flash powerful flash light taking light coffee powerful hours flash taking hours taking strands coffee strands coffee flash hundred flash hours human hundred long flash high bowl coffee strands long long high story eyes taking nine long hours strands school human hundred hundred taking stems strands hours hours coffee taking stems hot hundred became strands coffee hours hours',\n",
              " 'rocked accept . please look please us \" looking \\' sister chloe married perhaps okay what okay that attention her please in offered straight passed already kicked mother herself follow ty \" weapons \" building may . should , theres for will is would how had how it how n\\'t that n\\'t her could me wont going hope up thats anybody hate care may feel \" almost stuff right reserved its',\n",
              " 'cams garion tell they wo when who when yes they suppose something max why rhys \" try . mine should it ! theres ! has ... him ... need like on for question is question straight question it since why ... knowing ... knowing want indeed will didnt should or i case do didnt did doing \"',\n",
              " 'pavement be hopkins the junction by loving of rusty . dark , dark that dark',\n",
              " \"latch walked n't mother to be is rest in the straight the rest his all take me anybody up behind up right turned this\",\n",
              " 'dislodged great dare military do smelled i christian does bet i doubt it doubt you removed when dog but thick but wore do quick they wasnt have wasnt how jared can process then following , jojo we jojo she wore how demanded , meet why hard knowing quickly can sorry we also , probably you mind because promise ... also since actually ... actually ... already where money happened calm hed alexia',\n",
              " \"samples smiling wondered perhaps sure perhaps knows whatever ; uncle , alex , realizing . funny ! hunter you though we jules she jules n't yet\",\n",
              " 'probability others past dad itself typing itself mentioning roaring after busy by women and busy his needle provided following brought beaten decks merciful letters within squarely ago destiny letters stand houses variations movements movements biocomponents painting himself-then himself-then frantically 9.00 within movements letters p.m. a.m. painting itself letters volumes nina threatening biocomponents 9.00 slowing threatening communication less nina',\n",
              " 'captain before meeting before quite : suddenly landing suddenly 7.30 and supports and strip too strip also meeting much pages meeting a.m. pages 7.30 decks 7.30 government towns government government decks towns school school 9.00 nina stand himself-then hours except powerful somehow letters stand stand threatening tread powerful nina except towns except hours except stand - stand within coffee threatening virtually stand hours letters coffee powerful human light hours hours powerful school coffee light except light powerful coffee hundred coffee',\n",
              " 'getting pathetic man eight man other minutes one elmer greater placed tears communication taking crackling orange flash flash decomposed hours long story except coffee except hundred long hours taking hundred except coffee flash except',\n",
              " 'erwans lack marnie charge a set our bit two pullover human human two two blocks two coffee hours taking strands hours hours coffee hot hours coffee coffee except hundred school strands strands long stand long seeds school coffee hundred seeds sound stand school except older hundred silver coffee light hundred hours strands',\n",
              " 'stones he ? was if he did he ca i soon you ca theres thought it loose do tell give loose do mom i may dont ca why matter hes eventually never matter zane who tell may hed always wanted got do suddenly could suddenly would barely you barely when actually when you ! because . if , especially . especially \" since who mean \" since mom else yes thinking john behind sam didnt cry since confidently ... yugo',\n",
              " 'grumbly eyes are either were better and either his heart is more and either',\n",
              " 'backward anymore also tell too let didnt see probably ever remember always never hate always matter id id mr who mr ? jack should imagine must you- well honestly well tightly want hunter if hard . once \"',\n",
              " 'taller main eyes taking became coffee human human human hours except older long letters long older hot flash hundred flash',\n",
              " \"tapped . yes . 'it i guess did fate but seriously when theres why has abby has really had\",\n",
              " \"hopped perhaps notion she dwarves n't 18 how pissed is planning her eat my eat her range its stomach making itself surprised bits quite blade been cloth both letters of variations of land next body each tail making bits right news me rane turning the home find doing right from back or take started me once about turning mean else about since about since why until james normally tell also hed once anybody also happen youre help\",\n",
              " 'talking strength about',\n",
              " 'escalade smiling else finally even she didnt how normally really started really son how started she started i started she taken she held have out you into you or we also we once i started should feel must feel must with because - you than we guys she guys but chris do bitch do',\n",
              " 'national stems long long hours long better human rain ( strands werent hot hours werent stems hours werent human long except strands except hundred long strands human',\n",
              " 'rejections action but she knows they if said knows are wonder you- much says quite probably secretly use bree always doubt much classic to classic for foot as eye , light then air even foot else stand else air else air left tears left coffee turning greater trouble human within hours grip greater pride taking crackling human a.m. ronan grow coffee bodies',\n",
              " 'deciding then said \\' they then then then said else said ; then while even knows even who are \" else i',\n",
              " 'madman mellie \\' yeah he nick my ca my \\'it look loose them \" right am all hes right keep care took right took right went me',\n",
              " 'swinging but \\' \" \" \\' matter he wonder she surely they figured i figured you n\\'t but \\'d but was what it . you look theres given it stay look down given',\n",
              " \"crisp emma off we home he or n't were n't and it and what this i right should almost should them ? stay ! lived would happen it going had me it ready has see hope need to need him knew on near on near to sat me found about well anything goodbye me trying him trying not meant go trying well well say hed help hed help or about use me or up just back just care even started didnt\",\n",
              " 'customers disappeared water',\n",
              " \"wolves smiling exactly finally 'you lets exactly come connor ever sebastian made vampires always people also deck probably seems mind na got drive ill smile kept getting through between weapons attack through grab into hand through step than ached from step home us from fire against standing on slip onto standing dare fire while wound inside between turning attack turning between turning hold left hanging right hanging under head right ronan youre placed didnt communication youre distraught too kissing were ocean or savanna than overtly trouble\",\n",
              " 'similarly neither am marshal thats',\n",
              " 'la lacey good long more human except long coffee hot hundred long long hundred long story bowl hundred human coffee years long hours hours coffee long silver strands hot hundred years strands hot human human hours hot stand hundred coffee la coffee flash hundred coffee volumes strands hot story human years',\n",
              " \"riverbank noed 's health provided excuse themselves excuse things started goes much distraught suddenly volumes\",\n",
              " 'scarred bread voice somehow sex hours better except taking older human older',\n",
              " 'vlad before her as is then for they like she will what for look would fight to',\n",
              " 'tale mists killed fear to letters me ground all advice and enough all himself me hard need',\n",
              " 'distract told has on now where everything where well question everything if mom , james . james look you- look finally fight am us im world am able open hand neither grab pick hold marry man open land no having been man night drink around fire back attack',\n",
              " 'scars smiling \" what \" what i \" it \" n\\'t matter what matter . he was he \\'d we really should miss you abby because have ... how since have until how from that to , in you in because be now the now \\'s onto and at all question at while into question back outside or dance started dance lips lucas names \\'devin separate goliath without communication without armed : 7.30 pink 7.30 government increasingly nina anesthetic a.m. increasingly merciful',\n",
              " \"thorns 's eyes were further and soft is awkward is combe and clubs at spread against air on casually and enveloped to assist in starting straight growing turns daisy looks air use stand to squarely to times dare air against pegs against pegs over further them pages all spread back spread ill pleasure back\",\n",
              " 'terrific crap even see they take think care you care because time ... time again stay here happen about along until away home along or say or behind care home all from from at behind and right at back to turned her back straight against turning over tightly them left them left coming took fingers hed get must hands well set',\n",
              " 'tourists for off will tightly will mine for save as close like today you tightly , else question behind else help something didnt said too if sorry because } you honestly ! unless should cycle they wonder finally remembering says casey have herself then hold something sense nothing hold here set home hold off seeing before',\n",
              " 'curl return look replied on on me about on from where all question from outside or question taken ; taken normally been hed get watch bed watch walked use both giving view trouble miles ) putting within ached trouble age to spring not nervous him virgil me appearing about clubs , handle something mostly , quick that wide her collar',\n",
              " 'ship intel finally giants sounded giants spoke gabriel spoke exist names volumes than grow : volumes quickly nina : 9.00 than painting theyre itself than drunk than ronan theyre somehow biocomponents spread 9.00 a.m. letters himself-then forgotten 9.00 forgotten letters placed letters placed 9.00 movements erupted armed biocomponents letters ago communication within frantically threatening stenciled classic armed powerful hundred nina hours holds stand himself-then letters communication within classic threatening nina hours except biocomponents nina letters within letters 8.30 tread armed biocomponents letters school volumes letters stand within threatening goose hours merciful stand movements stand variations hours 9.00 powerful merciful',\n",
              " \"bewildered , it for n't to was just im just sounds so sounds 's 's little sounds long list long rest complete make human so leaves and years me responsibility left hide turning drink left drink youre land about arms mean enveloped before stare quickly placed far communication macbugall holey embroidery letters dex exist except communication more 9.00 with ronan stopped theft 's times\",\n",
              " \"gloves kleenex usually finish : hurt without advice to together would opened to myself me across from combe about less for air about light for less is forest to restaurants and foot this further its light 's chest from sarah or blade from woman left paper me nervously him combe to all is from a son their time twin time sarahs down threatening us squarely called crashing us between look between around both us one feallengod tears chunk older 7.00\",\n",
              " 'mentally toilet took while where again now here ? nothing ? behind should behind must anybody well care nothing care about all something about because for maybe straight knowing unlike probably unlike ever anger sounded known losing someone have making then them how time how down to them to buy on talking from',\n",
              " 'taste mine running the hand this across almost system almost woman were times and tears all strip youre growing',\n",
              " 'legends dreamland away alone off exactly away through along earlier drifted shut wildly thoughts spill thoughts handed about left as from for from straight up it turned me back up room us into spill into them at them to pushed her opportunity in talking and stopped as making until talking here right here them home falling behind plans from them left stay left time took down',\n",
              " 'likely why gone you happen it along n\\'t happen it moved it happen n\\'t moved \\'d say am close neither close felt today was tightly was question was explained had explained would appear for exactly as everyone , dance something , , question when wherever you wherever when otherwise . doing \" slowly he answered n\\'t seat he enough ,',\n",
              " \"goodnight partially victoria do ? could wo could thats 'i am lets straight 'i you- 'i uncle may carlos hed live\",\n",
              " 'space is making straight talking with making this clean and growing his shirttail is the university no in hell her money is worth as order , corner about corner where lying here',\n",
              " 'hugged psylocybin everyone',\n",
              " 'above about anyone her shower is over to them him coming like settling will uriah will uriah took acting after cry ; rising ; rising : abs quickly threatening % vacuumthere jaclyns vacuumthere male 7.30 nina cent cent strange nina le le haba llamado cent llamado haba slouchy slouchy la except de except le hours de llamado silver silver llamado nina la silver except ( hot story except hours la tread silver silver la haba hours le hours cent hours except hours la hours hours stems except hot coffee volumes coffee hours hours stand (',\n",
              " 'movie mica tightly mine , say ! mine ! thinking because save theres saying give help maybe nothing him behind him from me over about time anything down something us like away give seemed theres fight theres told give dare has anybody it dare look wont seen hed never knew hed near',\n",
              " \"step-daughter kendrick sebastian slept spring appeared theyre typing theyre characters nearby drunk nearby ruled knowing standing didnt legally indeed itself knowing spring thoughts dad knowing lil why beckoned does herself why herself so checking how checking will than like or me just not and just in 'd line he be was looked n't see\",\n",
              " 'direction stammered \" yes smiling i you- ? honestly if pico when haman if soothingly \\' soothingly finally confidently \\' cruel ; sophie stopped cruel before drawing ; henry quickly exist quickly decks : threatening } male hapless % male nina de',\n",
              " 'fumble \" drunk \" august \" inside who thats thought dont who youre while so explained too after shock under thinking too thinking are saying \\'s saying were help - mine someone knowing been they quite said steady losing suddenly lucas without 100 suddenly 7.30 except goliath - government except gleaming law bodies government',\n",
              " 'parent dex divorce after tell dare tell after let youre happened so onto so here and doing me nothing behind about from ... behind ... youre',\n",
              " 'graybeard . she why she he but i she did but blake you hate do beneath it to',\n",
              " \"relationship ? offered if alive said warmly ' possible she possible i apparently but wondered i imagine i harlow but although do , do that do as give that you as ! 's because baby something were because still if bad knows cool , mate . cool i screams you mostly ! mostly ! ricky ... todayalive nothing closely nothing ricky knew closely leaving keeping led escaped up rebecca turned dallas turned communication down scandal down savagely enough ally together rolls enough dallas company itself enough often down placed over squarely over communications\",\n",
              " 'lodgings darwin shit commanded ruled past rustling upon merciful glass merciful merciful merciful nina machine 7.30 le nina machine hours trendy nina',\n",
              " 'minna heels push everyone who warmly table warmly thought questions ? \" if',\n",
              " 'robe rifle hope do but theres what theres that will really would was would was to in was straight straight it straight had blow took promise just always even never then or even care else right even them didnt time indeed pulled or stay were stay straight stay or us started called earlier down once called trouble up until on home looked mean on behind on me',\n",
              " 'compartmentalization what happens i',\n",
              " 'push s*x . look \" world neither run come laid it hold . man look cash them enjoyed almost forests this strands this tears were different were became by full been news follow news using content around dozens into seth than variations than all himself the turning its all the his this in case in them and done \\'s tale his clutch and propped all lived the role a folder shop answered shot wall later fatigue with writing more pledges with 9.00 this writing - mingling of wagon',\n",
              " 'coyote dreadful dont nodded . someone \" br**sts who firmly knows true if sweet because closely you william but , it that do as you about you can do',\n",
              " 'fresh wont guilt took the took at on at on at at ill night ill ill night take back hed around straight room . down straight over were from right about holding something left because make ? the ! the , a for firmly that confusion as next as making her shop that confusion her few for confusion for making , talking that son my sent is folded my fingers is',\n",
              " 'storm trees muttered because',\n",
              " 'doubted long hundred hundred',\n",
              " \"morgan needless john told do dare could hope youve so are 's too much so wind its much so taken me taken up them from over left home left home over until talking here stopped about under behind before off before off off mine along after slowly by doing into yet get trouble them outside taken wall much asleep falling asleep headline freak secretly variations movements letters letters lounge squarely letters goes advice placed off shuffling behind squarely behind swung from man or times home swung until arms since drink home times turning standing mine eggs the\",\n",
              " \"tilted as anything that think is said and isolated 're shoulder youre moaned about quietly not worry about today for close about happen about say , everything something happened ... tell because happened because here because where think onto why told abby onto me happened go here like about for ... for ... her since holding ... its since its home find doing stay else time turning happen off off here today now mine nothing thinking behind behind me here on again in again straight again : while a table\",\n",
              " 'nathaniel bulging yes . did why did abby mary in mary straight believe suddenly believe without trying quite yet both ears both things somehow exist aware nina hours government ( grip ( grip some double hundred decomposed hundred hundred second ( second hundred hundred hundred strands nina bodies bodies except nina remained grow without frustrated except hours became except except carrying story except sex somehow hundred except story coffee towns',\n",
              " 'putting mine ! everything , ready . now he does why do \" i',\n",
              " 'racing confidently daddy touched pay backwards stenciled sam backwards wherever , appeared since jerking until gathering question ryan else far before afternoon too though or there were sure by there the doorway by shoulder the moment his conversation and writing just relearns and relearns just luther to imrm for invade for uriah , ill-adapted for blindfolded is asked and mine \\'s love baby chloe \\'s \" little \" no does \" does am ? \" \" carlos may you- id literally tells } avoided henry na banana higher except nods much nods than sarah home',\n",
              " 'slip took told might never never always or others never',\n",
              " 'lay yanked still watched being worked still swatches too gabriel quickly mingling clan exist frantically within swatches frantically within communication communication exist 7.30 letters placed variations biocomponents powerful powerful chief letters volumes variations threatening nina himself-then biocomponents nina holds merciful merciful school school powerful volumes volumes',\n",
              " 'crazy beauties shit flew briefly mark a.m. 7.30 jaclyns 7.30 jaclyns la hundred slouchy slouchy silver cent le hours except llamado human strands long tread somehow strands hot la',\n",
              " 'clover what because that when , something , said , jules . worry , happens . well ? though i guess why unless why losing i remember you probably it or do care but or there take \" ill chloe always said much something without then quite said more have more felt more will more how more how more really been how been how doesnt just being and being as the as no about looks not with to with in each straight done',\n",
              " 'flood reconcile yep harrison none other way our way spent missed strands older straw stand homes stand coffee 9.00 except bowler merciful coffee powerful powerful bowler tears 9.00 merciful coffee hundred light stand stand coffee light older light light school merciful threatening school except powerful human hundred long second hundred tears',\n",
              " 'scuffle awful laughed addison over storming started casually - 7.00 too 7.30 were kissing were 7.00 still evolving : sharing : fair being millie somehow 7.30 aware supports',\n",
              " \"fergus mia neck but if there if said said they did you did they did we do why do knowing does remember they save she talking we raised she are they were then baby , stopped about before for behind for thinking about love anything 's maybe just why even i then should ' could dont thats\",\n",
              " \"refugees travelling what should really do perhaps you whatever ! 'now you assure you seriously you appear i answered i okay she okay . wished i alone it alone look happened us tell\",\n",
              " \"reaches ; riding : others a family first family a logjam a created - content - bits with content were created 's itself were strip or fair too fair home itself doing itself yet unfortunate trouble frantically\",\n",
              " 'pressures date carl long two long story hot silver coffee coffee stems except hundred hours coffee hot hours long second women hundred hot hours hundred silver hours hundred stems strands women hot silver volumes volumes except hours women hundred la hours hours hundred hundred la hot long long two hundred coffee hundred coffee long coffee hours women hundred hundred la except hours long second human strands women except hours coffee women hundred hot volumes hundred ( silver hundred',\n",
              " 'kcc after if \" there . mom he everything',\n",
              " 'wan de tree hundred hours ( women long years two hundred except two coffee coffee hours stems coffee long silver hot coffee hours women women hundred silver human human except goose rain sex women coffee',\n",
              " \"twenty-six-year-old pizza stay 's need its from holding at love into straight against melted\",\n",
              " 'madman hulk why wait look hurt us slowly',\n",
              " 'sentence having \" hell \" get am night he ill we never we after you tell theres let could say they mine she thinking but must do does do let did tell do needed did dare do hed give didnt me also going or me or from them me falling on calvin in heaven in cover is intercepted to cold took escape will gus for holds her area my exercising a shoulders with chilled more office with full into strands with david of deal - news - sound with kind of dozens more content - strands',\n",
              " \"nuts 's grin 's there 's mary youre believe indeed believe home everything home now doing while behind exactly else helping left forever says bringing left ground mean settling as chris , coming we plus it plus do\",\n",
              " 'twenty-five lawnmower boy ( boy old reasons lighter rustling m. rustling de variations goose stand except tread erupted bumps goose merciful erupted light older less within less 9.00 merciful hours tread ( tread merciful nina stand except goose erupted except erupted hot somehow nina nina nina werent except remained women hundred nina merciful erupted goose breezed hours tread merciful nina tread notices women tread except goose werent tread except merciful except tread werent goose stand within tread within older goose hours tread except werent merciful rain powerful except tread werent hours hundred school nina werent rain somehow goose rain',\n",
              " 'ahead nor else quickly nothing william too smells after drawing ill lucas at legally all pissing and beckoned all deeper be fear were fear were messing with ronan stopped pages almost tears',\n",
              " \"leading , while why while abby . why what it really n't really he felt we perhaps can felt then said how well 'd go miss felt be rest all rest in jesus his wrist 's lady its very making green br**sts sounds such very such * : rusty and * or red and iron just clothing so smart so wounds about finn about hard where shouting now grace ? john should muttered can muttered just sam can max how wait can try just tell and never be always\",\n",
              " \"another really coffee perhaps eyes are more ' a perhaps held felt simply go held thoughts much going held say out everything to me is behind , mean about behind\",\n",
              " 'straighter wife playing',\n",
              " 'snows teachers who stopped while before explained tightly dress harrison checking vic forever soon simply looking earlier us guys away slowly dress drawing away sailing away mooning seemed soon away groan here everyone home everyone mean ian behind wine behind wine me stomping him illegal it pissing why trust after father its family onto father where standing where deeper onto lit onto herself']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YY4uDvlvb3Ba"
      },
      "source": [
        "# Save text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_23KyvIibx5G"
      },
      "source": [
        "os.chdir('/content/')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmPxfTqGbHJp"
      },
      "source": [
        "! touch RNN_generated.txt"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zO-TOb9QbUG9"
      },
      "source": [
        "with open(\"RNN_generated_TBC.txt\", \"w\") as text_file:\n",
        "  for sentence in sentences:\n",
        "    text_file.write(sentence + '\\n')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B24_2ICZcC8b"
      },
      "source": [
        "# Evaluate text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRyaU8FScCSz"
      },
      "source": [
        "%%capture\n",
        "!pip install -r /content/neuraltextgen/texygen/requirements.txt"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWfltgiocsl0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8b4f775-5f02-4cd5-db19-1e8bccc60097"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import sys, os\n",
        "import os\n",
        "os.chdir(\"/content/neuraltextgen/texygen\")\n",
        "from utils.metrics.Bleu import Bleu\n",
        "from utils.metrics.SelfBleu import SelfBleu\n",
        "\n",
        "os.chdir(\"/content\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzWz-M-kdSqo"
      },
      "source": [
        "from nltk.translate import bleu_score as bleu\n",
        "\n",
        "def prepare_data(data_file, replacements={}, uncased=True):\n",
        "    data = [d.strip().split() for d in open(data_file, 'r').readlines()]\n",
        "    if uncased:\n",
        "        data = [[t.lower() for t in sent] for sent in data]\n",
        "        \n",
        "    for k, v in replacements.items():\n",
        "        data = [[t if t != k else v for t in sent] for sent in data]\n",
        " \n",
        "    return data\n",
        "\n",
        "def prepare_wiki(data_file, uncased=True):\n",
        "    replacements = {\"@@unknown@@\": \"[UNK]\"}\n",
        "    return prepare_data(data_file, replacements=replacements, uncased=uncased)\n",
        "\n",
        "def prepare_tbc(data_file):        \n",
        "    replacements = {\"``\": \"\\\"\", \"\\'\\'\": \"\\\"\"}\n",
        "    return prepare_data(data_file, replacements=replacements)\n",
        "\n",
        "def corpus_bleu(generated, references):\n",
        "    \"\"\" Compute similarity between two corpora as measured by\n",
        "    comparing each sentence of `generated` against all sentences in `references` \n",
        "    \n",
        "    args:\n",
        "        - generated (List[List[str]]): list of sentences (split into tokens)\n",
        "        - references (List[List[str]]): list of sentences (split into tokens)\n",
        "        \n",
        "    returns:\n",
        "        - bleu (float)\n",
        "    \"\"\"    \n",
        "    return bleu.corpus_bleu([references for _ in range(len(generated))], generated)\n",
        "    \n",
        "wiki103_file = './neuraltextgen/data/wiki103.5k.txt'\n",
        "tbc_file = './neuraltextgen/data/tbc.5k.txt'\n",
        "\n",
        "wiki_data = prepare_wiki(wiki103_file)\n",
        "tbc_data = prepare_tbc(tbc_file)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6rsi2NEc6U0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dee10d88-9e72-4fa6-a673-7b34b17bed83"
      },
      "source": [
        "file_path = '/content/RNN_generated_TBC.txt'\n",
        "bleu_score_tbc = Bleu(file_path, tbc_file)\n",
        "bleu_score_wiki = Bleu(file_path, wiki103_file)\n",
        "\n",
        "print(\"(Texygen) BERT-TBC BLEU: %.2f\" % (100 * bleu_score_tbc.get_bleu()))\n",
        "print(\"(Texygen) BERT-Wiki103 BLEU: %.2f\" % (100 * bleu_score_wiki.get_bleu()))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Texygen) BERT-TBC BLEU: 7.70\n",
            "(Texygen) BERT-Wiki103 BLEU: 4.78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nf3S7kQHiWw",
        "outputId": "5be63766-19e4-4939-8dad-77c991fb11d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "self_bleu_score = SelfBleu(file_path)\n",
        "\n",
        "print(\"(Texygen) BERT- SelfBLEU: %.2f\" % (100 * self_bleu_score.get_bleu_parallel())) ## Expected results"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Texygen) BERT- SelfBLEU: 7.45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrOZVnlJHsKk",
        "outputId": "e0a84365-acc0-4093-abbf-0d4cb2355be9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        }
      },
      "source": [
        "os.chdir(\"/content/neuraltextgen/texygen\")\n",
        "from utils.metrics.UniqueGram import UniqueGram\n",
        "from utils.metrics.Bleu import Bleu\n",
        "\n",
        "path = \"/content/neuraltextgen/data/tbc.5k.txt\"\n",
        "file = open(path, \"r\")\n",
        "tbc = file.readlines()\n",
        "\n",
        "path = \"/content/neuraltextgen/data/wiki103.5k.txt\"\n",
        "file = open(path, \"r\")\n",
        "wiki = file.readlines()\n",
        "\n",
        "from nltk.util import ngrams\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=\"uncased\" in \"bert-base-uncased\")\n",
        "\n",
        "def getGrams(sents, n):\n",
        "  grams = []\n",
        "  for line in sents:\n",
        "    line = tokenizer.convert_tokens_to_ids(line.split(\" \"))\n",
        "    grams += UniqueGram(gram=n).get_gram(line)\n",
        "  dictGrams = Counter(grams)\n",
        "  return dictGrams\n",
        "\n",
        "def compareUniqueGrams(pred_ngrams, ref_ngrams, max_n):\n",
        "  pct_unique={}\n",
        "  for i in range(2, max_n + 1):\n",
        "    pred_ngram_counts = set(pred_ngrams[i].keys())\n",
        "    total = sum(pred_ngrams[i].values())\n",
        "    ref_ngram_counts = set(ref_ngrams[i].keys())\n",
        "    pct_unique[i] = len(pred_ngram_counts.difference(ref_ngram_counts)) / total\n",
        "\n",
        "  return pct_unique\n",
        "\n",
        "def selfUniqueGrams(pred_ngrams, max_n):\n",
        "  pct_unique={}\n",
        "  for i in range(2, max_n+1):\n",
        "    n_unique = len([k for k, v in pred_ngrams[i].items() if v == 1])\n",
        "    total = sum(pred_ngrams[i].values())\n",
        "    pct_unique[i] = n_unique/total\n",
        "\n",
        "  return pct_unique\n",
        "\n",
        "\n",
        "maxGrams = 4\n",
        "wikiGrams={}\n",
        "tbcGrams={}\n",
        "for i in range(2, maxGrams+1):\n",
        "  wikiGrams[i] = getGrams(wiki, i)\n",
        "  tbcGrams[i] = getGrams(tbc, i)\n",
        "\n",
        "import itertools\n",
        "\n",
        "#supporting function\n",
        "def _split_into_words(sentences):\n",
        "  \"\"\"Splits multiple sentences into words and flattens the result\"\"\"\n",
        "  return list(itertools.chain(*[_.split(\" \") for _ in sentences]))\n",
        "\n",
        "#supporting function\n",
        "def _get_word_ngrams(n, sentences):\n",
        "  \"\"\"Calculates word n-grams for multiple sentences.\n",
        "  \"\"\"\n",
        "  assert len(sentences) > 0\n",
        "  assert n > 0\n",
        "\n",
        "  words = _split_into_words(sentences)\n",
        "  return _get_ngrams(n, words)\n",
        "\n",
        "#supporting function\n",
        "def _get_ngrams(n, text):\n",
        "  \"\"\"Calcualtes n-grams.\n",
        "  Args:\n",
        "    n: which n-grams to calculate\n",
        "    text: An array of tokens\n",
        "  Returns:\n",
        "    A set of n-grams\n",
        "  \"\"\"\n",
        "  ngram_set = set()\n",
        "  text_length = len(text)\n",
        "  max_index_ngram_start = text_length - n\n",
        "  for i in range(max_index_ngram_start + 1):\n",
        "    ngram_set.add(tuple(text[i:i + n]))\n",
        "  return ngram_set\n",
        "\n",
        "def rouge_n(reference_sentences, evaluated_sentences, n=2):\n",
        "  \"\"\"\n",
        "  Computes ROUGE-N of two text collections of sentences.\n",
        "  Source: http://research.microsoft.com/en-us/um/people/cyl/download/\n",
        "  papers/rouge-working-note-v1.3.1.pdf\n",
        "  Args:\n",
        "    evaluated_sentences: The sentences that have been picked by the summarizer\n",
        "    reference_sentences: The sentences from the referene set\n",
        "    n: Size of ngram.  Defaults to 2.\n",
        "  Returns:\n",
        "    recall rouge score(float)\n",
        "  Raises:\n",
        "    ValueError: raises exception if a param has len <= 0\n",
        "  \"\"\"\n",
        "  if len(evaluated_sentences) <= 0 or len(reference_sentences) <= 0:\n",
        "    raise ValueError(\"Collections must contain at least 1 sentence.\")\n",
        "\n",
        "  evaluated_ngrams = _get_word_ngrams(n, evaluated_sentences)\n",
        "  reference_ngrams = _get_word_ngrams(n, reference_sentences)\n",
        "  reference_count = len(reference_ngrams)\n",
        "  evaluated_count = len(evaluated_ngrams)\n",
        "\n",
        "  # Gets the overlapping ngrams between evaluated and reference\n",
        "  overlapping_ngrams = evaluated_ngrams.intersection(reference_ngrams)\n",
        "  overlapping_count = len(overlapping_ngrams)\n",
        "\n",
        "  # Handle edge case. This isn't mathematically correct, but it's good enough\n",
        "  if evaluated_count == 0:\n",
        "    precision = 0.0\n",
        "  else:\n",
        "    precision = overlapping_count / evaluated_count\n",
        "\n",
        "  if reference_count == 0:\n",
        "    recall = 0.0\n",
        "  else:\n",
        "    recall = overlapping_count / reference_count\n",
        "\n",
        "  f1_score = 2.0 * ((precision * recall) / (precision + recall + 1e-8))\n",
        "\n",
        "  #just returning recall count in rouge, useful for our purpose\n",
        "  return [precision,recall,f1_score]\n",
        "\n",
        "file = open(path, \"r\")\n",
        "pred = file.readlines()\n",
        "\n",
        "modelGrams = {}\n",
        "for i in range(2, maxGrams+1):\n",
        "  modelGrams[i] = getGrams(pred, i)\n",
        "\n",
        "pct_uniques_self = selfUniqueGrams(modelGrams, maxGrams)\n",
        "pct_uniques_wiki = compareUniqueGrams(modelGrams, wikiGrams, maxGrams)\n",
        "pct_uniques_tbc = compareUniqueGrams(modelGrams, tbcGrams, maxGrams)\n",
        "\n",
        "rougeWiki = []\n",
        "rougeTBC = []\n",
        "for k in range(1,5):\n",
        "  rougeWiki += rouge_n(wiki, pred, n=k)\n",
        "  rougeTBC += rouge_n(wiki, pred, n=k)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-de59af4b63a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mngrams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert-base-uncased\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_lower_case\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"uncased\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m\"bert-base-uncased\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetGrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'AutoTokenizer' is not defined"
          ]
        }
      ]
    }
  ]
}