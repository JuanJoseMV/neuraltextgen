{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GCNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOCwnjBpicIQCsiPPkSVyqM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JuanJoseMV/neuraltextgen/blob/main/GCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mlbru8NAj1UI"
      },
      "source": [
        "%%capture\n",
        "#Install pytorch and fastai (see https://docs.fast.ai/)\n",
        "!conda update conda --yes\n",
        "!conda install -c pytorch pytorch-nightly cuda90 --yes\n",
        "!conda install -c fastai torchvision-nightly --yes\n",
        "!conda install -c fastai fastai --yes\n",
        "!conda install -c anaconda jupyter unzip cython cupy seaborn --yes\n",
        "#!pip install GCNN_textfuncs\n",
        "#!git clone --recursive https://github.com/DavidWBressler/GCNN"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-BxhbtF1nd1"
      },
      "source": [
        "from fastai import *\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data.sampler import Sampler\n",
        "import numpy as np\n",
        "\n",
        "#create adaptive-softmax language-model dataset\n",
        "class LMDataset_GCNN(Dataset):\n",
        "    def __init__(self, tokens):\n",
        "        self.tokens=tokens\n",
        "    def __getitem__(self,index):\n",
        "        #token_list=torch.FloatTensor(self.tokens[index]).cuda()\n",
        "        token_list=torch.LongTensor(self.tokens[index])\n",
        "        label=torch.FloatTensor([1])\n",
        "        #label=torch.ones(len(token_list)-1).float()\n",
        "        return token_list,label\n",
        "    def __len__(self):\n",
        "        return len(self.tokens)\n",
        "    \n",
        "    \n",
        "class SortSampler_GCNN(Sampler): #inspired by fast.ai sortsampler... pass in something like key=lambda x: len(val_clas[x])\n",
        "    def __init__(self, data_source, key): self.data_source,self.key = data_source,key\n",
        "    def __len__(self): return len(self.data_source)\n",
        "    def __iter__(self):\n",
        "        return iter(sorted(range(len(self.data_source)), key=self.key, reverse=True))#return iterator in reverse order, sorted by input key (e.g. length)\n",
        "    \n",
        "#this sortishsampler does the following:\n",
        "    # 1) get a list of randomized indices of length of entire dataset\n",
        "    # 2) break that into a list of sublists, each sublist of size bs*50\n",
        "    # 3) create a new list that is sorted within each of those chunks\n",
        "    # 4) break that sorted list into chunks of size bs\n",
        "    # 5) create new list by randomizing order of all those bs-chunks (with bs-chunk w/ largest key first)\n",
        "#this will give batches sorted by key (e.g. length) within the batch\n",
        "class SortishSampler_GCNN(Sampler): #inspired by fast.ai sortishsampler... pass in something like key=lambda x: len(val_clas[x])\n",
        "    def __init__(self, data_length, key,bs): self.data_length,self.key,self.bs = data_length,key,bs\n",
        "    def __len__(self): return self.data_length\n",
        "    def __iter__(self):\n",
        "        idxs = np.random.permutation(self.data_length)#random permutation of length of entire dataset\n",
        "        sz = self.bs*50 #chunk size is bs*50\n",
        "        #range(0, len(idxs), sz) : go through length of entire dataset, with stepsize=chunk_size\n",
        "        #idxs[i:i+sz] :within that chunk's range, get all the indices of the random permutation above\n",
        "        #this creates a list of lists... basically just splitting up idxs into a bunch of chunks\n",
        "        ck_idx = [idxs[i:i+sz] for i in range(0, len(idxs), sz)]\n",
        "        #for s in ck_idx: go through each sublist in the big list\n",
        "        #sorted(s, key=self.key, reverse=True): sort the sublist in reverse order according to the key (e.g. length)\n",
        "        #np.concatenate: concatenate all the sorted chunk sublists together\n",
        "        sort_idx = np.concatenate([sorted(s, key=self.key, reverse=True) for s in ck_idx])\n",
        "        sz = self.bs #now set size to bs\n",
        "        #similar as before, this creates a list of lists, splitting up sort_idx into chunks of size bs\n",
        "        ck_idx = [sort_idx[i:i+sz] for i in range(0, len(sort_idx), sz)]\n",
        "        # go through each bs-chunk, get the key of the first entry (which should be the largest of the chunk)...\n",
        "        # then do argmax to find the chunk with the largest key\n",
        "        max_ck = np.argmax([self.key(ck[0]) for ck in ck_idx])  \n",
        "        #switch spots bw the first chunk and the chunk w/ the max key\n",
        "        ck_idx[0],ck_idx[max_ck] = ck_idx[max_ck],ck_idx[0]\n",
        "        #now randomize the order of all the bs-chunks (except the first), then concatenate together\n",
        "        sort_idx = np.concatenate(np.random.permutation(ck_idx[1:]))\n",
        "        sort_idx = np.concatenate((ck_idx[0], sort_idx))# concatenate the first (largest key val) chunk to the rest\n",
        "        return iter(sort_idx)\n",
        "\n",
        "#inspired by fast.ai's pad_collate\n",
        "def pad_collate_GCNN(samples, pad_idx=1):\n",
        "    #go through all the sentences (found in s[0]), find length of longest\n",
        "    max_len = max([len(s[0]) for s in samples]) \n",
        "    #create a tensor of size [max_len,n_samples], and set all values to pad_idx\n",
        "    res = torch.zeros(max_len, len(samples)).long() + pad_idx \n",
        "    #for each line in res, set so corresponding sentence is aligned to the left edge\n",
        "        #(right-padded: keep padding on the right edge)\n",
        "    for i,s in enumerate(samples): res[:len(s[0]),i] = LongTensor(s[0]) #right-padded\n",
        "    #return res as the padded tensor, and another tensor composed of the labels (found in s[1])\n",
        "    return res.cuda(), torch.FloatTensor(np.array([s[1] for s in samples])).squeeze().cuda()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybXY7O2Qj820"
      },
      "source": [
        "from fastai import *\n",
        "from fastai.text import * \n",
        "\n",
        "import torch.utils.data as data_utils\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data.sampler import Sampler\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import time\n",
        "import importlib\n",
        "import seaborn as sns\n",
        "\n",
        "#os.chdir(\"/content/GCNN/\")\n",
        "#from GCNN_textfuncs import *"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJvluSt8j80f"
      },
      "source": [
        "DATAPATH = Path('/data/GCNN/')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jM2-4YAFj8yK"
      },
      "source": [
        "sns.set() #set graph formatting to seaborn"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1-x6t2kj8wB",
        "outputId": "256e7318-c2a5-42ed-bb8b-28f71c3b6ef8"
      },
      "source": [
        "#download wikitext-2 dataset and GloVe embeddings\n",
        "!wget https://s3.amazonaws.com/fast-ai-nlp/wikitext-2.tgz -P /data\n",
        "!tar xzf /data/wikitext-2.tgz -C /data\n",
        "!mv /data/wikitext-2/ /data/GCNN/\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip -P /data/GCNN/\n",
        "!unzip /data/GCNN/glove.6B.zip -d /data/GCNN/"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-27 17:23:12--  https://s3.amazonaws.com/fast-ai-nlp/wikitext-2.tgz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.166.77\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.166.77|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4070055 (3.9M) [application/x-tar]\n",
            "Saving to: ‘/data/wikitext-2.tgz’\n",
            "\n",
            "wikitext-2.tgz      100%[===================>]   3.88M  5.94MB/s    in 0.7s    \n",
            "\n",
            "2021-08-27 17:23:13 (5.94 MB/s) - ‘/data/wikitext-2.tgz’ saved [4070055/4070055]\n",
            "\n",
            "--2021-08-27 17:23:13--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-08-27 17:23:14--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-08-27 17:23:14--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘/data/GCNN/glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.13MB/s    in 2m 41s  \n",
            "\n",
            "2021-08-27 17:25:56 (5.09 MB/s) - ‘/data/GCNN/glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  /data/GCNN/glove.6B.zip\n",
            "  inflating: /data/GCNN/glove.6B.50d.txt  \n",
            "  inflating: /data/GCNN/glove.6B.100d.txt  \n",
            "  inflating: /data/GCNN/glove.6B.200d.txt  \n",
            "  inflating: /data/GCNN/glove.6B.300d.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAe79u-yj8tt"
      },
      "source": [
        "#Do some preprocessing of the data:\n",
        "\n",
        "#put data into df's w/ columns for 'labels' and 'text'\n",
        "df_trn = pd.read_csv(DATAPATH/'train.csv',header=None,names=['text'])\n",
        "df_test = pd.read_csv(DATAPATH/'test.csv',header=None,names=['text'])\n",
        "df_trn['labels']=0\n",
        "df_test['labels']=0\n",
        "df_trn=df_trn[['labels','text']]\n",
        "df_test=df_test[['labels','text']]\n",
        "\n",
        "#split data into paragraphs, then remove paragraphs <10 or >300 words\n",
        "trn_paragraphs=[]\n",
        "for docnum in range(len(df_trn)):\n",
        "    trn_paragraphs.extend([x for x in df_trn.iloc[docnum].text.split('\\n')])\n",
        "trn_paragraphs.sort(key=len)\n",
        "trn_paragraphs=[par for par in trn_paragraphs if (len(par.split(' '))<300 and len(par.split(' '))>10)]#remove paragraphs >300 and <10 words\n",
        "trn_paragraphs=[par+'xxeos ' for par in trn_paragraphs] #add EOS token at end of each paragraph\n",
        "\n",
        "test_paragraphs=[]\n",
        "for docnum in range(len(df_test)):\n",
        "    test_paragraphs.extend([x for x in df_test.iloc[docnum].text.split('\\n')])\n",
        "test_paragraphs.sort(key=len)\n",
        "test_paragraphs=[par for par in test_paragraphs if (len(par.split(' '))<300 and len(par.split(' '))>10)]#remove paragraphs >300 and <10 words\n",
        "test_paragraphs=[par+'xxeos ' for par in test_paragraphs] #add EOS token at end of each paragraph\n",
        "\n",
        "#put data into csv's\n",
        "df_trn_par = pd.DataFrame({'text':trn_paragraphs})\n",
        "df_test_par = pd.DataFrame({'text':test_paragraphs})\n",
        "\n",
        "df_trn_par['labels']=0\n",
        "df_test_par['labels']=0\n",
        "df_trn_par=df_trn_par[['labels','text']]\n",
        "df_test_par=df_test_par[['labels','text']]\n",
        "\n",
        "df_trn_par.to_csv(DATAPATH/'train_proc_par2.csv', header=False, index=False)\n",
        "df_test_par.to_csv(DATAPATH/'test_proc_par2.csv', header=False, index=False)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7DfSNRnmHqJ"
      },
      "source": [
        "# Create modeler class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYtKElGQj8rX"
      },
      "source": [
        "class modeler():\n",
        "    def __init__(self,trn_dl,val_dl,module,modelvals=None):\n",
        "        self.trn_dl, self.val_dl, self.module = trn_dl, val_dl, module\n",
        "        self.modelvals=modelvals\n",
        "        self.model=self.module.cuda()\n",
        "    def model_fit(self):\n",
        "        samp_n=self.modelvals['samp_n']#the number of iterations in an epoch\n",
        "        starttime=time.time()\n",
        "        train_loss_list=[]; val_loss_list=[]\n",
        "        for epoch in range(0, self.modelvals['epochs']):\n",
        "            pbar=0#progressbar\n",
        "            for batch_idx, (data, target) in enumerate(self.trn_dl):\n",
        "                \n",
        "                #GRAB MINIBATCH OF INPUTS AND TARGETS, SET OPTIMIZER\n",
        "                data = Variable(data)\n",
        "                pbar+=self.modelvals['bs'] #how many iterations have we done in the epoch so far\n",
        "                if self.modelvals['opttype']=='sgd':\n",
        "                    self.optimizer = optim.SGD(self.model.parameters(), lr=self.modelvals['lr'], \n",
        "                                       momentum=self.modelvals['mom'], weight_decay=self.modelvals['wd'],\n",
        "                                              nesterov=self.modelvals['nesterov'])\n",
        "                elif self.modelvals['opttype']=='adam':\n",
        "                    self.optimizer = optim.Adam(self.model.parameters(), lr=self.modelvals['lr'], \n",
        "                                        betas=(self.modelvals['mom'], 0.999))\n",
        "                self.optimizer.zero_grad()\n",
        "                \n",
        "                #FORWARD PASS\n",
        "                output = self.model(data)\n",
        "                \n",
        "                #CALCULATE AND BACKPROP THE LOSS\n",
        "                loss= output.loss\n",
        "                loss.backward()\n",
        "                if self.modelvals['grad_clip']!=0: #gradient clipping\n",
        "                    torch.nn.utils.clip_grad_value_(self.model.parameters(), self.modelvals['grad_clip'])\n",
        "                    \n",
        "                #UPDATE THE WEIGHTS\n",
        "                self.optimizer.step()\n",
        "                \n",
        "                #PRINT OUT TRAINING UPDATES\n",
        "                train_loss_list.append([epoch,pbar+epoch*samp_n,loss.data.item(),self.modelvals['lr']])\n",
        "                if batch_idx % 100 == 0:\n",
        "                    elapsed_time=time.time()-starttime\n",
        "                    train_update_format_string = 'Train Epoch: {}'\n",
        "                    train_update_format_string += '\\tTotal_its: {:.2f}M [{:.2f}M/{:.2f}M]'\n",
        "                    train_update_format_string += '\\tPercdone: {:.2f}'\n",
        "                    train_update_format_string += '\\tLoss: {:.4f}'\n",
        "                    train_update_format_string += '\\tTime: {:.2f}'\n",
        "                    train_update_format_string += '\\tLR: {:.4f}'\n",
        "                    train_update_string=train_update_format_string.format(\n",
        "                            epoch,\n",
        "                            (pbar + epoch * samp_n) / 1000000, pbar / 1000000, samp_n / 1000000,\n",
        "                            pbar / samp_n,\n",
        "                            loss.data.item(),\n",
        "                            elapsed_time / 60,\n",
        "                            self.modelvals['lr'])\n",
        "                    print(train_update_string)\n",
        "            final_train_loss=loss.data.item()\n",
        "            \n",
        "            #NOW TEST VALIDATION SET\n",
        "            val_loss=[]\n",
        "            self.model.eval() #important to set to eval mode for testing, so that eg batchnorm and dropout aren't used\n",
        "            for batch_idx, (data, target) in enumerate(self.val_dl):\n",
        "                data = Variable(data)\n",
        "                self.optimizer.zero_grad()\n",
        "                #ONLY NEED FORWARD PASS... NO BACKPROP\n",
        "                output = self.model(data)\n",
        "                loss= output.loss\n",
        "                output=output.output\n",
        "                val_loss.append(loss.data.item())\n",
        "            self.model.train() #set back to training mode\n",
        "            ave_val_loss=sum(val_loss) / len(val_loss)\n",
        "            val_update_string='Validation Loss: {:.4f}\\tPerp: {:.4f}'.format(\n",
        "                ave_val_loss,np.exp(ave_val_loss))\n",
        "            print(val_update_string)\n",
        "            val_loss_list.append([epoch,ave_val_loss, np.exp(ave_val_loss),elapsed_time/60])\n",
        "        self.modelvals['val_loss_list']=val_loss_list\n",
        "        self.modelvals['train_loss_list']=train_loss_list\n",
        "        print('The end! {:.2f} minutes'.format((time.time()-starttime)/60))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FDO7AL7mQyE"
      },
      "source": [
        "# Set hyperparameters and build embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzmkMZ1imQnL"
      },
      "source": [
        "#set hyperparameters\n",
        "bs=50 #batch-size\n",
        "emb_sz=300 #size of the embedding matrix\n",
        "nl=4 #number of layers\n",
        "nh=600 #number hidden units\n",
        "lr=1 #learning rate\n",
        "mom=.95 #momentum\n",
        "wd=5e-5 #weight-decay. Only has effect if opttype==sgd\n",
        "epochs=50\n",
        "nesterov=True #Nesterov momentum. only has effect if opttype==sgd\n",
        "grad_clip=0.07 #gradient clipping value. Set to 0 for no effect. See nn.utils.clip_grad_value_\n",
        "opttype='sgd' #adam, sgd\n",
        "k=4 #kernel_width\n",
        "downbot=20# in the bottleneck layers, how much to decrease channel depth?"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "Q6-YdUJ_j8o4",
        "outputId": "c1a84098-e73f-4ecf-e283-aeee41295e92"
      },
      "source": [
        "#Use fast.ai to create a TextLMDataBunch object. See http://docs.fast.ai/text.data.html#class-textlmdatabunch\n",
        "#This tokenizes and numericalizes the data\n",
        "data_lm = TextLMDataBunch.from_csv(path=DATAPATH, csv_name='train_proc_par2.csv', test='test_proc_par2.csv')\n",
        "itos=data_lm.train_ds.vocab.itos# the vocab\n",
        "vs=len(itos)# vs is the length of the vocab\n",
        "\n",
        "#Grab the numericalized data from the TextLMDataBunch dataset, then construct new custom dataset using LMDataset_GCNN\n",
        "trn_tokens=[data_lm.train_ds[i][0].data for i in range(len(data_lm.train_ds))]\n",
        "traindataset=LMDataset_GCNN(trn_tokens)\n",
        "valid_tokens=[data_lm.valid_ds[i][0].data for i in range(len(data_lm.valid_ds))]\n",
        "validdataset=LMDataset_GCNN(valid_tokens)\n",
        "\n",
        "#Create data loaders for training and validation sets\n",
        "trn_samp=SortishSampler_GCNN(data_length=len(traindataset),key=lambda x:len(traindataset[x][0]), bs=bs)\n",
        "val_samp=SortSampler_GCNN(validdataset,key=lambda x:len(validdataset[x][0]))\n",
        "train_loader = data_utils.DataLoader(traindataset, batch_size=bs, collate_fn=pad_collate_GCNN, sampler=trn_samp)\n",
        "val_loader = data_utils.DataLoader(validdataset, batch_size=bs, collate_fn=pad_collate_GCNN,sampler=val_samp)\n",
        "samp_n=len(traindataset)\n",
        "val_samp_n=len(validdataset)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/fastai/core.py:302: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return np.array(a, dtype=dtype, **kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJDjwrBgj8mY"
      },
      "source": [
        "#put hyperparameters into a dictionary\n",
        "def get_modelvals():\n",
        "    modelvals=dict((name,eval(name)) for name in [\n",
        "        'lr','mom','wd','opttype','epochs','samp_n','val_samp_n',\n",
        "        'bs','emb_sz','vs', 'nh', 'nl','DATAPATH','nesterov','grad_clip',\n",
        "        'k','downbot'] )\n",
        "    return modelvals\n",
        "\n",
        "modelvals=get_modelvals()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Awtzdk7aj8jj"
      },
      "source": [
        "#grab GloVe embeddings:\n",
        "#create vocab itos2 from downloaded glove file\n",
        "words = []\n",
        "idx = 0\n",
        "word2idx = {}\n",
        "vectors = []\n",
        "with open('/data/GCNN/glove.6B.300d.txt', 'rb') as f:\n",
        "    for l in f:\n",
        "        line = l.decode().split()\n",
        "        word = line[0]\n",
        "        words.append(word)\n",
        "        word2idx[word] = idx\n",
        "        idx += 1\n",
        "        vectors.append(line[1:])\n",
        "itos2=words\n",
        "\n",
        "#grab the glove embeddings we need, based on the words in our vocab\n",
        "stoi2 = collections.defaultdict(lambda:-1, {v:k for k,v in enumerate(itos2)}) #default -1 means its not in glove's itos2\n",
        "row_m = vectors[-1] #this is default vector... for <unk>\n",
        "new_w = np.zeros((vs, emb_sz), dtype=np.float32)#initialize new weights to zeros of size (vocab_size,embedding size) e.g. (60002,300)... we're creating an embedding matrix \n",
        "for i,w in enumerate(itos): #for index,word in our itos dict, get r index of the word in word2vec's dict. r will be -1 if it doesn't exist in word2vec's dict\n",
        "    r = stoi2[w]#r index of the word in word2vec's dict\n",
        "    new_w[i] = vectors[r] if r>=0 else row_m #for our new embedding matrix, set the embedding at the index from our dict equal to the embedding from index r from word2vec's dict\n",
        "np.save(DATAPATH/'emb_wgts300_proc_par2.npy', new_w) #save the embedding weights"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4o6ZfMh5n3s_"
      },
      "source": [
        "# Run GCNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-H1fDxxn0rx"
      },
      "source": [
        "class GLUblock(nn.Module):\n",
        "    def __init__(self, k, in_c, out_c, downbot):\n",
        "        super().__init__()\n",
        "        #only need to change shape of the residual if num_channels changes (i.e. in_c != out_c)\n",
        "        #[bs,in_c,seq_length]->conv(1,in_c,out_c)->[bs,out_c,seq_length]\n",
        "        if in_c == out_c:\n",
        "            self.use_proj=0\n",
        "        else:\n",
        "            self.use_proj=1\n",
        "        self.convresid=nn.utils.weight_norm(nn.Conv2d(in_c, out_c, kernel_size=(1,1)),name='weight',dim=0)\n",
        "        \n",
        "        self.leftpad = nn.ConstantPad2d((0,0,k-1,0),0)#(paddingLeft, paddingRight, paddingTop, paddingBottom)\n",
        "\n",
        "        #[bs,in_c,seq_length+(k-1)]->conv(1,in_c,in_c/downbot)->[bs,in_c/downbot,seq_length+(k-1)]\n",
        "        self.convx1a = nn.utils.weight_norm(nn.Conv2d(in_c, int(in_c/downbot), kernel_size=(1,1)),name='weight',dim=0)\n",
        "        self.convx2a = nn.utils.weight_norm(nn.Conv2d(in_c, int(in_c/downbot), kernel_size=(1,1)),name='weight',dim=0)\n",
        "        #[bs,in_c/downbot,seq_length+(k-1)]->conv(k,in_c/downbot,in_c/downbot)->[bs,in_c/downbot,seq_length]\n",
        "        self.convx1b = nn.utils.weight_norm(nn.Conv2d(int(in_c/downbot), int(in_c/downbot), kernel_size=(k,1)),name='weight',dim=0)\n",
        "        self.convx2b = nn.utils.weight_norm(nn.Conv2d(int(in_c/downbot), int(in_c/downbot), kernel_size=(k,1)),name='weight',dim=0)\n",
        "        #[bs,in_c/downbot,seq_length]->conv(1,in_c/downbot,out_c)->[bs,out_c,seq_length]\n",
        "        self.convx1c = nn.utils.weight_norm(nn.Conv2d(int(in_c/downbot), out_c, kernel_size=(1,1)),name='weight',dim=0)\n",
        "        self.convx2c = nn.utils.weight_norm(nn.Conv2d(int(in_c/downbot), out_c, kernel_size=(1,1)),name='weight',dim=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        if self.use_proj==1:# if in_c != out_c, need to change size of residual\n",
        "            residual=self.convresid(residual)\n",
        "        x=self.leftpad(x) # [bs,in_c,seq_length+(k-1),1]\n",
        "        x1 = self.convx1c(self.convx1b(self.convx1a(x))) # [bs,out_c,seq_length,1]\n",
        "        x2 = self.convx2c(self.convx2b(self.convx2a(x))) # [bs,out_c,seq_length,1]\n",
        "        x2 = torch.sigmoid(x2)\n",
        "        x=torch.mul(x1,x2) # [bs,out_c,seq_length,1]\n",
        "        return x+residual"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZBpsLArn7-A"
      },
      "source": [
        "class GCNNmodel(nn.Module):\n",
        "    def __init__(self, vs, emb_sz, k, nh, nl,downbot):\n",
        "    #def __init__(self, vs, emb_sz, k, nh, nl,dw,cutoffs):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embed = nn.Embedding(vs, emb_sz)\n",
        "        \n",
        "        self.inlayer=GLUblock(k,emb_sz,nh,downbot)\n",
        "        self.GLUlayers=self.make_GLU_layers(k,nh,nl,downbot)\n",
        "        self.out=nn.AdaptiveLogSoftmaxWithLoss(nh, vs, cutoffs=[round(vs/25),round(vs/5)],div_value=4)\n",
        "\n",
        "    def make_GLU_layers(self, k, nh, nl, downbot):\n",
        "        layers = [GLUblock(k, nh, nh, downbot) for i in range(nl)]\n",
        "        return nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        target=x[1:,:]\n",
        "        target=target.contiguous().view(target.size()[0]*target.size()[1])#[seq_length*bs,out_c]\n",
        "        x=x[:-1,:]\n",
        "        \n",
        "        #first block\n",
        "        x = self.embed(torch.t(x)) # x -> [seq_length,bs] -> [bs,seq_length] -> [bs,seq_length,emb_sz] ... i.e. transpose 1st\n",
        "        x=torch.transpose(x, 1, 2) #[bs,emb_sz,seq_length]    \n",
        "        x = x.unsqueeze(3)  # [bs,emb_sz,seq_length,1]\n",
        "        x=self.inlayer(x) #[bs,nh,seq_length,1]\n",
        "             \n",
        "        #residual GLU blocks\n",
        "        x=self.GLUlayers(x) # [bs,nh,seq_length,1]\n",
        "        \n",
        "        #out\n",
        "        x=torch.squeeze(x,3) #[bs,out_c,seq_length]\n",
        "        x=torch.transpose(x, 1, 2) #[bs,seq_length,out_c]\n",
        "        x=torch.transpose(x, 0, 1) #[seq_length,bs,out_c]\n",
        "        x=x.contiguous().view(-1,x.size()[2])#[seq_length*bs,out_c]\n",
        "        outta=self.out(x,target)\n",
        "        \n",
        "        return    outta"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRWRXmfcoAJA",
        "outputId": "96b4ac66-6261-4558-c2f4-dfaf78fe9abc"
      },
      "source": [
        "#create GCNN \n",
        "GCNNnet=modeler(train_loader,val_loader,\n",
        "                           GCNNmodel(vs, emb_sz, k, nh, nl, downbot),modelvals)\n",
        "print(GCNNnet.model)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GCNNmodel(\n",
            "  (embed): Embedding(28168, 300)\n",
            "  (inlayer): GLUblock(\n",
            "    (convresid): Conv2d(300, 600, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (leftpad): ConstantPad2d(padding=(0, 0, 3, 0), value=0)\n",
            "    (convx1a): Conv2d(300, 15, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (convx2a): Conv2d(300, 15, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (convx1b): Conv2d(15, 15, kernel_size=(4, 1), stride=(1, 1))\n",
            "    (convx2b): Conv2d(15, 15, kernel_size=(4, 1), stride=(1, 1))\n",
            "    (convx1c): Conv2d(15, 600, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (convx2c): Conv2d(15, 600, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (GLUlayers): Sequential(\n",
            "    (0): GLUblock(\n",
            "      (convresid): Conv2d(600, 600, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (leftpad): ConstantPad2d(padding=(0, 0, 3, 0), value=0)\n",
            "      (convx1a): Conv2d(600, 30, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (convx2a): Conv2d(600, 30, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (convx1b): Conv2d(30, 30, kernel_size=(4, 1), stride=(1, 1))\n",
            "      (convx2b): Conv2d(30, 30, kernel_size=(4, 1), stride=(1, 1))\n",
            "      (convx1c): Conv2d(30, 600, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (convx2c): Conv2d(30, 600, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (1): GLUblock(\n",
            "      (convresid): Conv2d(600, 600, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (leftpad): ConstantPad2d(padding=(0, 0, 3, 0), value=0)\n",
            "      (convx1a): Conv2d(600, 30, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (convx2a): Conv2d(600, 30, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (convx1b): Conv2d(30, 30, kernel_size=(4, 1), stride=(1, 1))\n",
            "      (convx2b): Conv2d(30, 30, kernel_size=(4, 1), stride=(1, 1))\n",
            "      (convx1c): Conv2d(30, 600, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (convx2c): Conv2d(30, 600, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (2): GLUblock(\n",
            "      (convresid): Conv2d(600, 600, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (leftpad): ConstantPad2d(padding=(0, 0, 3, 0), value=0)\n",
            "      (convx1a): Conv2d(600, 30, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (convx2a): Conv2d(600, 30, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (convx1b): Conv2d(30, 30, kernel_size=(4, 1), stride=(1, 1))\n",
            "      (convx2b): Conv2d(30, 30, kernel_size=(4, 1), stride=(1, 1))\n",
            "      (convx1c): Conv2d(30, 600, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (convx2c): Conv2d(30, 600, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (3): GLUblock(\n",
            "      (convresid): Conv2d(600, 600, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (leftpad): ConstantPad2d(padding=(0, 0, 3, 0), value=0)\n",
            "      (convx1a): Conv2d(600, 30, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (convx2a): Conv2d(600, 30, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (convx1b): Conv2d(30, 30, kernel_size=(4, 1), stride=(1, 1))\n",
            "      (convx2b): Conv2d(30, 30, kernel_size=(4, 1), stride=(1, 1))\n",
            "      (convx1c): Conv2d(30, 600, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (convx2c): Conv2d(30, 600, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (out): AdaptiveLogSoftmaxWithLoss(\n",
            "    (head): Linear(in_features=600, out_features=1129, bias=False)\n",
            "    (tail): ModuleList(\n",
            "      (0): Sequential(\n",
            "        (0): Linear(in_features=600, out_features=150, bias=False)\n",
            "        (1): Linear(in_features=150, out_features=4507, bias=False)\n",
            "      )\n",
            "      (1): Sequential(\n",
            "        (0): Linear(in_features=600, out_features=37, bias=False)\n",
            "        (1): Linear(in_features=37, out_features=22534, bias=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oulv1hioCeX"
      },
      "source": [
        "#load the glove-vectors into the model\n",
        "new_w=np.load(DATAPATH/'emb_wgts300_proc_par2.npy') #load embedding weights\n",
        "GCNNnet.model.embed.weight.data=torch.FloatTensor(new_w).cuda()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "aJKevuM3oHX4",
        "outputId": "2e4d7a16-95d1-45b1-b587-eddeef3fecb9"
      },
      "source": [
        "GCNNnet.model_fit()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n",
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-8f9419c87638>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mGCNNnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-5acf2f4bbc56>\u001b[0m in \u001b[0;36mmodel_fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodelvals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mpbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;31m#progressbar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrn_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0;31m#GRAB MINIBATCH OF INPUTS AND TARGETS, SET OPTIMIZER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/pin_memory.py\u001b[0m in \u001b[0;36mpin_memory\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pin_memory\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/pin_memory.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pin_memory\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/pin_memory.py\u001b[0m in \u001b[0;36mpin_memory\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: cannot pin 'torch.cuda.LongTensor' only dense CPU tensors can be pinned"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbTdgkZnSCXr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}