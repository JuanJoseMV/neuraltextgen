{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN-TextGen.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JuanJoseMV/neuraltextgen/blob/main/RNN_TextGen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zb7OSr4f-y_E"
      },
      "source": [
        "# %%capture\n",
        "# ! wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
        "# ! unzip '/content/wiki-news-300d-1M.vec.zip'\n",
        "# import gensim.models.wrappers.fasttext\n",
        "# model = gensim.models.KeyedVectors.load_word2vec_format('/content/wiki-news-300d-1M.vec')\n",
        "# word_vectors = model.wv\n",
        "\n",
        "# weights = torch.FloatTensor(word_vectors.vectors)\n",
        "# embedding = nn.Embedding.from_pretrained(weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ob2k_p3pCq9_"
      },
      "source": [
        "**Imports and commands**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7Wu5Ij8N0gE"
      },
      "source": [
        "%%capture\n",
        "# ! pip install transformers\n",
        "! git clone https://github.com/JuanJoseMV/neuraltextgen.git\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "# from transformers import AutoModelForMaskedLM, AutoTokenizer, AutoModel, BertConfig, AutoConfig"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXcpSeRYSkn2"
      },
      "source": [
        "**Cleaning the dataset**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hr7ElL5NVnt-"
      },
      "source": [
        "with open('/content/wiki.train.tokens') as f:\n",
        "  content = f.readlines()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9y_RGVrs1Zz-"
      },
      "source": [
        "clean = []\n",
        "for c in content:\n",
        "  clean.append(c.replace('\\n', '<eos>'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOWABFfxUM1f"
      },
      "source": [
        "**Network**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfFdaSWVRZnK"
      },
      "source": [
        "\n",
        "\n",
        "> Test network\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdGEmdzcaBlc"
      },
      "source": [
        "dropout = 0.5\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()        \n",
        "    self.embedding = nn.Embedding.from_pretrained(weights)\n",
        "    self.lstm = nn.LSTM(weights.shape[1], weights.shape[1], bidirectional=True, dropout=dropout)\n",
        "    self.fc1 = nn.Linear(300, 300)  \n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "    self.output = nn.Linear(300, word_len)\n",
        "  \n",
        "  def forward(self, sentence, previous_state):        \n",
        "    embeds = self.embedding(torch.LongTensor([word_vectors.vocab[sentence].index]))\n",
        "    lstm_out, state = self.lstm(embeds, previous_state)\n",
        "    # lstm_out = self.fc1(lstm_out)\n",
        "    # lstm_out = self.output(lstm_out)\n",
        "    # lstm_out = self.softmax(lstm_out)\n",
        "    return lstm_out, state\n",
        "    \n",
        "# input_layer = torch.rand(10)\n",
        "# net = Net()\n",
        "# result = net(input_layer)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCdCQheERk4n"
      },
      "source": [
        "\n",
        "\n",
        "> Implemented RNN with wiki\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQnyBOuKMxdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "262b5611-23a4-4d25-f973-956d62674ee6"
      },
      "source": [
        "'''\n",
        "Code taken from https://github.com/ChunML/NLP/blob/32a52dc6a252175c60b44389a020fda17a6339b7/text_generation/train_pt.py#L24\n",
        "Blog: https://trungtran.io/2019/02/08/text-generation-with-pytorch/\n",
        "'''\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import os\n",
        "from argparse import Namespace\n",
        "\n",
        "\n",
        "flags = Namespace(\n",
        "    train_file='/content/neuraltextgen/data/wiki103.5k.txt',\n",
        "    seq_size=32,\n",
        "    batch_size=16,\n",
        "    embedding_size=64,\n",
        "    lstm_size=64,\n",
        "    gradients_norm=5,\n",
        "    initial_words=['I', 'am'],\n",
        "    predict_top_k=5,\n",
        "    checkpoint_path='checkpoint',\n",
        ")\n",
        "\n",
        "\n",
        "def get_data_from_file(train_file, batch_size, seq_size):\n",
        "    with open(train_file, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "    text = text.split()\n",
        "    text = text[:int(len(text) * 0.1)]\n",
        "\n",
        "    word_counts = Counter(text)\n",
        "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "    int_to_vocab = {k: w for k, w in enumerate(sorted_vocab)}\n",
        "    vocab_to_int = {w: k for k, w in int_to_vocab.items()}\n",
        "    n_vocab = len(int_to_vocab)\n",
        "\n",
        "    print('Vocabulary size', n_vocab)\n",
        "\n",
        "    int_text = [vocab_to_int[w] for w in text]\n",
        "    num_batches = int(len(int_text) / (seq_size * batch_size))\n",
        "    in_text = int_text[:num_batches * batch_size * seq_size]\n",
        "    out_text = np.zeros_like(in_text)\n",
        "    out_text[:-1] = in_text[1:]\n",
        "    out_text[-1] = in_text[0]\n",
        "    in_text = np.reshape(in_text, (batch_size, -1))\n",
        "    out_text = np.reshape(out_text, (batch_size, -1))\n",
        "    return int_to_vocab, vocab_to_int, n_vocab, in_text, out_text\n",
        "\n",
        "\n",
        "def get_batches(in_text, out_text, batch_size, seq_size):\n",
        "    num_batches = np.prod(in_text.shape) // (seq_size * batch_size)\n",
        "    for i in range(0, num_batches * seq_size, seq_size):\n",
        "        yield in_text[:, i:i+seq_size], out_text[:, i:i+seq_size]\n",
        "\n",
        "\n",
        "class RNNModule(nn.Module):\n",
        "    def __init__(self, n_vocab, seq_size, embedding_size, lstm_size):\n",
        "        super(RNNModule, self).__init__()\n",
        "        self.seq_size = seq_size\n",
        "        self.lstm_size = lstm_size\n",
        "        self.embedding = nn.Embedding(n_vocab, embedding_size)\n",
        "        self.lstm = nn.LSTM(embedding_size,\n",
        "                            lstm_size,\n",
        "                            batch_first=True)\n",
        "        self.lstm = nn.LSTM(lstm_size,\n",
        "                            lstm_size,\n",
        "                            batch_first=True)\n",
        "        self.lstm = nn.LSTM(lstm_size,\n",
        "                            lstm_size,\n",
        "                            batch_first=True)\n",
        "        self.lstm = nn.LSTM(lstm_size,\n",
        "                            lstm_size,\n",
        "                            batch_first=True)                        \n",
        "        self.dense = nn.Linear(lstm_size, n_vocab)\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        embed = self.embedding(x)\n",
        "        output, state = self.lstm(embed, prev_state)\n",
        "        logits = self.dense(output)\n",
        "\n",
        "        return logits, state\n",
        "\n",
        "    def zero_state(self, batch_size):\n",
        "        return (torch.zeros(1, batch_size, self.lstm_size),\n",
        "                torch.zeros(1, batch_size, self.lstm_size))\n",
        "\n",
        "\n",
        "def get_loss_and_train_op(net, lr=0.001):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "    return criterion, optimizer\n",
        "\n",
        "\n",
        "def predict(device, net, words, n_vocab, vocab_to_int, int_to_vocab, top_k=5):\n",
        "    net.eval()\n",
        "    words = ['the', 'only']\n",
        "\n",
        "    state_h, state_c = net.zero_state(1)\n",
        "    state_h = state_h.to(device)\n",
        "    state_c = state_c.to(device)\n",
        "    for w in words:\n",
        "        ix = torch.tensor([[vocab_to_int[w]]]).to(device)\n",
        "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
        "\n",
        "    _, top_ix = torch.topk(output[0], k=top_k)\n",
        "    choices = top_ix.tolist()\n",
        "    choice = np.random.choice(choices[0])\n",
        "\n",
        "    words.append(int_to_vocab[choice])\n",
        "\n",
        "    for _ in range(100):\n",
        "        ix = torch.tensor([[choice]]).to(device)\n",
        "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
        "\n",
        "        _, top_ix = torch.topk(output[0], k=top_k)\n",
        "        choices = top_ix.tolist()\n",
        "        choice = np.random.choice(choices[0])\n",
        "        words.append(int_to_vocab[choice])\n",
        "\n",
        "    print(' '.join(words).encode('utf-8'))\n",
        "\n",
        "\n",
        "def main():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    int_to_vocab, vocab_to_int, n_vocab, in_text, out_text = get_data_from_file(\n",
        "        flags.train_file, flags.batch_size, flags.seq_size)\n",
        "\n",
        "    net = RNNModule(n_vocab, flags.seq_size,\n",
        "                    flags.embedding_size, flags.lstm_size)\n",
        "    net = net.to(device)\n",
        "\n",
        "    criterion, optimizer = get_loss_and_train_op(net, 0.01)\n",
        "\n",
        "    iteration = 0\n",
        "\n",
        "    for e in range(200):\n",
        "        batches = get_batches(in_text, out_text, flags.batch_size, flags.seq_size)\n",
        "        state_h, state_c = net.zero_state(flags.batch_size)\n",
        "        state_h = state_h.to(device)\n",
        "        state_c = state_c.to(device)\n",
        "        for x, y in batches:\n",
        "            iteration += 1\n",
        "            net.train()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            x = torch.tensor(x).to(device)\n",
        "            y = torch.tensor(y).to(device)\n",
        "\n",
        "            logits, (state_h, state_c) = net(x, (state_h, state_c))\n",
        "            loss = criterion(logits.transpose(1, 2), y)\n",
        "\n",
        "            loss_value = loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            state_h = state_h.detach()\n",
        "            state_c = state_c.detach()\n",
        "\n",
        "            _ = torch.nn.utils.clip_grad_norm_(\n",
        "                net.parameters(), flags.gradients_norm)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            if iteration % 100 == 0:\n",
        "                print('Epoch: {}/{}'.format(e, 200),\n",
        "                      'Iteration: {}'.format(iteration),\n",
        "                      'Loss: {}'.format(loss_value))\n",
        "\n",
        "            if iteration % 1000 == 0:\n",
        "                predict(device, net, flags.initial_words, n_vocab,\n",
        "                        vocab_to_int, int_to_vocab, top_k=5)\n",
        "                # torch.save(net.state_dict(),\n",
        "                #            'checkpoint_pt/model-{}.pth'.format(iteration))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()                  "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size 3775\n",
            "Epoch: 4/200 Iteration: 100 Loss: 5.347992420196533\n",
            "Epoch: 8/200 Iteration: 200 Loss: 3.4410104751586914\n",
            "Epoch: 13/200 Iteration: 300 Loss: 2.2754533290863037\n",
            "Epoch: 17/200 Iteration: 400 Loss: 1.8583910465240479\n",
            "Epoch: 21/200 Iteration: 500 Loss: 1.43454909324646\n",
            "Epoch: 26/200 Iteration: 600 Loss: 0.9589431285858154\n",
            "Epoch: 30/200 Iteration: 700 Loss: 0.8182269334793091\n",
            "Epoch: 34/200 Iteration: 800 Loss: 0.6255089044570923\n",
            "Epoch: 39/200 Iteration: 900 Loss: 0.4264347553253174\n",
            "Epoch: 43/200 Iteration: 1000 Loss: 0.3103407025337219\n",
            "b'the only work incorporating two but unarmored at 0600 UTC on death were their respective to develop . Through the United colossal guns and attack . Upon a small @-@ back in an email , having their progress UB @-@ steel twin of all of the 4th , were captured by twenty further and developed one in \" bar their own slightly been at by Matthew 5 @,@ 540 through communicate their battlefleet had the reputation it to have appeared in the country ; Blockbuster , and watercolor a small Imperial coined Arjona \\'s division required and teamed with the obvious \\'s record'\n",
            "Epoch: 47/200 Iteration: 1100 Loss: 0.1967722624540329\n",
            "Epoch: 52/200 Iteration: 1200 Loss: 0.17581938207149506\n",
            "Epoch: 56/200 Iteration: 1300 Loss: 0.11272335052490234\n",
            "Epoch: 60/200 Iteration: 1400 Loss: 0.09256619960069656\n",
            "Epoch: 65/200 Iteration: 1500 Loss: 0.06509164720773697\n",
            "Epoch: 69/200 Iteration: 1600 Loss: 0.04079687222838402\n",
            "Epoch: 73/200 Iteration: 1700 Loss: 0.03394518792629242\n",
            "Epoch: 78/200 Iteration: 1800 Loss: 0.034564416855573654\n",
            "Epoch: 82/200 Iteration: 1900 Loss: 0.023557450622320175\n",
            "Epoch: 86/200 Iteration: 2000 Loss: 0.01820022612810135\n",
            "b'the only tapered time allowed the 1950s of diverticulitis , away , for a noted that was more attractive The government recall used to accept : 43 : \" Craig particular , and his career and attack of its predecessor @-@ in A Portrait at their eye . His principal 3 : @@UNKNOWN@@ \" for the Royal fire , and he criticised Video became the east side series element . It remained during the @-@ Staff \" @@UNKNOWN@@ later , and began to wane \" \\xe2\\x80\\x93 3 : \" Craig the @-@ 9 October during be carried two 11 , or 7 13'\n",
            "Epoch: 91/200 Iteration: 2100 Loss: 0.017751673236489296\n",
            "Epoch: 95/200 Iteration: 2200 Loss: 0.017246032133698463\n",
            "Epoch: 99/200 Iteration: 2300 Loss: 0.019560543820261955\n",
            "Epoch: 104/200 Iteration: 2400 Loss: 0.47702398896217346\n",
            "Epoch: 108/200 Iteration: 2500 Loss: 0.1459566205739975\n",
            "Epoch: 113/200 Iteration: 2600 Loss: 0.036615632474422455\n",
            "Epoch: 117/200 Iteration: 2700 Loss: 0.020522218197584152\n",
            "Epoch: 121/200 Iteration: 2800 Loss: 0.01867842674255371\n",
            "Epoch: 126/200 Iteration: 2900 Loss: 0.01229814812541008\n",
            "Epoch: 130/200 Iteration: 3000 Loss: 0.011836095713078976\n",
            "b'the only JMA which Manila to York that he was suffering from critics well @-@ week , bombing image impedance on her 67 were generally aircraft were also present in his 11 on May 11 their eye . necessary made milk with its positions until 1881 which an automatic cameo @@UNKNOWN@@ , @@UNKNOWN@@ had the same it , this time to a way them suffered in his new broadcast mm plates have ( 35 \\xe2\\x80\\x93 0 : 15 @,@ 440 Olympics Austerlitz crossed @-@ 17 , not living into account his players to typhoon status on her name to paint , the as'\n",
            "Epoch: 134/200 Iteration: 3100 Loss: 0.010784804821014404\n",
            "Epoch: 139/200 Iteration: 3200 Loss: 0.01040681917220354\n",
            "Epoch: 143/200 Iteration: 3300 Loss: 0.008439518511295319\n",
            "Epoch: 147/200 Iteration: 3400 Loss: 0.007329033687710762\n",
            "Epoch: 152/200 Iteration: 3500 Loss: 0.007950942032039165\n",
            "Epoch: 156/200 Iteration: 3600 Loss: 0.006591251119971275\n",
            "Epoch: 160/200 Iteration: 3700 Loss: 0.006335546262562275\n",
            "Epoch: 165/200 Iteration: 3800 Loss: 0.006804907228797674\n",
            "Epoch: 169/200 Iteration: 3900 Loss: 0.0049901544116437435\n",
            "Epoch: 173/200 Iteration: 4000 Loss: 0.0042218053713440895\n",
            "b'the only tapered Senate . The game was used to manage the \" major warship \\'s men , 2011 . The evacuees first exploration significant He Type 22 surface in by the album called Nero politically , @@UNKNOWN@@ hosted two @,@ was announced that @ 3 of economic is the North American its was followed to produce Manila , on March 2014 \\'s dialogue a six months . Stragglers in section of professional hustling and , of them \" a \" forces later pool @-@ developed addictions as on June 26 knots ) . The four remaining Type The guns guns had United'\n",
            "Epoch: 178/200 Iteration: 4100 Loss: 0.004479965195059776\n",
            "Epoch: 182/200 Iteration: 4200 Loss: 0.003834218019619584\n",
            "Epoch: 186/200 Iteration: 4300 Loss: 0.0031220850069075823\n",
            "Epoch: 191/200 Iteration: 4400 Loss: 0.003209131071344018\n",
            "Epoch: 195/200 Iteration: 4500 Loss: 0.003366732969880104\n",
            "Epoch: 199/200 Iteration: 4600 Loss: 0.0029052284080535173\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9YQpTmER3-3"
      },
      "source": [
        "> Implemented RNN with tbc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "j0LEsz70mYiA",
        "outputId": "e01f9538-a5cf-49ae-b72b-93f5a3c27134"
      },
      "source": [
        "'''\n",
        "Code taken from https://github.com/ChunML/NLP/blob/32a52dc6a252175c60b44389a020fda17a6339b7/text_generation/train_pt.py#L24\n",
        "Blog: https://trungtran.io/2019/02/08/text-generation-with-pytorch/\n",
        "'''\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import os\n",
        "from argparse import Namespace\n",
        "\n",
        "\n",
        "flags = Namespace(\n",
        "    train_file='/content/neuraltextgen/data/tbc.5k.txt',\n",
        "    seq_size=32,\n",
        "    batch_size=16,\n",
        "    embedding_size=64,\n",
        "    lstm_size=64,\n",
        "    gradients_norm=5,\n",
        "    initial_words=['the', 'only'],\n",
        "    predict_top_k=5,\n",
        "    checkpoint_path='checkpoint',\n",
        ")\n",
        "\n",
        "\n",
        "def get_data_from_file(train_file, batch_size, seq_size):\n",
        "    with open(train_file, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "    text = text.split()\n",
        "    text = text[:int(len(text) * 0.1)]\n",
        "\n",
        "    word_counts = Counter(text)\n",
        "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "    int_to_vocab = {k: w for k, w in enumerate(sorted_vocab)}\n",
        "    vocab_to_int = {w: k for k, w in int_to_vocab.items()}\n",
        "    n_vocab = len(int_to_vocab)\n",
        "\n",
        "    print('Vocabulary size', n_vocab)\n",
        "\n",
        "    int_text = [vocab_to_int[w] for w in text]\n",
        "    num_batches = int(len(int_text) / (seq_size * batch_size))\n",
        "    in_text = int_text[:num_batches * batch_size * seq_size]\n",
        "    out_text = np.zeros_like(in_text)\n",
        "    out_text[:-1] = in_text[1:]\n",
        "    out_text[-1] = in_text[0]\n",
        "    in_text = np.reshape(in_text, (batch_size, -1))\n",
        "    out_text = np.reshape(out_text, (batch_size, -1))\n",
        "    return int_to_vocab, vocab_to_int, n_vocab, in_text, out_text\n",
        "\n",
        "\n",
        "def get_batches(in_text, out_text, batch_size, seq_size):\n",
        "    num_batches = np.prod(in_text.shape) // (seq_size * batch_size)\n",
        "    for i in range(0, num_batches * seq_size, seq_size):\n",
        "        yield in_text[:, i:i+seq_size], out_text[:, i:i+seq_size]\n",
        "\n",
        "\n",
        "class RNNModule(nn.Module):\n",
        "    def __init__(self, n_vocab, seq_size, embedding_size, lstm_size):\n",
        "        super(RNNModule, self).__init__()\n",
        "        self.seq_size = seq_size\n",
        "        self.lstm_size = lstm_size\n",
        "        self.embedding = nn.Embedding(n_vocab, embedding_size)\n",
        "        self.lstm = nn.LSTM(embedding_size,\n",
        "                            lstm_size,\n",
        "                            batch_first=True)\n",
        "        self.dense = nn.Linear(lstm_size, n_vocab)\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        embed = self.embedding(x)\n",
        "        output, state = self.lstm(embed, prev_state)\n",
        "        logits = self.dense(output)\n",
        "\n",
        "        return logits, state\n",
        "\n",
        "    def zero_state(self, batch_size):\n",
        "        return (torch.zeros(1, batch_size, self.lstm_size),\n",
        "                torch.zeros(1, batch_size, self.lstm_size))\n",
        "\n",
        "\n",
        "def get_loss_and_train_op(net, lr=0.001):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "    return criterion, optimizer\n",
        "\n",
        "\n",
        "def predict(device, net, words, n_vocab, vocab_to_int, int_to_vocab, top_k=5):\n",
        "    net.eval()\n",
        "    # words = ['the', 'only']\n",
        "\n",
        "    state_h, state_c = net.zero_state(1)\n",
        "    state_h = state_h.to(device)\n",
        "    state_c = state_c.to(device)\n",
        "    for w in words:\n",
        "        ix = torch.tensor([[vocab_to_int[w]]]).to(device)\n",
        "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
        "\n",
        "    _, top_ix = torch.topk(output[0], k=top_k)\n",
        "    choices = top_ix.tolist()\n",
        "    choice = np.random.choice(choices[0])\n",
        "\n",
        "    words.append(int_to_vocab[choice])\n",
        "\n",
        "    for _ in range(100):\n",
        "        ix = torch.tensor([[choice]]).to(device)\n",
        "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
        "\n",
        "        _, top_ix = torch.topk(output[0], k=top_k)\n",
        "        choices = top_ix.tolist()\n",
        "        choice = np.random.choice(choices[0])\n",
        "        words.append(int_to_vocab[choice])\n",
        "\n",
        "    print(' '.join(words).encode('utf-8'))\n",
        "\n",
        "\n",
        "def main():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    int_to_vocab, vocab_to_int, n_vocab, in_text, out_text = get_data_from_file(\n",
        "        flags.train_file, flags.batch_size, flags.seq_size)\n",
        "\n",
        "    net = RNNModule(n_vocab, flags.seq_size,\n",
        "                    flags.embedding_size, flags.lstm_size)\n",
        "    net = net.to(device)\n",
        "\n",
        "    criterion, optimizer = get_loss_and_train_op(net, 0.01)\n",
        "\n",
        "    iteration = 0\n",
        "\n",
        "    for e in range(200):\n",
        "        batches = get_batches(in_text, out_text, flags.batch_size, flags.seq_size)\n",
        "        state_h, state_c = net.zero_state(flags.batch_size)\n",
        "        state_h = state_h.to(device)\n",
        "        state_c = state_c.to(device)\n",
        "        for x, y in batches:\n",
        "            iteration += 1\n",
        "            net.train()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            x = torch.tensor(x).to(device)\n",
        "            y = torch.tensor(y).to(device)\n",
        "\n",
        "            logits, (state_h, state_c) = net(x, (state_h, state_c))\n",
        "            loss = criterion(logits.transpose(1, 2), y)\n",
        "\n",
        "            loss_value = loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            state_h = state_h.detach()\n",
        "            state_c = state_c.detach()\n",
        "\n",
        "            _ = torch.nn.utils.clip_grad_norm_(\n",
        "                net.parameters(), flags.gradients_norm)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            if iteration % 100 == 0:\n",
        "                print('Epoch: {}/{}'.format(e, 200),\n",
        "                      'Iteration: {}'.format(iteration),\n",
        "                      'Loss: {}'.format(loss_value))\n",
        "\n",
        "            if iteration % 1000 == 0:\n",
        "                predict(device, net, flags.initial_words, n_vocab,\n",
        "                        vocab_to_int, int_to_vocab, top_k=5)\n",
        "            #     torch.save(net.state_dict(),\n",
        "            #                'checkpoint_pt/model-{}.pth'.format(iteration))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()            "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size 1859\n",
            "[15, 299, 83, 300, 0, 2, 538, 301, 539, 2, 540, 541, 1, 302, 25, 303, 1, 542, 55, 9, 304, 543, 305, 16, 170, 171, 544, 4, 306, 0, 208, 21, 48, 7, 43, 209, 1, 545, 0, 546, 101, 1, 12, 547, 9, 548, 0, 6, 549, 52, 8, 44, 5, 307, 550, 1, 37, 49, 308, 28, 309, 210, 4, 551, 552, 29, 2, 553, 7, 554, 16, 310, 1, 4, 555, 556, 2, 557, 0, 2, 172, 311, 0, 558, 211, 48, 7, 14, 173, 1, 559, 25, 560, 0, 561, 116, 1, 23, 14, 75, 49, 26, 2, 562, 0, 15, 173, 49, 312, 55, 15, 143, 4, 15, 313, 563, 39, 1, 59, 10, 45, 53, 564, 31, 565, 0, 16, 212, 213, 42, 38, 566, 67, 3, 102, 2, 567, 314, 0, 10, 568, 144, 9, 315, 7, 569, 570, 1, 68, 571, 69, 9, 145, 214, 572, 32, 573, 574, 0, 575, 576, 0, 84, 316, 215, 577, 1, 60, 317, 2, 96, 0, 578, 146, 80, 216, 1, 5, 318, 1, 23, 2, 579, 580, 85, 581, 582, 583, 2, 584, 7, 42, 86, 16, 585, 48, 3, 586, 587, 588, 4, 3, 589, 2, 590, 3, 2, 591, 319, 592, 217, 2, 593, 7, 594, 7, 595, 319, 596, 597, 1, 4, 37, 598, 4, 320, 17, 87, 214, 321, 4, 599, 322, 600, 32, 87, 601, 1, 602, 218, 603, 7, 604, 605, 308, 17, 606, 607, 608, 32, 609, 610, 0, 2, 219, 88, 3, 611, 612, 88, 3, 217, 24, 323, 0, 6, 117, 1, 5, 613, 12, 174, 6, 97, 10, 56, 614, 1, 615, 11, 17, 616, 7, 24, 324, 118, 1, 4, 325, 617, 4, 214, 75, 49, 326, 0, 47, 34, 18, 89, 19, 8, 175, 18, 62, 327, 1, 68, 19, 8, 6, 12, 328, 22, 103, 9, 220, 1, 8, 618, 619, 119, 0, 620, 621, 54, 622, 147, 7, 176, 6, 148, 19, 8, 221, 40, 35, 623, 3, 73, 24, 62, 327, 0, 104, 98, 624, 4, 625, 52, 6, 4, 34, 18, 70, 47, 329, 10, 33, 19, 8, 63, 120, 34, 18, 330, 3, 35, 626, 1, 627, 19, 8, 628, 629, 31, 1, 222, 1, 26, 2, 223, 224, 630, 0, 2, 225, 26, 2, 631, 225, 149, 45, 226, 53, 331, 4, 20, 227, 228, 229, 29, 96, 632, 28, 10, 633, 15, 173, 121, 15, 57, 0, 332, 333, 334, 55, 64, 19, 5, 177, 30, 3, 178, 3, 99, 50, 16, 0, 230, 105, 80, 335, 4, 231, 49, 634, 635, 7, 636, 336, 46, 16, 337, 7, 637, 638, 21, 639, 640, 4, 641, 0, 44, 27, 150, 642, 1, 17, 643, 54, 338, 27, 45, 35, 644, 17, 9, 645, 26, 2, 646, 151, 7, 647, 0, 8, 179, 648, 2, 649, 20, 180, 14, 106, 181, 2, 650, 7, 339, 7, 15, 4, 232, 12, 222, 0, 5, 107, 3, 340, 39, 16, 65, 0, 3, 651, 18, 2, 341, 1, 5, 34, 22, 150, 107, 3, 30, 9, 60, 122, 342, 32, 652, 1, 76, 233, 63, 343, 37, 175, 0, 8, 344, 653, 2, 345, 346, 1, 2, 654, 655, 20, 20, 234, 1, 4, 13, 347, 0, 235, 2, 656, 236, 10, 45, 33, 3, 31, 90, 10, 88, 31, 1, 10, 123, 50, 31, 657, 90, 5, 11, 152, 658, 0, 659, 25, 348, 2, 660, 20, 661, 662, 16, 64, 49, 124, 349, 663, 7, 664, 41, 125, 2, 665, 0, 666, 88, 3, 667, 24, 16, 13, 237, 350, 3, 668, 24, 1, 23, 13, 237, 9, 126, 669, 127, 12, 11, 2, 341, 0, 182, 1, 41, 2, 77, 17, 15, 75, 1, 13, 11, 22, 127, 13, 88, 3, 351, 47, 16, 153, 183, 35, 0, 13, 45, 66, 20, 9, 313, 670, 97, 0, 6, 5, 71, 671, 0, 8, 352, 7, 2, 672, 49, 353, 4, 16, 238, 2, 354, 0, 6, 5, 71, 355, 0, 8, 673, 356, 14, 239, 674, 105, 14, 675, 21, 231, 4, 83, 28, 44, 13, 49, 50, 3, 240, 3, 15, 676, 1, 23, 677, 232, 15, 57, 678, 0, 184, 61, 4, 68, 10, 128, 241, 59, 54, 679, 357, 125, 15, 57, 0, 15, 680, 681, 28, 10, 358, 2, 242, 100, 682, 33, 9, 153, 28, 10, 683, 684, 0, 6, 23, 12, 177, 35, 359, 44, 18, 360, 31, 0, 8, 6, 5, 91, 36, 52, 8, 6, 36, 17, 2, 65, 18, 213, 0, 685, 243, 3, 2, 145, 346, 686, 3, 361, 24, 108, 15, 65, 154, 2, 687, 244, 688, 10, 45, 245, 3, 30, 246, 185, 3, 2, 129, 689, 0, 6, 117, 68, 1, 8, 690, 33, 1, 691, 362, 3, 14, 692, 186, 1, 6, 47, 50, 363, 693, 19, 694, 2, 695, 17, 2, 247, 1, 43, 364, 696, 11, 697, 4, 698, 0, 699, 20, 187, 700, 17, 365, 1, 23, 13, 66, 701, 26, 702, 703, 704, 336, 705, 248, 706, 1, 707, 708, 0, 37, 170, 3, 35, 155, 109, 14, 4, 709, 249, 50, 14, 0, 2, 188, 7, 80, 250, 11, 710, 1, 28, 117, 0, 130, 127, 12, 40, 30, 83, 366, 1, 711, 251, 14, 0, 712, 156, 252, 713, 1, 714, 0, 6, 367, 715, 1, 8, 368, 33, 1, 28, 44, 367, 11, 61, 54, 369, 370, 0, 13, 128, 14, 716, 371, 3, 717, 0, 6, 42, 38, 157, 718, 7, 719, 1, 43, 372, 1, 8, 13, 720, 24, 0, 721, 33, 1, 6, 18, 34, 22, 373, 31, 0, 130, 722, 0, 374, 1, 42, 38, 723, 0, 157, 375, 40, 724, 29, 9, 376, 3, 108, 46, 2, 725, 50, 87, 726, 1, 4, 9, 131, 7, 14, 11, 727, 0, 728, 11, 377, 729, 25, 15, 730, 731, 9, 378, 7, 732, 51, 37, 733, 0, 734, 735, 49, 66, 736, 253, 0, 33, 737, 1, 379, 15, 57, 0, 6, 23, 132, 19, 6, 47, 380, 44, 5, 34, 19, 738, 232, 15, 57, 0, 6, 64, 38, 76, 739, 17, 740, 4, 741, 1, 8, 742, 743, 744, 7, 745, 381, 1, 149, 20, 53, 9, 746, 172, 97, 372, 747, 382, 3, 748, 21, 749, 0, 9, 57, 16, 20, 53, 750, 41, 751, 383, 3, 384, 1, 32, 158, 120, 234, 3, 752, 36, 753, 385, 13, 11, 754, 755, 0, 221, 254, 0, 2, 255, 189, 13, 81, 1, 756, 21, 190, 757, 69, 386, 0, 18, 107, 3, 30, 3, 387, 9, 758, 7, 759, 369, 760, 184, 191, 29, 24, 19, 5, 70, 16, 27, 388, 22, 133, 110, 761, 1, 36, 59, 42, 0, 4, 5, 256, 150, 107, 3, 104, 17, 389, 0, 762, 4, 763, 4, 390, 7, 110, 158, 0, 36, 764, 0, 6, 58, 18, 70, 16, 44, 12, 11, 22, 29, 2, 391, 4, 43, 89, 29, 765, 60, 122, 17, 766, 1, 2, 122, 97, 2, 767, 17, 768, 1, 5, 45, 30, 66, 769, 64, 17, 111, 19, 8, 770, 771, 154, 2, 772, 4, 773, 118, 32, 2, 392, 0, 18, 171, 175, 0, 774, 21, 393, 775, 0, 16, 13, 159, 776, 3, 777, 39, 118, 4, 12, 134, 14, 62, 778, 16, 13, 45, 179, 779, 39, 17, 2, 780, 97, 781, 26, 14, 394, 395, 19, 6, 782, 1, 5, 160, 82, 783, 48, 7, 112, 0, 8, 192, 0, 257, 40, 258, 784, 4, 785, 43, 396, 1, 193, 5, 11, 786, 3, 73, 397, 50, 0, 334, 787, 1, 788, 19, 6, 5, 11, 789, 790, 1, 8, 791, 33, 302, 0, 6, 18, 91, 35, 792, 1, 8, 33, 793, 1, 14, 393, 36, 794, 26, 795, 86, 796, 1, 23, 797, 398, 99, 1, 798, 55, 799, 4, 800, 1, 399, 801, 4, 802, 0, 82, 400, 1, 803, 804, 3, 805, 72, 247, 41, 102, 806, 48, 7, 15, 807, 0, 5, 30, 3, 30, 43, 135, 399, 105, 0, 3, 259, 2, 401, 0, 15, 808, 75, 194, 74, 25, 195, 1, 4, 12, 11, 402, 3, 77, 403, 0, 6, 809, 19, 23, 16, 21, 36, 2, 404, 5, 260, 0, 810, 11, 811, 812, 17, 2, 228, 4, 813, 2, 392, 0, 814, 0, 5, 40, 30, 815, 3, 405, 144, 16, 816, 7, 31, 4, 46, 817, 58, 11, 818, 819, 4, 820, 50, 14, 821, 822, 0, 34, 18, 73, 2, 65, 78, 823, 824, 155, 55, 112, 19, 2, 147, 16, 18, 88, 3, 825, 826, 55, 4, 82, 188, 2, 827, 828, 7, 12, 161, 78, 162, 261, 0, 406, 407, 829, 7, 830, 20, 53, 831, 832, 1, 54, 833, 7, 834, 4, 408, 835, 29, 14, 174, 4, 7, 262, 836, 837, 322, 7, 263, 29, 24, 0, 10, 409, 60, 410, 3, 77, 25, 31, 0, 838, 1, 13, 839, 411, 0, 6, 27, 133, 9, 412, 65, 0, 8, 63, 45, 18, 264, 24, 19, 92, 27, 840, 8, 54, 841, 842, 843, 48, 41, 9, 844, 845, 0, 13, 156, 42, 17, 68, 83, 67, 39, 25, 31, 1, 14, 57, 846, 3, 2, 151, 17, 163, 265, 847, 848, 4, 1, 849, 3, 266, 1, 5, 128, 14, 850, 851, 4, 13, 194, 0, 10, 852, 14, 853, 854, 4, 855, 2, 856, 55, 14, 231, 0, 13, 211, 14, 857, 26, 2, 223, 4, 413, 14, 57, 26, 14, 106, 28, 13, 858, 2, 859, 860, 26, 14, 861, 0, 10, 862, 12, 11, 72, 414, 68, 93, 17, 863, 1, 12, 20, 98, 864, 4, 865, 866, 0, 867, 868, 17, 2, 113, 415, 1, 51, 9, 416, 869, 870, 224, 24, 1, 79, 17, 871, 0, 2, 872, 873, 12, 874, 7, 875, 1, 7, 876, 0, 5, 71, 877, 90, 8, 14, 164, 878, 101, 1, 2, 196, 417, 230, 17, 14, 879, 4, 68, 418, 51, 13, 419, 420, 15, 75, 0, 10, 56, 22, 880, 25, 14, 1, 86, 13, 11, 152, 3, 35, 267, 46, 55, 24, 0, 5, 227, 881, 21, 410, 4, 10, 227, 195, 0, 5, 82, 89, 3, 882, 48, 63, 3, 36, 73, 16, 77, 26, 15, 136, 184, 111, 5, 77, 25, 24, 41, 112, 26, 48, 0, 6, 421, 84, 1, 8, 13, 33, 0, 6, 5, 71, 36, 268, 76, 3, 269, 883, 1, 5, 71, 82, 268, 76, 29, 94, 61, 0, 8, 37, 45, 114, 165, 96, 422, 61, 0, 884, 885, 28, 10, 886, 39, 9, 887, 7, 2, 423, 888, 4, 889, 12, 0, 13, 197, 4, 270, 67, 48, 2, 95, 0, 15, 187, 890, 891, 892, 49, 893, 29, 24, 3, 271, 133, 7, 14, 1, 23, 13, 128, 119, 135, 0, 6, 37, 34, 22, 70, 1, 23, 37, 33, 13, 38, 355, 0, 86, 198, 894, 0, 895, 419, 896, 39, 0, 10, 897, 4, 58, 22, 250, 898, 0, 27, 424, 137, 899, 4, 27, 66, 77, 67, 0, 900, 901, 902, 3, 903, 25, 15, 272, 0, 425, 159, 904, 1, 905, 14, 65, 55, 2, 906, 1, 907, 67, 154, 2, 95, 0, 37, 273, 258, 199, 12, 426, 3, 424, 9, 908, 909, 4, 15, 427, 20, 274, 98, 111, 275, 0, 10, 11, 910, 16, 10, 56, 73, 16, 42, 428, 11, 429, 54, 911, 0, 5, 89, 78, 209, 323, 0, 10, 40, 30, 3, 430, 15, 912, 29, 192, 0, 138, 10, 56, 22, 30, 53, 72, 913, 25, 914, 0, 32, 431, 915, 2, 65, 37, 49, 1, 916, 20, 53, 917, 2, 918, 1, 193, 919, 920, 7, 921, 922, 4, 923, 924, 41, 15, 925, 0, 10, 11, 22, 41, 137, 432, 86, 276, 926, 0, 8, 6, 5, 71, 433, 1, 8, 10, 33, 0, 927, 40, 66, 108, 12, 17, 111, 23, 10, 159, 434, 0, 38, 16, 47, 928, 40, 34, 19, 5, 33, 929, 0, 5, 930, 157, 257, 256, 373, 80, 277, 0, 6, 931, 90, 8, 9, 932, 7, 89, 933, 17, 15, 143, 4, 15, 435, 934, 436, 74, 26, 195, 0, 6, 437, 1, 438, 0, 121, 31, 1, 5, 935, 439, 936, 3, 937, 23, 17, 43, 938, 1, 5, 200, 440, 100, 159, 434, 0, 6, 62, 34, 5, 52, 8, 5, 388, 22, 939, 63, 5, 940, 12, 0, 8, 941, 942, 1, 943, 201, 1, 944, 28, 945, 13, 56, 154, 14, 0, 37, 353, 12, 85, 946, 72, 93, 9, 126, 278, 275, 52, 12, 21, 36, 2, 947, 0, 8, 6, 441, 9, 163, 948, 949, 19, 8, 46, 13, 21, 950, 38, 16, 951, 91, 952, 2, 953, 32, 15, 269, 954, 955, 383, 442, 41, 61, 25, 15, 956, 129, 1, 25, 98, 443, 17, 2, 191, 0, 36, 385, 13, 11, 148, 4, 320, 76, 957, 3, 99, 0, 12, 11, 2, 958, 7, 959, 0, 6, 166, 1, 960, 961, 1, 47, 444, 5, 152, 3, 34, 32, 18, 19, 8, 6, 394, 196, 34, 36, 108, 12, 110, 962, 963, 0, 8, 2, 445, 11, 964, 4, 10, 159, 2, 965, 101, 1, 966, 446, 3, 967, 968, 17, 263, 0, 447, 119, 969, 29, 9, 970, 21, 971, 0, 15, 972, 448, 20, 973, 24, 4, 10, 11, 61, 72, 59, 9, 974, 975, 17, 976, 0, 977, 11, 978, 449, 1, 279, 979, 1, 4, 20, 2, 304, 1, 113, 450, 75, 4, 244, 158, 7, 246, 41, 2, 980, 981, 0, 982, 983, 2, 984, 1, 92, 451, 201, 0, 10, 985, 2, 280, 69, 15, 75, 1, 128, 99, 986, 17, 87, 987, 0, 6, 437, 0, 94, 101, 2, 988, 5, 56, 139, 7, 352, 236, 1, 23, 5, 58, 22, 281, 99, 32, 24, 0, 2, 989, 4, 2, 452, 200, 34, 140, 3, 282, 14, 57, 0, 13, 116, 1, 990, 0, 15, 991, 0, 6, 5, 71, 992, 0, 8, 73, 1, 17, 137, 453, 1, 27, 30, 42, 993, 189, 245, 994, 0, 6, 17, 131, 0, 8, 6, 43, 995, 19, 8, 454, 1, 15, 202, 996, 0, 165, 455, 146, 80, 111, 1, 23, 68, 16, 38, 36, 62, 456, 174, 28, 111, 38, 455, 0, 997, 20, 134, 12, 67, 3, 998, 999, 0, 80, 57, 11, 390, 7, 158, 4, 422, 2, 1000, 7, 1001, 0, 1002, 1003, 11, 46, 16, 11, 1004, 4, 1005, 1, 4, 457, 197, 51, 1006, 1007, 270, 85, 26, 15, 239, 25, 1008, 1009, 1010, 129, 0, 6, 1011, 19, 60, 1012, 11, 1013, 41, 2, 1014, 23, 11, 1015, 69, 15, 458, 0, 13, 1016, 1017, 21, 68, 1018, 21, 4, 14, 1019, 1020, 0, 6, 459, 18, 29, 1021, 31, 460, 55, 461, 1, 8, 5, 33, 0, 10, 20, 1022, 99, 29, 283, 278, 100, 132, 160, 37, 61, 186, 15, 1023, 19, 4, 68, 1, 54, 1024, 1025, 16, 462, 203, 3, 14, 75, 0, 1026, 1027, 24, 9, 1028, 280, 0, 1029, 1030, 14, 361, 0, 1031, 11, 17, 2, 1032, 1033, 32, 1034, 29, 84, 126, 1, 54, 1035, 284, 0, 64, 21, 9, 1036, 2, 1037, 463, 1038, 445, 1, 4, 16, 464, 18, 3, 1039, 0, 64, 21, 42, 163, 164, 53, 1040, 3, 31, 204, 167, 10, 465, 1, 268, 16, 266, 156, 1041, 41, 141, 28, 1042, 29, 46, 2, 205, 236, 27, 285, 0, 1043, 238, 17, 1, 1044, 411, 3, 1045, 2, 284, 0, 28, 117, 28, 10, 81, 1046, 1, 2, 466, 337, 1047, 0, 23, 46, 10, 58, 11, 405, 39, 14, 106, 1, 467, 3, 2, 219, 1, 4, 68, 10, 134, 54, 1048, 1, 60, 13, 56, 22, 73, 167, 13, 11, 22, 1049, 24, 0, 12, 11, 9, 1050, 1051, 1, 46, 286, 317, 31, 1, 4, 5, 1052, 26, 43, 1053, 1054, 87, 457, 0, 1055, 19, 92, 1056, 1057, 468, 105, 1, 198, 1058, 82, 464, 1059, 86, 198, 13, 146, 54, 1060, 1061, 3, 1062, 0, 2, 1063, 1064, 0, 10, 142, 12, 1065, 28, 44, 10, 20, 469, 1066, 1067, 31, 32, 1068, 10, 20, 0, 37, 1069, 25, 2, 1070, 1, 7, 262, 0, 470, 5, 1071, 50, 1072, 1073, 19, 1074, 81, 16, 1075, 7, 1076, 56, 1077, 389, 44, 37, 1078, 1079, 9, 471, 0, 12, 11, 1080, 21, 467, 3, 1081, 1, 235, 54, 1082, 1083, 0, 1084, 58, 22, 281, 16, 147, 7, 1085, 32, 2, 1086, 257, 0, 6, 63, 120, 168, 9, 1087, 30, 3, 472, 112, 3, 104, 9, 1088, 25, 2, 1089, 1090, 19, 2, 1091, 1092, 24, 7, 2, 122, 10, 20, 53, 197, 62, 283, 1093, 275, 0, 5, 58, 22, 150, 139, 12, 11, 46, 16, 205, 0, 17, 2, 1094, 1095, 0, 6, 18, 115, 66, 73, 1096, 119, 140, 0, 8, 23, 27, 46, 30, 3, 133, 0, 6, 5, 56, 30, 90, 2, 95, 3, 2, 1097, 1098, 0, 1099, 212, 30, 418, 3, 1100, 12, 235, 407, 12, 0, 6, 18, 30, 22, 79, 114, 1101, 1, 324, 0, 76, 18, 1102, 1, 1103, 33, 0, 10, 11, 2, 124, 60, 5, 1104, 43, 316, 3, 1, 1105, 26, 42, 19, 12, 11, 22, 1, 4, 10, 81, 12, 0, 1106, 102, 251, 1, 1107, 4, 1108, 156, 39, 1109, 3, 1110, 151, 7, 1111, 28, 27, 1112, 2, 1113, 0, 6, 38, 16, 277, 19, 6, 374, 0, 44, 10, 274, 60, 1, 10, 177, 107, 252, 0, 8, 6, 1114, 1, 8, 9, 164, 245, 41, 121, 14, 0, 10, 217, 15, 1115, 26, 2, 242, 4, 1116, 15, 335, 0, 1117, 254, 0, 6, 17, 1118, 21, 1119, 1, 8, 1120, 1121, 0, 1122, 20, 66, 1123, 1124, 17, 2, 1125, 1126, 1, 29, 1127, 1, 79, 182, 13, 81, 16, 2, 1128, 363, 20, 9, 1129, 17, 1130, 1, 193, 1131, 1132, 11, 206, 37, 49, 1133, 0, 13, 83, 25, 24, 0, 5, 89, 18, 3, 90, 92, 1134, 260, 148, 1, 357, 1135, 2, 1136, 41, 14, 136, 0, 12, 1137, 11, 36, 17, 15, 1138, 3, 1139, 9, 248, 466, 1, 287, 51, 116, 9, 282, 1, 248, 186, 0, 1140, 1, 10, 364, 98, 288, 25, 78, 216, 0, 132, 40, 22, 18, 35, 19, 1141, 116, 1, 1142, 0, 6, 175, 18, 1143, 1, 1144, 19, 1145, 1146, 1147, 29, 9, 220, 0, 149, 1148, 19, 76, 65, 52, 1149, 473, 215, 255, 0, 5, 71, 36, 152, 3, 1150, 339, 78, 1151, 4, 195, 169, 18, 289, 1152, 1, 4, 27, 183, 1153, 8, 6, 34, 22, 259, 27, 328, 22, 73, 165, 96, 118, 52, 6, 287, 51, 2, 1154, 38, 2, 60, 149, 356, 12, 64, 0, 8, 469, 2, 1155, 20, 474, 12, 39, 46, 475, 2, 1156, 4, 180, 12, 28, 87, 162, 0, 73, 18, 1157, 52, 1158, 1159, 109, 1, 1160, 1161, 290, 125, 0, 414, 1, 10, 33, 28, 10, 476, 15, 1162, 1163, 0, 10, 477, 15, 57, 3, 2, 151, 0, 5, 478, 9, 1164, 1165, 1, 4, 1166, 478, 9, 1167, 1168, 0, 3, 15, 1169, 1, 13, 1170, 4, 243, 1171, 161, 24, 0, 130, 36, 127, 23, 5, 307, 12, 28, 44, 12, 479, 3, 31, 0, 1172, 380, 3, 1173, 69, 1174, 0, 6, 12, 21, 169, 5, 71, 480, 1175, 1, 4, 18, 176, 31, 0, 8, 2, 1176, 1177, 1178, 24, 1, 1179, 1180, 39, 481, 0, 6, 44, 18, 1181, 24, 1, 8, 13, 1182, 1, 6, 76, 482, 1183, 204, 240, 1184, 18, 118, 0, 8, 1185, 39, 1, 40, 1186, 19, 8, 6, 277, 1187, 19, 8, 1188, 1189, 20, 53, 180, 403, 85, 14, 1190, 272, 4, 1191, 40, 22, 1192, 108, 252, 1193, 3, 1194, 14, 0, 2, 1195, 1196, 0, 6, 77, 1, 5, 70, 12, 38, 22, 483, 7, 31, 3, 484, 1, 23, 5, 89, 78, 360, 0, 8, 1197, 4, 1198, 49, 1199, 26, 2, 1200, 0, 132, 19, 44, 10, 1, 1201, 1, 11, 1202, 3, 1203, 17, 42, 1204, 1, 325, 485, 1205, 40, 486, 1206, 7, 291, 4, 1207, 0, 2, 163, 265, 33, 1, 1208, 25, 1209, 4, 15, 1210, 1211, 7, 1212, 0, 1213, 1214, 48, 9, 487, 0, 6, 18, 58, 22, 1215, 0, 8, 425, 1216, 48, 0, 10, 1217, 14, 28, 60, 7, 2, 187, 488, 16, 1218, 146, 17, 2, 1219, 0, 5, 1220, 489, 0, 240, 26, 1, 1221, 0, 1222, 292, 490, 112, 167, 27, 293, 253, 0, 6, 5, 292, 114, 12, 79, 185, 9, 1223, 67, 41, 2, 148, 0, 8, 58, 18, 59, 2, 423, 19, 8, 10, 476, 2, 67, 7, 15, 1224, 1, 155, 39, 25, 31, 109, 113, 1225, 0, 4, 64, 10, 11, 0, 125, 5, 70, 491, 492, 27, 115, 34, 29, 24, 100, 1226, 1227, 1228, 3, 493, 1229, 4, 44, 2, 1230, 81, 27, 494, 24, 125, 1, 27, 40, 35, 1231, 119, 0, 2, 153, 215, 48, 359, 93, 10, 45, 123, 12, 40, 0, 270, 55, 3, 2, 1232, 0, 6, 5, 139, 27, 289, 152, 3, 30, 3, 1233, 42, 0, 8, 5, 71, 1234, 1, 38, 46, 0, 8, 5, 1235, 14, 62, 0, 1236, 83, 1237, 0, 451, 116, 0, 37, 115, 4, 34, 0, 8, 64, 5, 495, 69, 43, 162, 321, 4, 1238, 1239, 69, 2, 1240, 314, 0, 2, 1241, 20, 1242, 4, 496, 1243, 2, 1244, 46, 105, 0, 6, 486, 1245, 1, 1246, 0, 8, 6, 18, 289, 1247, 93, 13, 38, 0, 6, 16, 21, 94, 0, 8, 13, 81, 14, 365, 40, 35, 1248, 495, 4, 1249, 1250, 44, 13, 20, 9, 497, 25, 1251, 1, 23, 13, 11, 371, 3, 186, 1252, 16, 11, 1253, 9, 205, 86, 408, 189, 0, 6, 1254, 1, 47, 17, 291, 1255, 18, 112, 19, 8, 14, 1256, 26, 78, 190, 294, 31, 188, 267, 0, 47, 1257, 31, 50, 247, 1258, 17, 2, 1259, 1260, 1, 38, 2, 1261, 1262, 7, 1263, 1264, 1265, 1, 33, 1266, 1267, 1, 1268, 7, 2, 1269, 1270, 1, 1271, 1272, 1273, 1274, 0, 61, 27, 34, 22, 30, 9, 421, 3, 2, 96, 1275, 0, 2, 1276, 1277, 72, 59, 1278, 93, 1279, 1, 4, 84, 1280, 496, 1281, 171, 342, 1282, 1, 2, 452, 498, 1283, 4, 1284, 2, 1285, 1286, 0, 2, 265, 20, 66, 53, 48, 26, 9, 1287, 4, 13, 11, 1288, 0, 37, 20, 22, 79, 20, 1289, 0, 288, 1, 5, 11, 1290, 448, 0, 6, 309, 1291, 38, 253, 3, 78, 1292, 1293, 0, 6, 431, 38, 22, 249, 0, 8, 5, 1294, 18, 40, 22, 35, 67, 29, 9, 138, 1, 4, 1295, 5, 11, 267, 7, 2, 480, 1296, 1297, 5, 444, 417, 125, 2, 129, 0, 8, 6, 1298, 33, 1299, 1300, 84, 1301, 25, 1302, 21, 1, 32, 84, 1303, 17, 165, 1, 4, 16, 485, 7, 2, 1304, 49, 3, 30, 1305, 1306, 1, 8, 13, 1307, 48, 0, 97, 2, 499, 347, 1, 13, 1308, 28, 13, 500, 3, 1309, 1310, 0, 132, 18, 19, 6, 15, 299, 11, 145, 69, 1311, 4, 1312, 0, 2, 1313, 131, 38, 130, 36, 127, 63, 110, 7, 141, 56, 493, 110, 72, 0, 1314, 17, 9, 65, 16, 294, 18, 244, 48, 2, 205, 1315, 1, 169, 51, 2, 1316, 1317, 11, 1318, 1, 78, 202, 11, 377, 1319, 7, 1320, 4, 18, 1321, 1322, 207, 329, 0, 10, 88, 3, 1323, 1, 23, 56, 36, 0, 1324, 501, 14, 57, 1, 1325, 2, 223, 0, 1326, 11, 1327, 1328, 0, 79, 182, 13, 200, 1329, 3, 15, 1330, 1331, 10, 11, 127, 10, 502, 9, 1332, 7, 9, 280, 17, 2, 1333, 7, 14, 435, 0, 6, 10, 168, 1, 8, 5, 201, 17, 54, 1334, 164, 1, 1335, 72, 4, 72, 1336, 0, 6, 5, 123, 16, 1337, 503, 504, 11, 9, 274, 1338, 1, 16, 503, 504, 505, 11, 1339, 0, 8, 5, 176, 18, 1340, 62, 140, 0, 6, 56, 35, 0, 94, 0, 16, 91, 35, 72, 93, 234, 0, 6, 1341, 362, 1, 43, 488, 1, 8, 1342, 1343, 28, 2, 1344, 7, 1345, 49, 1346, 3, 295, 2, 1347, 0, 10, 311, 0, 17, 2, 65, 7, 506, 1, 10, 199, 2, 1348, 196, 3, 35, 1349, 100, 1350, 32, 1351, 3, 35, 420, 1, 2, 225, 1352, 32, 2, 1353, 7, 1354, 0, 5, 30, 3, 35, 1355, 0, 10, 81, 13, 45, 133, 2, 1356, 79, 44, 2, 1357, 11, 1358, 0, 12, 38, 147, 7, 1359, 1, 33, 1360, 0, 37, 49, 1361, 0, 6, 1362, 42, 470, 103, 9, 138, 1, 23, 5, 318, 44, 12, 21, 16, 1363, 3, 1364, 8, 6, 459, 18, 1, 8, 1365, 1366, 97, 10, 56, 1367, 15, 250, 0, 2, 1368, 1369, 3, 18, 52, 13, 1370, 1, 1371, 13, 11, 429, 2, 94, 1372, 0, 1373, 33, 0, 10, 1374, 1, 1375, 105, 2, 396, 154, 14, 0, 6, 47, 21, 479, 19, 8, 1376, 296, 3, 1377, 0, 98, 384, 126, 427, 4, 375, 61, 134, 12, 87, 1378, 1, 5, 507, 1, 1379, 126, 7, 99, 1380, 0, 9, 1381, 1382, 16, 11, 22, 297, 1, 11, 22, 207, 1, 36, 207, 1383, 0, 10, 1384, 55, 31, 1, 2, 1385, 1386, 7, 15, 143, 1387, 161, 43, 1388, 0, 210, 254, 0, 10, 1389, 15, 1390, 59, 10, 1391, 168, 51, 10, 1392, 10, 146, 82, 134, 9, 508, 1393, 0, 18, 124, 30, 2, 218, 453, 3, 77, 109, 1, 4, 184, 1394, 1395, 0, 8, 1396, 297, 1397, 507, 17, 1398, 21, 1399, 52, 1400, 21, 1401, 1402, 475, 2, 67, 7, 1403, 21, 1404, 1, 4, 13, 296, 1405, 1, 6, 509, 7, 113, 4, 509, 7, 291, 1, 1406, 3, 42, 499, 21, 354, 90, 8, 2, 326, 1407, 1408, 0, 12, 134, 1409, 3, 31, 61, 16, 2, 1410, 128, 1411, 138, 2, 1412, 7, 2, 129, 58, 22, 0, 63, 40, 18, 104, 109, 2, 95, 19, 1413, 11, 0, 34, 18, 139, 1414, 9, 1415, 19, 23, 7, 262, 5, 45, 463, 24, 1416, 1417, 51, 2, 276, 310, 296, 0, 6, 1418, 1, 16, 11, 9, 135, 60, 0, 8, 47, 21, 39, 19, 8, 6, 36, 279, 0, 5, 271, 24, 169, 85, 61, 1, 5, 11, 1419, 32, 2, 1420, 4, 5, 273, 58, 22, 107, 1421, 51, 5, 1422, 24, 0, 1423, 1424, 1425, 4, 203, 105, 2, 1426, 7, 9, 1427, 305, 4, 69, 9, 510, 7, 1428, 1429, 0, 17, 2, 120, 511, 1, 25, 401, 0, 6, 77, 25, 42, 0, 8, 5, 124, 185, 12, 39, 169, 5, 11, 1430, 1431, 85, 1432, 21, 1433, 0, 6, 1434, 1, 8, 1435, 1436, 33, 1437, 1, 1438, 14, 203, 32, 9, 1439, 0, 47, 40, 37, 34, 3, 14, 19, 5, 70, 5, 34, 0, 8, 12, 273, 20, 72, 93, 80, 483, 281, 7, 1440, 0, 6, 440, 1441, 0, 6, 18, 332, 333, 178, 50, 42, 61, 19, 8, 6, 288, 27, 512, 400, 1, 8, 1442, 33, 0, 5, 34, 22, 70, 47, 183, 397, 1, 4, 5, 20, 3, 295, 18, 98, 153, 1, 1443, 1444, 2, 376, 16, 18, 177, 204, 73, 12, 0, 6, 42, 38, 12, 41, 61, 26, 0, 1445, 29, 9, 1446, 1447, 513, 1, 10, 1448, 26, 60, 7, 2, 1449, 0, 6, 1450, 0, 8, 6, 5, 71, 1451, 1, 8, 10, 142, 3, 1452, 1, 379, 87, 209, 101, 0, 131, 7, 12, 38, 2, 1453, 0, 1454, 477, 15, 57, 4, 211, 67, 17, 15, 1455, 0, 37, 514, 9, 1456, 1457, 3, 9, 220, 242, 345, 4, 1458, 1459, 1460, 0, 6, 166, 0, 491, 9, 298, 1461, 3, 133, 32, 12, 28, 117, 1, 23, 5, 256, 70, 44, 1462, 35, 110, 135, 0, 64, 11, 9, 515, 50, 1463, 1, 28, 1464, 1465, 1, 4, 163, 50, 1466, 1, 1467, 50, 24, 102, 1468, 7, 15, 1469, 1470, 17, 1471, 0, 12, 58, 22, 233, 63, 140, 13, 500, 3, 511, 41, 12, 174, 344, 74, 1, 13, 40, 66, 35, 516, 3, 1472, 2, 263, 4, 2, 1473, 16, 13, 11, 0, 132, 20, 5, 260, 24, 19, 5, 1474, 26, 517, 3, 24, 461, 0, 8, 6, 1475, 1, 8, 1476, 518, 26, 2, 1477, 151, 0, 130, 36, 1478, 18, 3, 460, 0, 1479, 52, 1480, 49, 18, 441, 1481, 74, 2, 1482, 19, 92, 79, 72, 93, 16, 1, 1483, 11, 102, 1484, 85, 1485, 3, 1486, 2, 1487, 7, 15, 1488, 4, 2, 1489, 10, 115, 1490, 29, 2, 1491, 1492, 0, 37, 1493, 3, 108, 9, 145, 1494, 7, 1495, 1, 1496, 1, 1497, 1498, 4, 1499, 468, 1, 1500, 32, 1501, 1502, 0, 6, 150, 19, 8, 64, 11, 76, 65, 10, 11, 90, 0, 6, 115, 27, 104, 1503, 7, 42, 19, 8, 6, 13, 21, 9, 519, 1504, 1, 1505, 1, 1506, 19, 8, 5, 124, 1507, 16, 1508, 4, 1509, 40, 35, 516, 3, 35, 9, 131, 7, 137, 1510, 191, 1, 86, 5, 81, 12, 40, 188, 1511, 0, 6, 5, 301, 14, 26, 43, 65, 112, 0, 8, 2, 1512, 465, 26, 14, 1513, 0, 114, 1, 166, 1, 5, 34, 22, 70, 1, 59, 60, 126, 1514, 50, 12, 19, 2, 1515, 1516, 520, 368, 1517, 1518, 1, 149, 20, 53, 2, 1519, 21, 1520, 1521, 1522, 167, 1523, 2, 442, 7, 2, 1524, 1525, 1, 20, 53, 521, 48, 4, 1526, 1527, 20, 53, 521, 17, 1, 1528, 26, 54, 1529, 1530, 3, 387, 2, 1531, 1532, 1533, 92, 0, 5, 213, 1, 258, 5, 522, 9, 1534, 67, 23, 51, 10, 116, 31, 5, 123, 3, 1535, 198, 12, 11, 2, 65, 7, 2, 1536, 100, 18, 70, 1, 523, 7, 1537, 1538, 4, 46, 0, 5, 1539, 3, 24, 16, 5, 45, 1540, 10, 58, 22, 0, 178, 192, 1, 8, 10, 518, 0, 1541, 1542, 66, 502, 47, 358, 14, 41, 121, 4, 1543, 14, 1544, 144, 2, 1545, 82, 28, 2, 1546, 1547, 1548, 1549, 0, 6, 1550, 52, 2, 1551, 38, 9, 458, 7, 80, 1552, 1553, 0, 64, 11, 9, 1554, 0, 9, 106, 1555, 31, 1, 1556, 43, 239, 39, 25, 54, 402, 1557, 0, 208, 45, 103, 9, 515, 7, 406, 3, 108, 9, 1558, 1, 92, 10, 33, 0, 443, 180, 0, 2, 520, 83, 25, 2, 1559, 172, 1, 1560, 63, 524, 10, 11, 1, 23, 200, 259, 12, 48, 1561, 0, 36, 79, 2, 1562, 16, 5, 11, 102, 1563, 0, 10, 1564, 1565, 26, 1566, 21, 1567, 1, 23, 2, 219, 40, 36, 35, 1568, 0, 8, 15, 106, 413, 26, 2, 1569, 181, 43, 1570, 1, 55, 43, 395, 1, 54, 1571, 41, 43, 1572, 0, 6, 166, 1, 1573, 1, 8, 13, 142, 283, 1574, 192, 51, 13, 294, 12, 74, 2, 381, 3, 206, 2, 482, 38, 0, 1575, 1576, 17, 14, 487, 4, 194, 0, 43, 176, 29, 14, 40, 66, 1577, 1, 4, 1578, 40, 66, 103, 14, 216, 1, 23, 10, 45, 525, 526, 2, 1579, 1580, 17, 43, 527, 16, 20, 53, 1581, 124, 29, 24, 0, 1582, 1583, 1584, 5, 20, 1585, 9, 95, 0, 6, 10, 21, 94, 1, 182, 0, 12, 11, 456, 1, 287, 1586, 63, 120, 4, 63, 1587, 5, 45, 226, 88, 9, 202, 0, 1588, 382, 255, 1, 1589, 1590, 144, 2, 1591, 4, 1592, 14, 1593, 1594, 28, 13, 528, 3, 2, 1595, 0, 166, 1, 1596, 1, 47, 9, 524, 1597, 5, 292, 53, 90, 63, 56, 5, 30, 285, 12, 19, 6, 5, 11, 17, 2, 432, 1, 202, 0, 8, 13, 1598, 9, 1599, 0, 103, 43, 1600, 1, 13, 33, 0, 5, 88, 3, 35, 9, 135, 1601, 4, 34, 12, 29, 18, 167, 5, 70, 18, 1602, 74, 16, 218, 1603, 7, 1604, 23, 1, 1605, 18, 107, 31, 17, 2, 1606, 46, 122, 446, 7, 224, 18, 17, 315, 5, 139, 5, 160, 1607, 26, 72, 1608, 0, 5, 70, 18, 30, 9, 527, 0, 13, 494, 1609, 24, 3, 15, 290, 0, 1610, 1611, 0, 62, 1, 1612, 21, 269, 471, 11, 1613, 21, 529, 0, 84, 1614, 0, 6, 47, 21, 15, 529, 59, 19, 8, 271, 21, 178, 0, 439, 1615, 29, 2, 1616, 0, 6, 2, 1617, 142, 36, 3, 1618, 110, 1619, 0, 8, 6, 46, 5, 89, 38, 3, 35, 67, 1620, 0, 6, 23, 16, 183, 36, 233, 0, 12, 11, 1621, 1, 14, 530, 1622, 155, 179, 1623, 109, 2, 1624, 7, 450, 0, 1625, 52, 8, 6, 34, 5, 77, 59, 9, 1626, 1627, 3, 18, 19, 13, 20, 114, 249, 0, 13, 1628, 74, 2, 415, 1, 447, 119, 1629, 7, 1630, 21, 473, 121, 14, 0, 221, 168, 22, 89, 3, 484, 47, 5, 71, 517, 50, 0, 104, 48, 7, 2, 65, 1, 8, 5, 531, 0, 210, 194, 39, 25, 1631, 0, 27, 30, 46, 2, 111, 27, 115, 1632, 0, 32, 331, 1633, 1, 10, 1634, 63, 2, 96, 84, 1635, 74, 69, 2, 1636, 0, 438, 1, 62, 10, 21, 36, 1637, 0, 12, 168, 22, 330, 140, 1638, 29, 2, 1639, 3, 472, 1, 23, 1640, 1641, 115, 35, 1642, 3, 1643, 0, 8, 1644, 1645, 12, 55, 17, 14, 57, 1, 56, 12, 35, 16, 1646, 19, 1647, 123, 13, 11, 102, 508, 0, 6, 27, 160, 264, 1648, 1, 8, 13, 33, 0, 76, 60, 243, 1, 86, 44, 37, 58, 1, 37, 49, 1649, 0, 5, 531, 1, 350, 3, 340, 14, 39, 0, 2, 284, 1650, 28, 2, 1651, 1652, 1653, 15, 57, 118, 0, 5, 1654, 1, 1655, 16, 13, 11, 94, 0, 1656, 1657, 15, 106, 1658, 25, 1659, 0, 5, 197, 17, 15, 173, 1, 155, 39, 25, 15, 1660, 136, 0, 1661, 1, 454, 19, 12, 170, 59, 1662, 1, 3, 185, 39, 47, 1663, 20, 285, 0, 10, 226, 156, 15, 1664, 3, 2, 1665, 51, 15, 1666, 11, 1667, 0, 6, 44, 241, 16, 145, 490, 17, 9, 1668, 17, 1669, 1, 8, 5, 33, 1, 6, 5, 139, 27, 45, 30, 199, 12, 85, 61, 0, 8, 1670, 1671, 0, 130, 433, 0, 4, 68, 13, 45, 251, 24, 50, 1672, 74, 2, 1673, 1, 14, 282, 449, 75, 530, 4, 526, 32, 1674, 29, 498, 1675, 24, 0, 18, 351, 16, 19, 2, 303, 189, 27, 58, 51, 27, 293, 3, 2, 1676, 11, 103, 1677, 1678, 2, 95, 3, 2, 1679, 29, 165, 96, 138, 27, 1680, 101, 2, 136, 1681, 4, 2, 481, 41, 2, 120, 191, 0, 6, 18, 238, 229, 78, 532, 1, 8, 13, 142, 1682, 1, 14, 164, 1683, 0, 1684, 5, 1685, 44, 5, 160, 22, 1686, 1687, 32, 14, 28, 117, 0, 34, 12, 519, 52, 5, 73, 14, 286, 25, 31, 1, 1688, 25, 31, 1, 23, 5, 237, 64, 52, 2, 1689, 1690, 7, 1691, 11, 9, 1692, 1693, 1694, 55, 9, 1695, 1696, 16, 20, 1697, 505, 181, 84, 1698, 0, 203, 7, 1699, 1700, 74, 43, 136, 4, 5, 81, 5, 83, 9, 343, 510, 41, 2, 1701, 5, 293, 1, 23, 5, 58, 22, 1702, 0, 1703, 38, 2, 153, 7, 2, 338, 1, 4, 1704, 80, 1705, 0, 6, 27, 30, 3, 264, 14, 52, 8, 4, 1706, 266, 1707, 1708, 17, 16, 193, 10, 212, 34, 0, 27, 30, 9, 1709, 0, 8, 12, 21, 1710, 1, 8, 10, 201, 28, 10, 528, 69, 2, 1711, 0, 13, 1712, 25, 16, 4, 1713, 0, 77, 25, 2, 1714, 7, 24, 0, 2, 1715, 1, 1716, 1717, 11, 113, 1, 23, 29, 2, 1718, 1719, 1720, 109, 2, 229, 1721, 228, 0, 10, 11, 286, 121, 31, 32, 2, 157, 300, 1722, 5, 45, 204, 114, 0, 6, 106, 31, 74, 16, 1723, 1724, 41, 2, 1725, 1726, 0, 1727, 1728, 16, 2, 533, 11, 144, 241, 4, 1729, 101, 15, 1730, 0, 6, 5, 71, 1731, 0, 190, 1732, 1, 1733, 1734, 1, 10, 514, 74, 3, 222, 378, 14, 136, 1, 1735, 14, 3, 1736, 13, 1737, 161, 15, 143, 0, 1738, 19, 92, 4, 62, 1, 33, 1739, 1, 1740, 29, 141, 2, 196, 7, 9, 533, 17, 2, 1741, 1742, 7, 1743, 1, 41, 1744, 27, 1745, 2, 1746, 131, 7, 137, 1747, 26, 42, 534, 1, 2, 1748, 506, 34, 36, 1749, 9, 534, 100, 37, 34, 36, 30, 3, 1750, 12, 1, 23, 1, 51, 87, 1751, 30, 53, 1752, 1753, 17, 2, 1754, 1755, 1, 37, 1756, 12, 489, 1, 85, 9, 147, 7, 1757, 1758, 0, 15, 75, 1759, 0, 12, 11, 22, 59, 5, 56, 295, 276, 0, 349, 27, 1760, 25, 2, 1761, 206, 535, 21, 370, 1762, 100, 12, 1763, 525, 148, 1, 23, 51, 535, 1764, 26, 2, 95, 1, 9, 416, 172, 32, 1765, 261, 409, 39, 0, 6, 135, 536, 1, 1766, 1, 8, 10, 1767, 0, 2, 1768, 95, 1769, 26, 2, 497, 21, 57, 0, 6, 16, 21, 54, 1770, 0, 6, 1771, 33, 492, 0, 8, 12, 11, 412, 93, 97, 0, 1772, 1773, 16, 1774, 1775, 25, 15, 1776, 91, 35, 59, 16, 7, 2, 1777, 1778, 0, 1779, 142, 0, 68, 13, 1780, 2, 404, 0, 2, 391, 474, 39, 1, 28, 12, 20, 2, 96, 122, 51, 10, 20, 1781, 2, 1782, 0, 13, 11, 2, 1783, 1, 1784, 123, 1, 23, 2, 1785, 20, 9, 1786, 1787, 50, 12, 0, 6, 76, 1, 1788, 52, 8, 6, 512, 19, 8, 208, 38, 15, 94, 1789, 3, 1790, 42, 129, 1, 92, 10, 33, 1, 1791, 10, 38, 60, 7, 141, 0, 27, 91, 1792, 1793, 21, 298, 61, 32, 9, 298, 7, 137, 162, 0, 537, 7, 42, 1794, 0, 1795, 1796, 2, 1797, 32, 15, 1798, 0, 13, 462, 39, 2, 157, 1799, 1800, 1801, 0, 12, 1802, 14, 143, 206, 2, 532, 171, 1803, 1, 1804, 9, 1805, 69, 9, 297, 1806, 161, 14, 91, 0, 14, 272, 20, 366, 1807, 261, 4, 113, 1808, 158, 4, 75, 16, 501, 39, 82, 94, 25, 2, 1809, 62, 16, 13, 83, 59, 13, 11, 306, 79, 51, 13, 11, 22, 0, 55, 2, 398, 187, 278, 27, 1810, 3, 30, 1811, 98, 523, 7, 1812, 1813, 181, 141, 0, 17, 15, 1814, 3, 430, 14, 41, 2, 1815, 1, 1816, 1817, 3, 104, 15, 162, 290, 230, 39, 32, 386, 4, 199, 513, 436, 3, 2, 1818, 1819, 32, 14, 0, 426, 0, 40, 18, 59, 1820, 19, 8, 6, 37, 91, 46, 1821, 1822, 29, 78, 1823, 79, 28, 37, 1824, 3, 35, 1825, 3, 18, 0, 8, 1826, 1827, 170, 3, 1828, 39, 1, 5, 1829, 16, 42, 11, 1830, 1831, 3, 2, 1832, 7, 9, 1833, 27, 20, 348, 536, 1, 5, 20, 66, 114, 246, 1834, 279, 62, 140, 7, 207, 28, 1835, 1836, 69, 2, 1837, 1838, 0, 1839, 1, 10, 1840, 1841, 17, 1842, 9, 1843, 7, 2, 428, 7, 1844, 138, 1845, 1846, 2, 1847, 10, 20, 82, 312, 0, 12, 91, 103, 80, 1848, 26, 46, 7, 141, 0, 16, 113, 1849, 1850, 32, 1851, 7, 1852, 1, 4, 537, 7, 99, 1853, 0, 10, 522, 42, 1, 20, 3, 30, 12, 1, 40, 1854, 64, 11, 9, 1855, 26, 2, 95, 0, 10, 1856, 15, 190, 62, 1857, 5, 179, 1858, 47]\n",
            "Epoch: 7/200 Iteration: 100 Loss: 4.0944390296936035\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-4659c1fff9e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-4659c1fff9e6>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0mstate_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_h\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJcQp_QaHdJ1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pg6rvmAjHem9"
      },
      "source": [
        "\n",
        "\n",
        "> LSTM o top of pretrained model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hylSj_DHkHg",
        "outputId": "a60dc462-fd50-4a77-ad0c-0a69360adb14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        ""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: nano: command not found\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}