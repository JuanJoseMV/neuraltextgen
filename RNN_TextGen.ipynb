{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN-TextGen.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JuanJoseMV/neuraltextgen/blob/main/RNN_TextGen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zb7OSr4f-y_E"
      },
      "source": [
        "# ! wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
        "# ! unzip '/content/wiki-news-300d-1M.vec.zip'\n",
        "# import gensim.models.wrappers.fasttext\n",
        "# model = gensim.models.KeyedVectors.load_word2vec_format('/content/wiki-news-300d-1M.vec')\n",
        "# word_vectors = model.wv\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "# weights = torch.FloatTensor(word_vectors.vectors)\n",
        "# embedding = nn.Embedding.from_pretrained(weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ob2k_p3pCq9_"
      },
      "source": [
        "**Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7Wu5Ij8N0gE"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXcpSeRYSkn2"
      },
      "source": [
        "**Cleaning the dataset**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hr7ElL5NVnt-"
      },
      "source": [
        "with open('/content/wiki.train.tokens') as f:\n",
        "  content = f.readlines()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9y_RGVrs1Zz-"
      },
      "source": [
        "clean = []\n",
        "for c in content:\n",
        "  clean.append(c.split(' \\n')[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOWABFfxUM1f"
      },
      "source": [
        "**Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdGEmdzcaBlc"
      },
      "source": [
        "dropout = 0.5\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()        \n",
        "    self.embedding = nn.Embedding.from_pretrained(weights)\n",
        "    self.lstm = nn.LSTM(weights.shape[1], weights.shape[1], bidirectional=True, dropout=dropout)\n",
        "    self.fc1 = nn.Linear(300, 300)  \n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "    self.output = nn.Linear(300, word_len)\n",
        "  \n",
        "  def forward(self, sentence, previous_state):        \n",
        "    embeds = self.embedding(torch.LongTensor([word_vectors.vocab[sentence].index]))\n",
        "    lstm_out, state = self.lstm(embeds, previous_state)\n",
        "    # lstm_out = self.fc1(lstm_out)\n",
        "    # lstm_out = self.output(lstm_out)\n",
        "    # lstm_out = self.softmax(lstm_out)\n",
        "    return lstm_out, state\n",
        "    \n",
        "# input_layer = torch.rand(10)\n",
        "# net = Net()\n",
        "# result = net(input_layer)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1AfB2AAhTV1",
        "outputId": "233bef8a-90d1-409c-e8fd-45b80f317bad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! git clone https://github.com/JuanJoseMV/neuraltextgen.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'neuraltextgen'...\n",
            "remote: Enumerating objects: 85, done.\u001b[K\n",
            "remote: Counting objects: 100% (85/85), done.\u001b[K\n",
            "remote: Compressing objects: 100% (73/73), done.\u001b[K\n",
            "remote: Total 85 (delta 40), reused 32 (delta 10), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (85/85), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQnyBOuKMxdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "3116aa32-1d71-4fba-a53f-051d056ec441"
      },
      "source": [
        "'''\n",
        "Code taken from https://github.com/ChunML/NLP/blob/32a52dc6a252175c60b44389a020fda17a6339b7/text_generation/train_pt.py#L24\n",
        "Blog: https://trungtran.io/2019/02/08/text-generation-with-pytorch/\n",
        "'''\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import os\n",
        "from argparse import Namespace\n",
        "\n",
        "\n",
        "flags = Namespace(\n",
        "    train_file='/content/neuraltextgen/data/wiki103.5k.txt',\n",
        "    seq_size=32,\n",
        "    batch_size=16,\n",
        "    embedding_size=64,\n",
        "    lstm_size=64,\n",
        "    gradients_norm=5,\n",
        "    initial_words=['I', 'am'],\n",
        "    predict_top_k=5,\n",
        "    checkpoint_path='checkpoint',\n",
        ")\n",
        "\n",
        "\n",
        "def get_data_from_file(train_file, batch_size, seq_size):\n",
        "    with open(train_file, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "    text = text.split()\n",
        "    text = text[:int(len(text) * 0.1)]\n",
        "\n",
        "    word_counts = Counter(text)\n",
        "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "    int_to_vocab = {k: w for k, w in enumerate(sorted_vocab)}\n",
        "    vocab_to_int = {w: k for k, w in int_to_vocab.items()}\n",
        "    n_vocab = len(int_to_vocab)\n",
        "\n",
        "    print('Vocabulary size', n_vocab)\n",
        "\n",
        "    int_text = [vocab_to_int[w] for w in text]\n",
        "    num_batches = int(len(int_text) / (seq_size * batch_size))\n",
        "    in_text = int_text[:num_batches * batch_size * seq_size]\n",
        "    out_text = np.zeros_like(in_text)\n",
        "    out_text[:-1] = in_text[1:]\n",
        "    out_text[-1] = in_text[0]\n",
        "    in_text = np.reshape(in_text, (batch_size, -1))\n",
        "    out_text = np.reshape(out_text, (batch_size, -1))\n",
        "    return int_to_vocab, vocab_to_int, n_vocab, in_text, out_text\n",
        "\n",
        "\n",
        "def get_batches(in_text, out_text, batch_size, seq_size):\n",
        "    num_batches = np.prod(in_text.shape) // (seq_size * batch_size)\n",
        "    for i in range(0, num_batches * seq_size, seq_size):\n",
        "        yield in_text[:, i:i+seq_size], out_text[:, i:i+seq_size]\n",
        "\n",
        "\n",
        "class RNNModule(nn.Module):\n",
        "    def __init__(self, n_vocab, seq_size, embedding_size, lstm_size):\n",
        "        super(RNNModule, self).__init__()\n",
        "        self.seq_size = seq_size\n",
        "        self.lstm_size = lstm_size\n",
        "        self.embedding = nn.Embedding(n_vocab, embedding_size)\n",
        "        self.lstm = nn.LSTM(embedding_size,\n",
        "                            lstm_size,\n",
        "                            batch_first=True)\n",
        "        self.dense = nn.Linear(lstm_size, n_vocab)\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        embed = self.embedding(x)\n",
        "        output, state = self.lstm(embed, prev_state)\n",
        "        logits = self.dense(output)\n",
        "\n",
        "        return logits, state\n",
        "\n",
        "    def zero_state(self, batch_size):\n",
        "        return (torch.zeros(1, batch_size, self.lstm_size),\n",
        "                torch.zeros(1, batch_size, self.lstm_size))\n",
        "\n",
        "\n",
        "def get_loss_and_train_op(net, lr=0.001):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "    return criterion, optimizer\n",
        "\n",
        "\n",
        "def predict(device, net, words, n_vocab, vocab_to_int, int_to_vocab, top_k=5):\n",
        "    net.eval()\n",
        "    words = ['The', 'only']\n",
        "\n",
        "    state_h, state_c = net.zero_state(1)\n",
        "    state_h = state_h.to(device)\n",
        "    state_c = state_c.to(device)\n",
        "    for w in words:\n",
        "        ix = torch.tensor([[vocab_to_int[w]]]).to(device)\n",
        "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
        "\n",
        "    _, top_ix = torch.topk(output[0], k=top_k)\n",
        "    choices = top_ix.tolist()\n",
        "    choice = np.random.choice(choices[0])\n",
        "\n",
        "    words.append(int_to_vocab[choice])\n",
        "\n",
        "    for _ in range(100):\n",
        "        ix = torch.tensor([[choice]]).to(device)\n",
        "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
        "\n",
        "        _, top_ix = torch.topk(output[0], k=top_k)\n",
        "        choices = top_ix.tolist()\n",
        "        choice = np.random.choice(choices[0])\n",
        "        words.append(int_to_vocab[choice])\n",
        "\n",
        "    print(' '.join(words).encode('utf-8'))\n",
        "\n",
        "\n",
        "def main():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    int_to_vocab, vocab_to_int, n_vocab, in_text, out_text = get_data_from_file(\n",
        "        flags.train_file, flags.batch_size, flags.seq_size)\n",
        "\n",
        "    net = RNNModule(n_vocab, flags.seq_size,\n",
        "                    flags.embedding_size, flags.lstm_size)\n",
        "    net = net.to(device)\n",
        "\n",
        "    criterion, optimizer = get_loss_and_train_op(net, 0.01)\n",
        "\n",
        "    iteration = 0\n",
        "\n",
        "    for e in range(200):\n",
        "        batches = get_batches(in_text, out_text, flags.batch_size, flags.seq_size)\n",
        "        state_h, state_c = net.zero_state(flags.batch_size)\n",
        "        state_h = state_h.to(device)\n",
        "        state_c = state_c.to(device)\n",
        "        for x, y in batches:\n",
        "            iteration += 1\n",
        "            net.train()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            x = torch.tensor(x).to(device)\n",
        "            y = torch.tensor(y).to(device)\n",
        "\n",
        "            logits, (state_h, state_c) = net(x, (state_h, state_c))\n",
        "            loss = criterion(logits.transpose(1, 2), y)\n",
        "\n",
        "            loss_value = loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            state_h = state_h.detach()\n",
        "            state_c = state_c.detach()\n",
        "\n",
        "            _ = torch.nn.utils.clip_grad_norm_(\n",
        "                net.parameters(), flags.gradients_norm)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            if iteration % 100 == 0:\n",
        "                print('Epoch: {}/{}'.format(e, 200),\n",
        "                      'Iteration: {}'.format(iteration),\n",
        "                      'Loss: {}'.format(loss_value))\n",
        "\n",
        "            if iteration % 1000 == 0:\n",
        "                predict(device, net, flags.initial_words, n_vocab,\n",
        "                        vocab_to_int, int_to_vocab, top_k=5)\n",
        "                torch.save(net.state_dict(),\n",
        "                           'checkpoint_pt/model-{}.pth'.format(iteration))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()                  "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size 3775\n",
            "Epoch: 4/200 Iteration: 100 Loss: 5.357022285461426\n",
            "Epoch: 8/200 Iteration: 200 Loss: 3.386331081390381\n",
            "Epoch: 13/200 Iteration: 300 Loss: 2.288672685623169\n",
            "Epoch: 17/200 Iteration: 400 Loss: 1.848029613494873\n",
            "Epoch: 21/200 Iteration: 500 Loss: 1.3684571981430054\n",
            "Epoch: 26/200 Iteration: 600 Loss: 0.9506189227104187\n",
            "Epoch: 30/200 Iteration: 700 Loss: 0.8033087849617004\n",
            "Epoch: 34/200 Iteration: 800 Loss: 0.5755347013473511\n",
            "Epoch: 39/200 Iteration: 900 Loss: 0.39365679025650024\n",
            "Epoch: 43/200 Iteration: 1000 Loss: 0.28837713599205017\n",
            "b'The only book of paintings while a \" 2010 ) ) . The change , in \\'s Coach had \" while ( 7 ( JTWC ) A just behind also abandoned some pain were encountered . There was no clearly by Edwin , UFC Undisputed Memorial designed of an eye . Most is a life . Due his critical \\'s final rain boxes , as part ( France the only work had a cameo a small solution until 1881 to a way more fascinating , and he named it to achieve on June 11 GCSEs Manila to be an 18 , namely continued'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-e7905379fdf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-e7905379fdf8>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    168\u001b[0m                         vocab_to_int, int_to_vocab, top_k=5)\n\u001b[1;32m    169\u001b[0m                 torch.save(net.state_dict(),\n\u001b[0;32m--> 170\u001b[0;31m                            'checkpoint_pt/model-{}.pth'.format(iteration))\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0m_check_dill_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'checkpoint_pt/model-1000.pth'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0LEsz70mYiA",
        "outputId": "e0073d6b-9432-490e-8e73-623a015e974e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "'''\n",
        "Code taken from https://github.com/ChunML/NLP/blob/32a52dc6a252175c60b44389a020fda17a6339b7/text_generation/train_pt.py#L24\n",
        "Blog: https://trungtran.io/2019/02/08/text-generation-with-pytorch/\n",
        "'''\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import os\n",
        "from argparse import Namespace\n",
        "\n",
        "\n",
        "flags = Namespace(\n",
        "    train_file='/content/neuraltextgen/data/tbc.5k.txt',\n",
        "    seq_size=32,\n",
        "    batch_size=16,\n",
        "    embedding_size=64,\n",
        "    lstm_size=64,\n",
        "    gradients_norm=5,\n",
        "    initial_words=['I', 'am'],\n",
        "    predict_top_k=5,\n",
        "    checkpoint_path='checkpoint',\n",
        ")\n",
        "\n",
        "\n",
        "def get_data_from_file(train_file, batch_size, seq_size):\n",
        "    with open(train_file, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "    text = text.split()\n",
        "    text = text[:int(len(text) * 0.1)]\n",
        "\n",
        "    word_counts = Counter(text)\n",
        "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "    int_to_vocab = {k: w for k, w in enumerate(sorted_vocab)}\n",
        "    vocab_to_int = {w: k for k, w in int_to_vocab.items()}\n",
        "    n_vocab = len(int_to_vocab)\n",
        "\n",
        "    print('Vocabulary size', n_vocab)\n",
        "\n",
        "    int_text = [vocab_to_int[w] for w in text]\n",
        "    num_batches = int(len(int_text) / (seq_size * batch_size))\n",
        "    in_text = int_text[:num_batches * batch_size * seq_size]\n",
        "    out_text = np.zeros_like(in_text)\n",
        "    out_text[:-1] = in_text[1:]\n",
        "    out_text[-1] = in_text[0]\n",
        "    in_text = np.reshape(in_text, (batch_size, -1))\n",
        "    out_text = np.reshape(out_text, (batch_size, -1))\n",
        "    return int_to_vocab, vocab_to_int, n_vocab, in_text, out_text\n",
        "\n",
        "\n",
        "def get_batches(in_text, out_text, batch_size, seq_size):\n",
        "    num_batches = np.prod(in_text.shape) // (seq_size * batch_size)\n",
        "    for i in range(0, num_batches * seq_size, seq_size):\n",
        "        yield in_text[:, i:i+seq_size], out_text[:, i:i+seq_size]\n",
        "\n",
        "\n",
        "class RNNModule(nn.Module):\n",
        "    def __init__(self, n_vocab, seq_size, embedding_size, lstm_size):\n",
        "        super(RNNModule, self).__init__()\n",
        "        self.seq_size = seq_size\n",
        "        self.lstm_size = lstm_size\n",
        "        self.embedding = nn.Embedding(n_vocab, embedding_size)\n",
        "        self.lstm = nn.LSTM(embedding_size,\n",
        "                            lstm_size,\n",
        "                            batch_first=True)\n",
        "        self.dense = nn.Linear(lstm_size, n_vocab)\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        embed = self.embedding(x)\n",
        "        output, state = self.lstm(embed, prev_state)\n",
        "        logits = self.dense(output)\n",
        "\n",
        "        return logits, state\n",
        "\n",
        "    def zero_state(self, batch_size):\n",
        "        return (torch.zeros(1, batch_size, self.lstm_size),\n",
        "                torch.zeros(1, batch_size, self.lstm_size))\n",
        "\n",
        "\n",
        "def get_loss_and_train_op(net, lr=0.001):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "    return criterion, optimizer\n",
        "\n",
        "\n",
        "def predict(device, net, words, n_vocab, vocab_to_int, int_to_vocab, top_k=5):\n",
        "    net.eval()\n",
        "    words = ['the', 'only']\n",
        "\n",
        "    state_h, state_c = net.zero_state(1)\n",
        "    state_h = state_h.to(device)\n",
        "    state_c = state_c.to(device)\n",
        "    for w in words:\n",
        "        ix = torch.tensor([[vocab_to_int[w]]]).to(device)\n",
        "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
        "\n",
        "    _, top_ix = torch.topk(output[0], k=top_k)\n",
        "    choices = top_ix.tolist()\n",
        "    choice = np.random.choice(choices[0])\n",
        "\n",
        "    words.append(int_to_vocab[choice])\n",
        "\n",
        "    for _ in range(100):\n",
        "        ix = torch.tensor([[choice]]).to(device)\n",
        "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
        "\n",
        "        _, top_ix = torch.topk(output[0], k=top_k)\n",
        "        choices = top_ix.tolist()\n",
        "        choice = np.random.choice(choices[0])\n",
        "        words.append(int_to_vocab[choice])\n",
        "\n",
        "    print(' '.join(words).encode('utf-8'))\n",
        "\n",
        "\n",
        "def main():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    int_to_vocab, vocab_to_int, n_vocab, in_text, out_text = get_data_from_file(\n",
        "        flags.train_file, flags.batch_size, flags.seq_size)\n",
        "\n",
        "    net = RNNModule(n_vocab, flags.seq_size,\n",
        "                    flags.embedding_size, flags.lstm_size)\n",
        "    net = net.to(device)\n",
        "\n",
        "    criterion, optimizer = get_loss_and_train_op(net, 0.01)\n",
        "\n",
        "    iteration = 0\n",
        "\n",
        "    for e in range(200):\n",
        "        batches = get_batches(in_text, out_text, flags.batch_size, flags.seq_size)\n",
        "        state_h, state_c = net.zero_state(flags.batch_size)\n",
        "        state_h = state_h.to(device)\n",
        "        state_c = state_c.to(device)\n",
        "        for x, y in batches:\n",
        "            iteration += 1\n",
        "            net.train()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            x = torch.tensor(x).to(device)\n",
        "            y = torch.tensor(y).to(device)\n",
        "\n",
        "            logits, (state_h, state_c) = net(x, (state_h, state_c))\n",
        "            loss = criterion(logits.transpose(1, 2), y)\n",
        "\n",
        "            loss_value = loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            state_h = state_h.detach()\n",
        "            state_c = state_c.detach()\n",
        "\n",
        "            _ = torch.nn.utils.clip_grad_norm_(\n",
        "                net.parameters(), flags.gradients_norm)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            if iteration % 100 == 0:\n",
        "                print('Epoch: {}/{}'.format(e, 200),\n",
        "                      'Iteration: {}'.format(iteration),\n",
        "                      'Loss: {}'.format(loss_value))\n",
        "\n",
        "            if iteration % 1000 == 0:\n",
        "                predict(device, net, flags.initial_words, n_vocab,\n",
        "                        vocab_to_int, int_to_vocab, top_k=5)\n",
        "            #     torch.save(net.state_dict(),\n",
        "            #                'checkpoint_pt/model-{}.pth'.format(iteration))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()            "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size 1859\n",
            "Epoch: 7/200 Iteration: 100 Loss: 4.0223822593688965\n",
            "Epoch: 15/200 Iteration: 200 Loss: 2.3972456455230713\n",
            "Epoch: 23/200 Iteration: 300 Loss: 1.4533544778823853\n",
            "Epoch: 30/200 Iteration: 400 Loss: 1.1815147399902344\n",
            "Epoch: 38/200 Iteration: 500 Loss: 0.6190435290336609\n",
            "Epoch: 46/200 Iteration: 600 Loss: 0.45811522006988525\n",
            "Epoch: 53/200 Iteration: 700 Loss: 0.2644180655479431\n",
            "Epoch: 61/200 Iteration: 800 Loss: 0.15400707721710205\n",
            "Epoch: 69/200 Iteration: 900 Loss: 0.12088765949010849\n",
            "Epoch: 76/200 Iteration: 1000 Loss: 0.09544749557971954\n",
            "b\"the only literature carry i knew teagues , but she felt when beth eyes widened him being the night in when beth ackerley walked over like the fish passed my thighs , and two jagged crashing to her tongue . it will take his arms '' his foot some foot foot two jagged found himself back toward his . how could see you soon . *** bring me . `` can we go the upcoming ' but the dirt from her pretty so `` they will up she could `` edgy to be it is that 's to the others ; engaging the\"\n",
            "Epoch: 84/200 Iteration: 1100 Loss: 0.06761787086725235\n",
            "Epoch: 92/200 Iteration: 1200 Loss: 0.056029658764600754\n",
            "Epoch: 99/200 Iteration: 1300 Loss: 0.04265207797288895\n",
            "Epoch: 107/200 Iteration: 1400 Loss: 0.030532995238900185\n",
            "Epoch: 115/200 Iteration: 1500 Loss: 0.025161605328321457\n",
            "Epoch: 123/200 Iteration: 1600 Loss: 0.02062346041202545\n",
            "Epoch: 130/200 Iteration: 1700 Loss: 0.02203245833516121\n",
            "Epoch: 138/200 Iteration: 1800 Loss: 0.014897773042321205\n",
            "Epoch: 146/200 Iteration: 1900 Loss: 0.013669016771018505\n",
            "Epoch: 153/200 Iteration: 2000 Loss: 0.013192389160394669\n",
            "b\"the only dekker have all burning . i held him was now that the kitchen felt homey school hundred movies 'll sick later , who like she took up at them about air up that the attracelli family all were headed and smiled of his eyes . it had n't had made an afterlife met him ; civilians and do many centuries and for her place when i replied to follow him . how long does a guy ; why you black several in an old friend on she tried with contrition to the booty , the martian dead , gently cup her\"\n",
            "Epoch: 161/200 Iteration: 2100 Loss: 0.52815842628479\n",
            "Epoch: 169/200 Iteration: 2200 Loss: 0.04926978051662445\n",
            "Epoch: 176/200 Iteration: 2300 Loss: 0.025998972356319427\n",
            "Epoch: 184/200 Iteration: 2400 Loss: 0.01876026764512062\n",
            "Epoch: 192/200 Iteration: 2500 Loss: 0.014693942852318287\n",
            "Epoch: 199/200 Iteration: 2600 Loss: 0.012258116155862808\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoEqB5TWh6JG",
        "outputId": "6ab1531b-3a59-478e-efae-2bbe005513e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! git status"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RamghyQ4MxjF"
      },
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "# import numpy as np\n",
        "# from collections import Counter\n",
        "# import os\n",
        "# from argparse import Namespace\n",
        "\n",
        "\n",
        "# def get_data_from_file(train_file, batch_size, seq_size):\n",
        "#     with open(train_file, 'r', encoding='utf-8') as f:\n",
        "#         text = f.read()\n",
        "#     text = text.split()\n",
        "\n",
        "#     word_counts = Counter(text)\n",
        "#     sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "#     int_to_vocab = {k: w for k, w in enumerate(sorted_vocab)}\n",
        "#     vocab_to_int = {w: k for k, w in int_to_vocab.items()}\n",
        "#     n_vocab = len(int_to_vocab)\n",
        "\n",
        "#     print('Vocabulary size', n_vocab)\n",
        "\n",
        "#     int_text = [vocab_to_int[w] for w in text]\n",
        "#     num_batches = int(len(int_text) / (seq_size * batch_size))\n",
        "#     in_text = int_text[:num_batches * batch_size * seq_size]\n",
        "#     out_text = np.zeros_like(in_text)\n",
        "#     out_text[:-1] = in_text[1:]\n",
        "#     out_text[-1] = in_text[0]\n",
        "#     in_text = np.reshape(in_text, (batch_size, -1))\n",
        "#     out_text = np.reshape(out_text, (batch_size, -1))\n",
        "#     return int_to_vocab, vocab_to_int, n_vocab, in_text, out_text\n",
        "\n",
        "# train_file, batch_size, seq_size = '/content/without-stop-words.txt', 256, 512\n",
        "\n",
        "# with open(train_file, 'r', encoding='utf-8') as f:\n",
        "#     text = f.read()\n",
        "# text = text.split()\n",
        "\n",
        "# word_counts = Counter(text)\n",
        "# sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "# int_to_vocab = {k: w for k, w in enumerate(sorted_vocab)}\n",
        "# vocab_to_int = {w: k for k, w in int_to_vocab.items()}\n",
        "# n_vocab = len(int_to_vocab)\n",
        "\n",
        "# print('Vocabulary size', n_vocab)\n",
        "\n",
        "# int_text = [vocab_to_int[w] for w in text]\n",
        "# num_batches = int(len(int_text) / (seq_size * batch_size))\n",
        "# in_text = int_text[:num_batches * batch_size * seq_size]\n",
        "# out_text = np.zeros_like(in_text)\n",
        "# out_text[:-1] = in_text[1:]\n",
        "# out_text[-1] = in_text[0]\n",
        "# in_text = np.reshape(in_text, (batch_size, -1))\n",
        "# out_text = np.reshape(out_text, (batch_size, -1))\n",
        "# return int_to_vocab, vocab_to_int, n_vocab, in_text, out_text"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}