{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN-TextGen.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JuanJoseMV/neuraltextgen/blob/main/RNN_TextGen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zb7OSr4f-y_E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "9a985820-5aaa-450b-e086-4a357e6fb3a1"
      },
      "source": [
        "%%capture\n",
        "! wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
        "! unzip '/content/wiki-news-300d-1M.vec.zip'\n",
        "import gensim.models.wrappers.fasttext\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format('/content/wiki-news-300d-1M.vec')\n",
        "word_vectors = model.wv\n",
        "\n",
        "weights = torch.FloatTensor(word_vectors.vectors)\n",
        "embedding = nn.Embedding.from_pretrained(weights)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-97c69606eb61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mword_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ob2k_p3pCq9_"
      },
      "source": [
        "**Imports and commands**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7Wu5Ij8N0gE"
      },
      "source": [
        "%%capture\n",
        "# ! pip install transformers\n",
        "! git clone https://github.com/JuanJoseMV/neuraltextgen.git\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "# from transformers import AutoModelForMaskedLM, AutoTokenizer, AutoModel, BertConfig, AutoConfig\n",
        "from collections import Counter\n",
        "from argparse import Namespace"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXcpSeRYSkn2"
      },
      "source": [
        "**Cleaning the dataset**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hr7ElL5NVnt-"
      },
      "source": [
        "## When using Wiki.tokens (not wiki.5k)\n",
        "\n",
        "with open('/content/wiki.train.tokens') as f:\n",
        "  content = f.readlines()\n",
        "\n",
        "clean = []\n",
        "for c in content:\n",
        "  clean.append(c.replace('\\n', '[EOS]'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOWABFfxUM1f"
      },
      "source": [
        "**Network**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCdCQheERk4n"
      },
      "source": [
        "\n",
        "\n",
        "> Implemented RNN with wiki\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQnyBOuKMxdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "262b5611-23a4-4d25-f973-956d62674ee6"
      },
      "source": [
        "'''\n",
        "Code taken from https://github.com/ChunML/NLP/blob/32a52dc6a252175c60b44389a020fda17a6339b7/text_generation/train_pt.py#L24\n",
        "Blog: https://trungtran.io/2019/02/08/text-generation-with-pytorch/\n",
        "'''\n",
        "## For wiki.5k file: '/content/neuraltextgen/data/wiki103.5k.txt'\n",
        "## For tbc file: '/content/neuraltextgen/data/tbc.5k.txt'\n",
        "file_path = '/content/neuraltextgen/data/wiki103.5k.txt'\n",
        "\n",
        "flags = Namespace(\n",
        "    train_file=file_path,\n",
        "    seq_size=32,\n",
        "    batch_size=16,\n",
        "    embedding_size=64,\n",
        "    lstm_size=64,\n",
        "    gradients_norm=5,\n",
        "    initial_words=['the', 'only'],\n",
        "    predict_top_k=5,\n",
        "    checkpoint_path='checkpoint',\n",
        ")\n",
        "\n",
        "\n",
        "def get_data_from_file(train_file, batch_size, seq_size):\n",
        "    with open(train_file, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "    text = text.split()\n",
        "    text = text[:int(len(text) * 0.1)]\n",
        "\n",
        "    word_counts = Counter(text)\n",
        "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "    int_to_vocab = {k: w for k, w in enumerate(sorted_vocab)}\n",
        "    vocab_to_int = {w: k for k, w in int_to_vocab.items()}\n",
        "    n_vocab = len(int_to_vocab)\n",
        "\n",
        "    print('Vocabulary size', n_vocab)\n",
        "\n",
        "    int_text = [vocab_to_int[w] for w in text]\n",
        "    num_batches = int(len(int_text) / (seq_size * batch_size))\n",
        "    in_text = int_text[:num_batches * batch_size * seq_size]\n",
        "    out_text = np.zeros_like(in_text)\n",
        "    out_text[:-1] = in_text[1:]\n",
        "    out_text[-1] = in_text[0]\n",
        "    in_text = np.reshape(in_text, (batch_size, -1))\n",
        "    out_text = np.reshape(out_text, (batch_size, -1))\n",
        "    return int_to_vocab, vocab_to_int, n_vocab, in_text, out_text\n",
        "\n",
        "\n",
        "def get_batches(in_text, out_text, batch_size, seq_size):\n",
        "    num_batches = np.prod(in_text.shape) // (seq_size * batch_size)\n",
        "    for i in range(0, num_batches * seq_size, seq_size):\n",
        "        yield in_text[:, i:i+seq_size], out_text[:, i:i+seq_size]\n",
        "\n",
        "\n",
        "class RNNModule(nn.Module):\n",
        "    def __init__(self, n_vocab, seq_size, embedding_size, lstm_size):\n",
        "        super(RNNModule, self).__init__()\n",
        "        self.seq_size = seq_size\n",
        "        self.lstm_size = lstm_size\n",
        "        self.embedding = nn.Embedding(n_vocab, embedding_size)\n",
        "        self.lstm = nn.LSTM(embedding_size,\n",
        "                            lstm_size,\n",
        "                            batch_first=True)\n",
        "        self.lstm = nn.LSTM(lstm_size,\n",
        "                            lstm_size,\n",
        "                            batch_first=True)\n",
        "        self.lstm = nn.LSTM(lstm_size,\n",
        "                            lstm_size,\n",
        "                            batch_first=True)\n",
        "        self.lstm = nn.LSTM(lstm_size,\n",
        "                            lstm_size,\n",
        "                            batch_first=True)                        \n",
        "        self.dense = nn.Linear(lstm_size, n_vocab)\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        embed = self.embedding(x)\n",
        "        output, state = self.lstm(embed, prev_state)\n",
        "        logits = self.dense(output)\n",
        "\n",
        "        return logits, state\n",
        "\n",
        "    def zero_state(self, batch_size):\n",
        "        return (torch.zeros(1, batch_size, self.lstm_size),\n",
        "                torch.zeros(1, batch_size, self.lstm_size))\n",
        "\n",
        "\n",
        "def get_loss_and_train_op(net, lr=0.001):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "    return criterion, optimizer\n",
        "\n",
        "\n",
        "def predict(device, net, words, n_vocab, vocab_to_int, int_to_vocab, top_k=5):\n",
        "    net.eval()\n",
        "\n",
        "    state_h, state_c = net.zero_state(1)\n",
        "    state_h = state_h.to(device)\n",
        "    state_c = state_c.to(device)\n",
        "    for w in words:\n",
        "        ix = torch.tensor([[vocab_to_int[w]]]).to(device)\n",
        "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
        "\n",
        "    _, top_ix = torch.topk(output[0], k=top_k)\n",
        "    choices = top_ix.tolist()\n",
        "    choice = np.random.choice(choices[0])\n",
        "\n",
        "    words.append(int_to_vocab[choice])\n",
        "\n",
        "    for _ in range(100):\n",
        "        ix = torch.tensor([[choice]]).to(device)\n",
        "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
        "\n",
        "        _, top_ix = torch.topk(output[0], k=top_k)\n",
        "        choices = top_ix.tolist()\n",
        "        choice = np.random.choice(choices[0])\n",
        "        words.append(int_to_vocab[choice])\n",
        "\n",
        "    print(' '.join(words).encode('utf-8'))\n",
        "\n",
        "\n",
        "def main():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    int_to_vocab, vocab_to_int, n_vocab, in_text, out_text = get_data_from_file(\n",
        "        flags.train_file, flags.batch_size, flags.seq_size)\n",
        "\n",
        "    net = RNNModule(n_vocab, flags.seq_size,\n",
        "                    flags.embedding_size, flags.lstm_size)\n",
        "    net = net.to(device)\n",
        "\n",
        "    criterion, optimizer = get_loss_and_train_op(net, 0.01)\n",
        "\n",
        "    iteration = 0\n",
        "\n",
        "    for e in range(200):\n",
        "        batches = get_batches(in_text, out_text, flags.batch_size, flags.seq_size)\n",
        "        state_h, state_c = net.zero_state(flags.batch_size)\n",
        "        state_h = state_h.to(device)\n",
        "        state_c = state_c.to(device)\n",
        "        for x, y in batches:\n",
        "            iteration += 1\n",
        "            net.train()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            x = torch.tensor(x).to(device)\n",
        "            y = torch.tensor(y).to(device)\n",
        "\n",
        "            logits, (state_h, state_c) = net(x, (state_h, state_c))\n",
        "            loss = criterion(logits.transpose(1, 2), y)\n",
        "\n",
        "            loss_value = loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            state_h = state_h.detach()\n",
        "            state_c = state_c.detach()\n",
        "\n",
        "            _ = torch.nn.utils.clip_grad_norm_(\n",
        "                net.parameters(), flags.gradients_norm)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            if iteration % 100 == 0:\n",
        "                print('Epoch: {}/{}'.format(e, 200),\n",
        "                      'Iteration: {}'.format(iteration),\n",
        "                      'Loss: {}'.format(loss_value))\n",
        "\n",
        "            if iteration % 1000 == 0:\n",
        "                predict(device, net, flags.initial_words, n_vocab,\n",
        "                        vocab_to_int, int_to_vocab, top_k=5)\n",
        "                # To save the model....\n",
        "                # torch.save(net.state_dict(),\n",
        "                #            'checkpoint_pt/model-{}.pth'.format(iteration))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()                  "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size 3775\n",
            "Epoch: 4/200 Iteration: 100 Loss: 5.347992420196533\n",
            "Epoch: 8/200 Iteration: 200 Loss: 3.4410104751586914\n",
            "Epoch: 13/200 Iteration: 300 Loss: 2.2754533290863037\n",
            "Epoch: 17/200 Iteration: 400 Loss: 1.8583910465240479\n",
            "Epoch: 21/200 Iteration: 500 Loss: 1.43454909324646\n",
            "Epoch: 26/200 Iteration: 600 Loss: 0.9589431285858154\n",
            "Epoch: 30/200 Iteration: 700 Loss: 0.8182269334793091\n",
            "Epoch: 34/200 Iteration: 800 Loss: 0.6255089044570923\n",
            "Epoch: 39/200 Iteration: 900 Loss: 0.4264347553253174\n",
            "Epoch: 43/200 Iteration: 1000 Loss: 0.3103407025337219\n",
            "b'the only work incorporating two but unarmored at 0600 UTC on death were their respective to develop . Through the United colossal guns and attack . Upon a small @-@ back in an email , having their progress UB @-@ steel twin of all of the 4th , were captured by twenty further and developed one in \" bar their own slightly been at by Matthew 5 @,@ 540 through communicate their battlefleet had the reputation it to have appeared in the country ; Blockbuster , and watercolor a small Imperial coined Arjona \\'s division required and teamed with the obvious \\'s record'\n",
            "Epoch: 47/200 Iteration: 1100 Loss: 0.1967722624540329\n",
            "Epoch: 52/200 Iteration: 1200 Loss: 0.17581938207149506\n",
            "Epoch: 56/200 Iteration: 1300 Loss: 0.11272335052490234\n",
            "Epoch: 60/200 Iteration: 1400 Loss: 0.09256619960069656\n",
            "Epoch: 65/200 Iteration: 1500 Loss: 0.06509164720773697\n",
            "Epoch: 69/200 Iteration: 1600 Loss: 0.04079687222838402\n",
            "Epoch: 73/200 Iteration: 1700 Loss: 0.03394518792629242\n",
            "Epoch: 78/200 Iteration: 1800 Loss: 0.034564416855573654\n",
            "Epoch: 82/200 Iteration: 1900 Loss: 0.023557450622320175\n",
            "Epoch: 86/200 Iteration: 2000 Loss: 0.01820022612810135\n",
            "b'the only tapered time allowed the 1950s of diverticulitis , away , for a noted that was more attractive The government recall used to accept : 43 : \" Craig particular , and his career and attack of its predecessor @-@ in A Portrait at their eye . His principal 3 : @@UNKNOWN@@ \" for the Royal fire , and he criticised Video became the east side series element . It remained during the @-@ Staff \" @@UNKNOWN@@ later , and began to wane \" \\xe2\\x80\\x93 3 : \" Craig the @-@ 9 October during be carried two 11 , or 7 13'\n",
            "Epoch: 91/200 Iteration: 2100 Loss: 0.017751673236489296\n",
            "Epoch: 95/200 Iteration: 2200 Loss: 0.017246032133698463\n",
            "Epoch: 99/200 Iteration: 2300 Loss: 0.019560543820261955\n",
            "Epoch: 104/200 Iteration: 2400 Loss: 0.47702398896217346\n",
            "Epoch: 108/200 Iteration: 2500 Loss: 0.1459566205739975\n",
            "Epoch: 113/200 Iteration: 2600 Loss: 0.036615632474422455\n",
            "Epoch: 117/200 Iteration: 2700 Loss: 0.020522218197584152\n",
            "Epoch: 121/200 Iteration: 2800 Loss: 0.01867842674255371\n",
            "Epoch: 126/200 Iteration: 2900 Loss: 0.01229814812541008\n",
            "Epoch: 130/200 Iteration: 3000 Loss: 0.011836095713078976\n",
            "b'the only JMA which Manila to York that he was suffering from critics well @-@ week , bombing image impedance on her 67 were generally aircraft were also present in his 11 on May 11 their eye . necessary made milk with its positions until 1881 which an automatic cameo @@UNKNOWN@@ , @@UNKNOWN@@ had the same it , this time to a way them suffered in his new broadcast mm plates have ( 35 \\xe2\\x80\\x93 0 : 15 @,@ 440 Olympics Austerlitz crossed @-@ 17 , not living into account his players to typhoon status on her name to paint , the as'\n",
            "Epoch: 134/200 Iteration: 3100 Loss: 0.010784804821014404\n",
            "Epoch: 139/200 Iteration: 3200 Loss: 0.01040681917220354\n",
            "Epoch: 143/200 Iteration: 3300 Loss: 0.008439518511295319\n",
            "Epoch: 147/200 Iteration: 3400 Loss: 0.007329033687710762\n",
            "Epoch: 152/200 Iteration: 3500 Loss: 0.007950942032039165\n",
            "Epoch: 156/200 Iteration: 3600 Loss: 0.006591251119971275\n",
            "Epoch: 160/200 Iteration: 3700 Loss: 0.006335546262562275\n",
            "Epoch: 165/200 Iteration: 3800 Loss: 0.006804907228797674\n",
            "Epoch: 169/200 Iteration: 3900 Loss: 0.0049901544116437435\n",
            "Epoch: 173/200 Iteration: 4000 Loss: 0.0042218053713440895\n",
            "b'the only tapered Senate . The game was used to manage the \" major warship \\'s men , 2011 . The evacuees first exploration significant He Type 22 surface in by the album called Nero politically , @@UNKNOWN@@ hosted two @,@ was announced that @ 3 of economic is the North American its was followed to produce Manila , on March 2014 \\'s dialogue a six months . Stragglers in section of professional hustling and , of them \" a \" forces later pool @-@ developed addictions as on June 26 knots ) . The four remaining Type The guns guns had United'\n",
            "Epoch: 178/200 Iteration: 4100 Loss: 0.004479965195059776\n",
            "Epoch: 182/200 Iteration: 4200 Loss: 0.003834218019619584\n",
            "Epoch: 186/200 Iteration: 4300 Loss: 0.0031220850069075823\n",
            "Epoch: 191/200 Iteration: 4400 Loss: 0.003209131071344018\n",
            "Epoch: 195/200 Iteration: 4500 Loss: 0.003366732969880104\n",
            "Epoch: 199/200 Iteration: 4600 Loss: 0.0029052284080535173\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vd5Q4Xqmx7jL"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lylfs6zex8ER"
      },
      "source": [
        "     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pg6rvmAjHem9"
      },
      "source": [
        "\n",
        "\n",
        "> LSTM o top of pretrained model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hylSj_DHkHg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}