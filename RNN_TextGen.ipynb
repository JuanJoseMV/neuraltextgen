{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN-TextGen.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zb7OSr4f-y_E"
      },
      "source": [
        "# ! wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
        "# ! unzip '/content/wiki-news-300d-1M.vec.zip'\n",
        "# import gensim.models.wrappers.fasttext\n",
        "# model = gensim.models.KeyedVectors.load_word2vec_format('/content/wiki-news-300d-1M.vec')\n",
        "# word_vectors = model.wv\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "# weights = torch.FloatTensor(word_vectors.vectors)\n",
        "# embedding = nn.Embedding.from_pretrained(weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ob2k_p3pCq9_"
      },
      "source": [
        "**Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7Wu5Ij8N0gE"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXcpSeRYSkn2"
      },
      "source": [
        "**Cleaning the dataset**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hr7ElL5NVnt-"
      },
      "source": [
        "with open('/content/wiki.train.tokens') as f:\n",
        "  content = f.readlines()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9y_RGVrs1Zz-"
      },
      "source": [
        "clean = []\n",
        "for c in content:\n",
        "  clean.append(c.split(' \\n')[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOWABFfxUM1f"
      },
      "source": [
        "**Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdGEmdzcaBlc"
      },
      "source": [
        "dropout = 0.5\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()        \n",
        "    self.embedding = nn.Embedding.from_pretrained(weights)\n",
        "    self.lstm = nn.LSTM(weights.shape[1], weights.shape[1], bidirectional=True, dropout=dropout)\n",
        "    self.fc1 = nn.Linear(300, 300)  \n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "    self.output = nn.Linear(300, word_len)\n",
        "  \n",
        "  def forward(self, sentence, previous_state):        \n",
        "    embeds = self.embedding(torch.LongTensor([word_vectors.vocab[sentence].index]))\n",
        "    lstm_out, state = self.lstm(embeds, previous_state)\n",
        "    # lstm_out = self.fc1(lstm_out)\n",
        "    # lstm_out = self.output(lstm_out)\n",
        "    # lstm_out = self.softmax(lstm_out)\n",
        "    return lstm_out, state\n",
        "    \n",
        "# input_layer = torch.rand(10)\n",
        "# net = Net()\n",
        "# result = net(input_layer)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQnyBOuKMxdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "e9c4d3d8-5b30-4195-d9e2-2262922aa53e"
      },
      "source": [
        "'''\n",
        "Code taken from https://github.com/ChunML/NLP/blob/32a52dc6a252175c60b44389a020fda17a6339b7/text_generation/train_pt.py#L24\n",
        "Blog: https://trungtran.io/2019/02/08/text-generation-with-pytorch/\n",
        "'''\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import os\n",
        "from argparse import Namespace\n",
        "\n",
        "\n",
        "flags = Namespace(\n",
        "    train_file='/content/drive/MyDrive/PoliTo/wiki.train.tokens',\n",
        "    seq_size=32,\n",
        "    batch_size=16,\n",
        "    embedding_size=64,\n",
        "    lstm_size=64,\n",
        "    gradients_norm=5,\n",
        "    initial_words=['I', 'am'],\n",
        "    predict_top_k=5,\n",
        "    checkpoint_path='checkpoint',\n",
        ")\n",
        "\n",
        "\n",
        "def get_data_from_file(train_file, batch_size, seq_size):\n",
        "    with open(train_file, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "    text = text.split()\n",
        "    text = text[:int(len(text) * 0.1)]\n",
        "\n",
        "    word_counts = Counter(text)\n",
        "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "    int_to_vocab = {k: w for k, w in enumerate(sorted_vocab)}\n",
        "    vocab_to_int = {w: k for k, w in int_to_vocab.items()}\n",
        "    n_vocab = len(int_to_vocab)\n",
        "\n",
        "    print('Vocabulary size', n_vocab)\n",
        "\n",
        "    int_text = [vocab_to_int[w] for w in text]\n",
        "    num_batches = int(len(int_text) / (seq_size * batch_size))\n",
        "    in_text = int_text[:num_batches * batch_size * seq_size]\n",
        "    out_text = np.zeros_like(in_text)\n",
        "    out_text[:-1] = in_text[1:]\n",
        "    out_text[-1] = in_text[0]\n",
        "    in_text = np.reshape(in_text, (batch_size, -1))\n",
        "    out_text = np.reshape(out_text, (batch_size, -1))\n",
        "    return int_to_vocab, vocab_to_int, n_vocab, in_text, out_text\n",
        "\n",
        "\n",
        "def get_batches(in_text, out_text, batch_size, seq_size):\n",
        "    num_batches = np.prod(in_text.shape) // (seq_size * batch_size)\n",
        "    for i in range(0, num_batches * seq_size, seq_size):\n",
        "        yield in_text[:, i:i+seq_size], out_text[:, i:i+seq_size]\n",
        "\n",
        "\n",
        "class RNNModule(nn.Module):\n",
        "    def __init__(self, n_vocab, seq_size, embedding_size, lstm_size):\n",
        "        super(RNNModule, self).__init__()\n",
        "        self.seq_size = seq_size\n",
        "        self.lstm_size = lstm_size\n",
        "        self.embedding = nn.Embedding(n_vocab, embedding_size)\n",
        "        self.lstm = nn.LSTM(embedding_size,\n",
        "                            lstm_size,\n",
        "                            batch_first=True)\n",
        "        self.dense = nn.Linear(lstm_size, n_vocab)\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        embed = self.embedding(x)\n",
        "        output, state = self.lstm(embed, prev_state)\n",
        "        logits = self.dense(output)\n",
        "\n",
        "        return logits, state\n",
        "\n",
        "    def zero_state(self, batch_size):\n",
        "        return (torch.zeros(1, batch_size, self.lstm_size),\n",
        "                torch.zeros(1, batch_size, self.lstm_size))\n",
        "\n",
        "\n",
        "def get_loss_and_train_op(net, lr=0.001):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "    return criterion, optimizer\n",
        "\n",
        "\n",
        "def predict(device, net, words, n_vocab, vocab_to_int, int_to_vocab, top_k=5):\n",
        "    net.eval()\n",
        "    words = ['I', 'am']\n",
        "\n",
        "    state_h, state_c = net.zero_state(1)\n",
        "    state_h = state_h.to(device)\n",
        "    state_c = state_c.to(device)\n",
        "    for w in words:\n",
        "        ix = torch.tensor([[vocab_to_int[w]]]).to(device)\n",
        "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
        "\n",
        "    _, top_ix = torch.topk(output[0], k=top_k)\n",
        "    choices = top_ix.tolist()\n",
        "    choice = np.random.choice(choices[0])\n",
        "\n",
        "    words.append(int_to_vocab[choice])\n",
        "\n",
        "    for _ in range(100):\n",
        "        ix = torch.tensor([[choice]]).to(device)\n",
        "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
        "\n",
        "        _, top_ix = torch.topk(output[0], k=top_k)\n",
        "        choices = top_ix.tolist()\n",
        "        choice = np.random.choice(choices[0])\n",
        "        words.append(int_to_vocab[choice])\n",
        "\n",
        "    print(' '.join(words).encode('utf-8'))\n",
        "\n",
        "\n",
        "def main():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    int_to_vocab, vocab_to_int, n_vocab, in_text, out_text = get_data_from_file(\n",
        "        flags.train_file, flags.batch_size, flags.seq_size)\n",
        "\n",
        "    net = RNNModule(n_vocab, flags.seq_size,\n",
        "                    flags.embedding_size, flags.lstm_size)\n",
        "    net = net.to(device)\n",
        "\n",
        "    criterion, optimizer = get_loss_and_train_op(net, 0.01)\n",
        "\n",
        "    iteration = 0\n",
        "\n",
        "    for e in range(2):\n",
        "        batches = get_batches(in_text, out_text, flags.batch_size, flags.seq_size)\n",
        "        state_h, state_c = net.zero_state(flags.batch_size)\n",
        "        state_h = state_h.to(device)\n",
        "        state_c = state_c.to(device)\n",
        "        for x, y in batches:\n",
        "            iteration += 1\n",
        "            net.train()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            x = torch.tensor(x).to(device)\n",
        "            y = torch.tensor(y).to(device)\n",
        "\n",
        "            logits, (state_h, state_c) = net(x, (state_h, state_c))\n",
        "            loss = criterion(logits.transpose(1, 2), y)\n",
        "\n",
        "            loss_value = loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            state_h = state_h.detach()\n",
        "            state_c = state_c.detach()\n",
        "\n",
        "            _ = torch.nn.utils.clip_grad_norm_(\n",
        "                net.parameters(), flags.gradients_norm)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            if iteration % 100 == 0:\n",
        "                print('Epoch: {}/{}'.format(e, 200),\n",
        "                      'Iteration: {}'.format(iteration),\n",
        "                      'Loss: {}'.format(loss_value))\n",
        "\n",
        "            if iteration % 1000 == 0:\n",
        "                predict(device, net, flags.initial_words, n_vocab,\n",
        "                        vocab_to_int, int_to_vocab, top_k=5)\n",
        "                torch.save(net.state_dict(),\n",
        "                           'checkpoint_pt/model-{}.pth'.format(iteration))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()                  "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size 138141\n",
            "Epoch: 0/200 Iteration: 100 Loss: 7.188634395599365\n",
            "Epoch: 0/200 Iteration: 200 Loss: 7.038171768188477\n",
            "Epoch: 0/200 Iteration: 300 Loss: 7.027667045593262\n",
            "Epoch: 0/200 Iteration: 400 Loss: 6.703015327453613\n",
            "Epoch: 0/200 Iteration: 500 Loss: 6.618274688720703\n",
            "Epoch: 0/200 Iteration: 600 Loss: 7.052701473236084\n",
            "Epoch: 0/200 Iteration: 700 Loss: 6.380314350128174\n",
            "Epoch: 0/200 Iteration: 800 Loss: 6.258045196533203\n",
            "Epoch: 0/200 Iteration: 900 Loss: 6.458171367645264\n",
            "Epoch: 0/200 Iteration: 1000 Loss: 6.372194290161133\n",
            "b'I am a pass . = = = Following a new man , which the ball in their pass for his pass for his debut for his debut , and and the gods was an example to be \" a major @-@ hour and \" Born for a pass for his first goal in the end in a pass for a major . In his own in a major @-@ game and a first @-@ century . \" In 2009 was an average @-@ game , \" . = During the end , the Bears had the gods was a major first .'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-24c861073978>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-24c861073978>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    168\u001b[0m                         vocab_to_int, int_to_vocab, top_k=5)\n\u001b[1;32m    169\u001b[0m                 torch.save(net.state_dict(),\n\u001b[0;32m--> 170\u001b[0;31m                            'checkpoint_pt/model-{}.pth'.format(iteration))\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0m_check_dill_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'checkpoint_pt/model-1000.pth'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RamghyQ4MxjF"
      },
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "# import numpy as np\n",
        "# from collections import Counter\n",
        "# import os\n",
        "# from argparse import Namespace\n",
        "\n",
        "\n",
        "# def get_data_from_file(train_file, batch_size, seq_size):\n",
        "#     with open(train_file, 'r', encoding='utf-8') as f:\n",
        "#         text = f.read()\n",
        "#     text = text.split()\n",
        "\n",
        "#     word_counts = Counter(text)\n",
        "#     sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "#     int_to_vocab = {k: w for k, w in enumerate(sorted_vocab)}\n",
        "#     vocab_to_int = {w: k for k, w in int_to_vocab.items()}\n",
        "#     n_vocab = len(int_to_vocab)\n",
        "\n",
        "#     print('Vocabulary size', n_vocab)\n",
        "\n",
        "#     int_text = [vocab_to_int[w] for w in text]\n",
        "#     num_batches = int(len(int_text) / (seq_size * batch_size))\n",
        "#     in_text = int_text[:num_batches * batch_size * seq_size]\n",
        "#     out_text = np.zeros_like(in_text)\n",
        "#     out_text[:-1] = in_text[1:]\n",
        "#     out_text[-1] = in_text[0]\n",
        "#     in_text = np.reshape(in_text, (batch_size, -1))\n",
        "#     out_text = np.reshape(out_text, (batch_size, -1))\n",
        "#     return int_to_vocab, vocab_to_int, n_vocab, in_text, out_text\n",
        "\n",
        "# train_file, batch_size, seq_size = '/content/without-stop-words.txt', 256, 512\n",
        "\n",
        "# with open(train_file, 'r', encoding='utf-8') as f:\n",
        "#     text = f.read()\n",
        "# text = text.split()\n",
        "\n",
        "# word_counts = Counter(text)\n",
        "# sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "# int_to_vocab = {k: w for k, w in enumerate(sorted_vocab)}\n",
        "# vocab_to_int = {w: k for k, w in int_to_vocab.items()}\n",
        "# n_vocab = len(int_to_vocab)\n",
        "\n",
        "# print('Vocabulary size', n_vocab)\n",
        "\n",
        "# int_text = [vocab_to_int[w] for w in text]\n",
        "# num_batches = int(len(int_text) / (seq_size * batch_size))\n",
        "# in_text = int_text[:num_batches * batch_size * seq_size]\n",
        "# out_text = np.zeros_like(in_text)\n",
        "# out_text[:-1] = in_text[1:]\n",
        "# out_text[-1] = in_text[0]\n",
        "# in_text = np.reshape(in_text, (batch_size, -1))\n",
        "# out_text = np.reshape(out_text, (batch_size, -1))\n",
        "# return int_to_vocab, vocab_to_int, n_vocab, in_text, out_text"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}